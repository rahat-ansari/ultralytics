{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "614b4e62",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "134c0aa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "556291ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import face_recognition\n",
    "import torch\n",
    "import ultralytics\n",
    "print(f\"Dlib: {dlib.__version__}\")\n",
    "print(f\"Face-Recognition: {face_recognition.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "\n",
    "import mediapipe\n",
    "print(f\"mediapipe: {mediapipe.__version__}\")\n",
    "\n",
    "import pygame\n",
    "print(f\"pygame: {pygame.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ea0054",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from numpy import source\n",
    "\n",
    "from ultralytics import solutions\n",
    "from ultralytics.utils.plotting import Annotator\n",
    "\n",
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import pygame\n",
    "from ultralytics import solutions\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.solutions.config import SolutionConfig\n",
    "from ultralytics.utils import LOGGER\n",
    "\n",
    "from ultralytics.solutions.solutions import BaseSolution, SolutionAnnotator, SolutionResults\n",
    "from ultralytics.utils.plotting import colors\n",
    "\n",
    "# ========== üîä SOUND SETUP ==========\n",
    "pygame.mixer.init()\n",
    "ALARM_FILE = \"pols-aagyi-pols.mp3\"\n",
    "if os.path.exists(ALARM_FILE):\n",
    "    pygame.mixer.music.load(ALARM_FILE)\n",
    "else:\n",
    "    print(f\"[WARNING] Alarm file '{ALARM_FILE}' not found.\")\n",
    "\n",
    "\n",
    "# ========== üß† KNOWN FACE ENCODING LOADER ==========\n",
    "KNOWN_FACE_DIR = \"family_members\"\n",
    "known_face_encodings, known_face_names = [], []\n",
    "\n",
    "if os.path.exists(KNOWN_FACE_DIR):\n",
    "    for name in os.listdir(KNOWN_FACE_DIR):\n",
    "        person_dir = os.path.join(KNOWN_FACE_DIR, name)\n",
    "        if not os.path.isdir(person_dir):\n",
    "            continue\n",
    "        for filename in os.listdir(person_dir):\n",
    "            path = os.path.join(person_dir, filename)\n",
    "            try:\n",
    "                img = face_recognition.load_image_file(path)\n",
    "                enc = face_recognition.face_encodings(img)\n",
    "                if enc:\n",
    "                    known_face_encodings.append(enc[0])\n",
    "                    known_face_names.append(name)\n",
    "                    print(f\"[INFO] Loaded face for {name} from {filename}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[ERROR] Failed loading {path}: {e}\")\n",
    "else:\n",
    "    print(\"[WARNING] No known_faces directory found.\")\n",
    "\n",
    "\n",
    "# ========== üëÅÔ∏è FACE-RECOGNITION ALARM (REVISED & OPTIMIZED) ==========\n",
    "class FaceRecognitionAlarmVisionEye(solutions.VisionEye):\n",
    "    def __init__(self, *args, known_face_encodings=None, known_face_names=None, **kwargs):\n",
    "        super().__init__(*args, **kwargs)\n",
    "        self.known_face_encodings = known_face_encodings or []\n",
    "        self.known_face_names = known_face_names or []\n",
    "        self.sound_played = False\n",
    "        # Best practice: Set face recognition tolerance during initialization\n",
    "        self.face_tolerance = 0.55\n",
    "        self.vision_point = self.CFG[\"vision_point\"]\n",
    "        self.records = self.CFG.get(\"records\", 1)\n",
    "        # self.show = self.CFG.get(\"show\", True)\n",
    "    \n",
    "    def play_sound(self):\n",
    "        \"\"\"Plays the alarm sound if it's not already playing.\"\"\"\n",
    "        if not self.sound_played:\n",
    "            if pygame.mixer.get_init() and not pygame.mixer.music.get_busy():\n",
    "                pygame.mixer.music.play()\n",
    "                self.sound_played = True\n",
    "                LOGGER.info(\"üö® Alarm Triggered: Unknown person count reached threshold.\")\n",
    "\n",
    "    def reset_sound(self):\n",
    "        \"\"\"Stops the alarm sound and resets the state.\"\"\"\n",
    "        if self.sound_played:\n",
    "            if pygame.mixer.get_init():\n",
    "                pygame.mixer.music.stop()\n",
    "            self.sound_played = False\n",
    "            LOGGER.info(\"üü¢ Alarm Reset: Area clear.\")\n",
    "\n",
    "    def __call__(self, im0):\n",
    "        \"\"\"\n",
    "        Processes a single frame for person detection and face recognition.\n",
    "        This implementation follows best practices for accuracy and performance.\n",
    "        \"\"\"\n",
    "        # 1. Get person detections from the base class\n",
    "        self.extract_tracks(im0)\n",
    "        annotator = SolutionAnnotator(im0, line_width=self.line_width)\n",
    "        \n",
    "        unknown_person_count = 0\n",
    "\n",
    "        # 2. Optimize by finding all faces in the frame at once (on a smaller version)\n",
    "        # This is much faster than processing crops for each person.\n",
    "        h, w, _ = im0.shape\n",
    "        small_frame = cv2.resize(im0, (0, 0), fx=0.25, fy=0.25)\n",
    "        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "        face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "\n",
    "        # 3. Iterate through detected PERSONS from YOLO\n",
    "        for box, conf, cls, t_id in zip(self.boxes, self.confs, self.clss, self.track_ids):\n",
    "            if int(cls) == 0:  # Skip if not a person\n",
    "                \n",
    "\n",
    "                name = \"Unknown\"\n",
    "                is_known = False\n",
    "                \n",
    "                # 4. Associate faces with person boxes\n",
    "                # Check if any detected face is inside this person's bounding box\n",
    "                person_box_left, person_box_top, person_box_right, person_box_bottom = map(int, box)\n",
    "                \n",
    "                for (face_top, face_right, face_bottom, face_left), face_encoding in zip(face_locations, face_encodings):\n",
    "                    # Scale face locations back to original image size\n",
    "                    face_top *= 4\n",
    "                    face_right *= 4\n",
    "                    face_bottom *= 4\n",
    "                    face_left *= 4\n",
    "\n",
    "                    # Check if the center of the face is inside the person's box\n",
    "                    face_center_x = (face_left + face_right) // 2\n",
    "                    face_center_y = (face_top + face_bottom) // 2\n",
    "\n",
    "                    if (person_box_left <= face_center_x <= person_box_right and\n",
    "                        person_box_top <= face_center_y <= person_box_bottom):\n",
    "                        \n",
    "                        # 5. Use robust face matching for the associated face\n",
    "                        if self.known_face_encodings:\n",
    "                            face_distances = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "                            best_match_index = np.argmin(face_distances)\n",
    "                            \n",
    "                            if face_distances[best_match_index] < self.face_tolerance:\n",
    "                                name = self.known_face_names[best_match_index]\n",
    "                                is_known = True\n",
    "                        \n",
    "                        # Once a face is matched to this person, stop checking other faces\n",
    "                        break \n",
    "                \n",
    "                # 6. Update counter and draw labels\n",
    "                if not is_known:\n",
    "                    unknown_person_count += 1\n",
    "                    color = (0, 0, 255) # Red for Unknown\n",
    "                    # label = f\"Unknown ({conf:.2f})\"\n",
    "                    label = f\"Unknown\"\n",
    "                else:\n",
    "                    color = (0, 255, 0) # Green for Known\n",
    "                    label = f\"{name}\"\n",
    "                    # label = f\"{name} ({conf:.2f})\"\n",
    "                \n",
    "                # annotator.box_label(box, label, color=color)\n",
    "                \n",
    "                # annotator.visioneye(box, self.vision_point)\n",
    "                # build base label from the existing adjust_box_label()\n",
    "                base_label = self.adjust_box_label(int(cls), float(conf) if conf is not None else 0.0, t_id)\n",
    "\n",
    "                # custom label for 'person' class (COCO id 0). Use CFG override if provided.\n",
    "                if int(cls) == 0:\n",
    "                    prefix = str(self.CFG.get(\"person_label_prefix\", label))\n",
    "                    custom_label = f\"{prefix}:\"\n",
    "                    # if base_label exists, concat both for full display\n",
    "                    final_label = f\"{custom_label} {base_label}\" if base_label else custom_label\n",
    "                else:\n",
    "                    final_label = base_label\n",
    "\n",
    "                # draw final label and vision eye mapping\n",
    "                annotator.box_label(box, label=final_label, color=colors(int(t_id), True))\n",
    "            else:\n",
    "                # For non-person classes, use default labeling\n",
    "                annotator.box_label(box, label=self.adjust_box_label(cls, conf, t_id), color=colors(int(t_id), True))\n",
    "            \n",
    "            annotator.visioneye(box, self.vision_point) \n",
    "\n",
    "        # 7. Trigger alarm based on the COUNT of unknown people and the 'records' threshold\n",
    "        if unknown_person_count >= self.records:\n",
    "            self.play_sound()\n",
    "        else:\n",
    "            self.reset_sound()\n",
    "\n",
    "        plot_im = annotator.result()\n",
    "        self.display_output(plot_im) \n",
    "        \n",
    "        \n",
    "        # Display track count on the frame\n",
    "        total_tracks = len(getattr(self, \"track_ids\", []))\n",
    "        cv2.putText(plot_im, f\"Tracks: {total_tracks}\", (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 1, (255, 255, 255), 2)\n",
    "\n",
    "        return SolutionResults(plot_im=plot_im, total_tracks=len(self.track_ids))\n",
    "\n",
    "   \n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    # cap = cv2.VideoCapture(0)\n",
    "    cap = cv2.VideoCapture(\"media_files/WIN_20251103_14_11_20_Pro.mp4\")\n",
    "    # cap = cv2.VideoCapture(\"media_files/person/ruhama/VID_20251122_142652.mp4\")\n",
    "    # cap = cv2.VideoCapture(\"media_files/WIN_20251103_14_11_20_Pro.mp4\")\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "    # Video writer\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "    video_writer = cv2.VideoWriter(\"visioneye_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "    # Initialize vision eye object\n",
    "    visioneyeInterface = FaceRecognitionAlarmVisionEye(\n",
    "        show=True,  # display the output\n",
    "        model=\"yolo11m.pt\",  # use any model that Ultralytics support, i.e, YOLOv10\n",
    "        # classes=[0, 19],  # generate visioneye view for specific classes\n",
    "        vision_point=(550, 50),  # the point, where vision will view objects and draw tracks\n",
    "        known_face_encodings=known_face_encodings, \n",
    "        known_face_names=known_face_names,\n",
    "        records=3,\n",
    "        conf=0.5,\n",
    "        # show_labels=True,\n",
    "    )\n",
    "\n",
    " \n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    results = visioneyeInterface(im0)\n",
    "\n",
    "    print(results)  # access the output\n",
    "\n",
    "    # video_writer.write(results.plot_im)  # write the video file\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV_YOLO_WITH_FACE_RECOGNITION",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
