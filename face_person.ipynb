{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e37578",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import face_recognition\n",
    "import torch\n",
    "import ultralytics\n",
    "print(f\"Dlib: {dlib.__version__}\")\n",
    "print(f\"Face-Recognition: {face_recognition.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "\n",
    "import mediapipe\n",
    "print(f\"mediapipe: {mediapipe.__version__}\")\n",
    "\n",
    "import pygame\n",
    "print(f\"pygame: {pygame.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display current Ultralytics YOLO settings.\n",
    "\n",
    "This command shows the current configuration settings for the Ultralytics YOLO framework.\n",
    "\"\"\"\n",
    "!yolo settings\n",
    "\n",
    "# Reset settings to default values\n",
    "!yolo settings reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import settings\n",
    "\n",
    "# View all settings\n",
    "print(settings)\n",
    "\n",
    "# Return a specific setting\n",
    "value = settings[\"datasets_dir\"]\n",
    "print(f\"Datasets directory: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "\n",
    "# Check CUDA version if available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "\tprint(f\"CUDA version: {torch.version.cuda}\")\n",
    "\tprint(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\tprint(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92887475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# HOME = os.getcwd()\n",
    "# print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d78f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training from a pretrained *.pt model\n",
    "!yolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n",
    "# !yolo detect train data=coco8.yaml model=yolo11x.pt epochs=100 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# # model = YOLO(\"yolo11n.yaml\")  # build a new model from YAML\n",
    "# model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "# # model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n",
    "\n",
    "# # Train the model\n",
    "# results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd821ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo detect val model=yolo11n.pt      # val official model\n",
    "# !yolo detect val model=path/to/best.pt # val custom model\n",
    "!yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
    "# !yolo val model=yolo11x.pt data=coco8.yaml batch=1 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d797408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train the model on COCO8\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f01d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "# # model = YOLO(\"path/to/best.pt\")  # load a custom model\n",
    "\n",
    "# # Validate the model\n",
    "# metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "# metrics.box.map  # map50-95\n",
    "# metrics.box.map50  # map50\n",
    "# metrics.box.map75  # map75\n",
    "# metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52500993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "# model = YOLO(\"./runs/detect/train/weights/best.pt\")  # load a custom model\n",
    "\n",
    "# # Validate the model\n",
    "# metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "# metrics.box.map  # map50-95\n",
    "# metrics.box.map50  # map50\n",
    "# metrics.box.map75  # map75\n",
    "# metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a new model from YAML and start training from scratch\n",
    "# !yolo detect train data=coco8.yaml model=yolo11n.yaml epochs=100 imgsz=640\n",
    "\n",
    "# # Start training from a pretrained *.pt model\n",
    "# !yolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n",
    "\n",
    "# # Build a new model from YAML, transfer pretrained weights to it and start training\n",
    "# !yolo detect train data=coco8.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e07bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo detect val model=yolo11n.pt\n",
    "# !yolo detect val model=runs/detect/train/weights/best.pt # val custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# model = YOLO('yolo11n.pt')  # load a pretrained YOLO detection model\n",
    "model = YOLO('runs/detect/train/weights/best.pt')  # load a pretrained YOLO detection model\n",
    "# model.train(data='coco8.yaml', epochs=3)  # train the model\n",
    "# model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
    "# results = model.predict(source=\"./testImages/vid1.mp4\", show=True)  \n",
    "# model.predict(\"https://media.roboflow.com/notebooks/examples/dog.jpeg\", save=True, imgsz=640, conf=0.25)\n",
    "model.predict(\"https://media.roboflow.com/notebooks/examples/dog.jpeg\", save=True, conf=0.25)\n",
    "# results = model.export(format='onnx')  # export the model to ONNX format# print(results)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b84ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11n, train it on COCO128 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "# model = YOLO('yolo11x.pt')  # load a pretrained YOLO detection model\n",
    "model = YOLO('runs/detect/train2/weights/best.pt')  # load a pretrained YOLO detection model\n",
    "# model.train(data='coco8.yaml', epochs=3)  # train the model\n",
    "\n",
    "# results = model.val()  # evaluate model performance on the validation set\n",
    "# results = model.predict('./testImages/vid1.mp4', show=True)  # predict on an image\n",
    "# results = model.predict('./testImages/vid1.mp4', show=True)  # predict on an image\n",
    "\n",
    "# results = model.export(format='onnx')  # export the model to ONNX format\n",
    "\n",
    "\n",
    "# Run inference on 'bus.jpg' with arguments\n",
    "results = model.predict(\"./Test-Video-And-Images/istockphoto-457334218-640_adpp_is.mp4\", show=True, save=True, conf=0.25, iou=0.99, classes=[0], imgsz=640)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4edd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('runs/detect/train2/weights/best.pt')  # Load your model\n",
    "video_path = './Test-Video-And-Images/855564-hd_1920_1080_24fps.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video's native resolution\n",
    "orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "aspect_ratio = orig_width / orig_height\n",
    "\n",
    "# Set your base dimension (choose one)\n",
    "base_width = 800  # Fixed width, auto height\n",
    "# base_height = 600  # Fixed height, auto width\n",
    "\n",
    "# Calculate dimensions (choose one method)\n",
    "# Option 1: Fixed width, auto height\n",
    "display_width = base_width\n",
    "display_height = int(base_width / aspect_ratio)\n",
    "\n",
    "# Option 2: Fixed height, auto width\n",
    "# display_height = base_height\n",
    "# display_width = int(base_height * aspect_ratio)\n",
    "\n",
    "window_name = \"YOLOv11x Inference\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, display_width, display_height)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Run inference\n",
    "    results = model(frame, imgsz=640)\n",
    "    \n",
    "    # Get annotated frame\n",
    "    annotated_frame = results[0].plot()\n",
    "    \n",
    "    # Show results\n",
    "    cv2.imshow(window_name, annotated_frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "404039be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo checks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/istockphoto-1139869319-640_adpp_is.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define region points\n",
    "# region_points = [(150, 150), (1130, 150), (1130, 570), (150, 570)]\n",
    "region_points = [(0, 0), (1130, 0), (1130, 480), (0, 480)]\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Init trackzone (object tracking in zones, not complete frame)\n",
    "trackzone = solutions.TrackZone(\n",
    "    show=True,  # display the output\n",
    "    region=region_points,  # pass region points\n",
    "    model=\"yolo11n.pt\",\n",
    "    classes=[0, 47],\n",
    "    conf=0.3,\n",
    "    iou=0.99,\n",
    "    # tracker='bytetrack.yaml',\n",
    "    # track_high_thresh=0.5\n",
    "    \n",
    "    # tracker=\"bytetrack.yaml\",  # use ByteTrack for tracking\n",
    "    # track_high_thresh=0.5\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "    results = trackzone(im0)\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"Test-Video-And-Images/istockphoto-1139869319-640_adpp_is.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Store the track history\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        result = model.track(frame, persist=True, conf=0.5, iou=0.5, max_det=1000, classes=[0])[0]\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        if result.boxes and result.boxes.is_track:\n",
    "            boxes = result.boxes.xywh.cpu()\n",
    "            track_ids = result.boxes.id.int().cpu().tolist()\n",
    "\n",
    "            # Visualize the result on the frame\n",
    "            frame = result.plot()\n",
    "\n",
    "            # Plot the tracks\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                x, y, w, h = box\n",
    "                track = track_history[track_id]\n",
    "                track.append((float(x), float(y)))  # x, y center point\n",
    "                if len(track) > 30:  # retain 30 tracks for 30 frames\n",
    "                    track.pop(0)\n",
    "\n",
    "                # Draw the tracking lines\n",
    "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c221c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"Test-Video-And-Images/istockphoto-1139869319-640_adpp_is.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"instance-segmentation.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Init InstanceSegmentation\n",
    "isegment = solutions.InstanceSegmentation(\n",
    "    show=True,  # display the output\n",
    "    model=\"yolo11n-seg.pt\",  # model=\"yolo11n-seg.pt\" for object segmentation using YOLO11.\n",
    "    classes=[0],\n",
    "    conf=0.7,\n",
    "    iou=0.99,\n",
    "    \n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "    results = isegment(im0)\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/test.jpeg\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Initialize object cropper object\n",
    "cropper = solutions.ObjectCropper(\n",
    "    show=True,  # display the output with bounding boxes\n",
    "    model=\"runs/detect/train2/weights/best.pt\",  # use trained model\n",
    "    conf=0.45,  # confidence threshold\n",
    "    iou=0.99,  # intersection over union threshold\n",
    "    crop_dir=\"cropped-detections\",  # output directory for cropped detections \n",
    "    line_width=2,  # thicker bounding box lines for better visibility\n",
    "    show_labels=True,  # display labels on bounding boxes\n",
    "    \n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = cropper(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03add51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/20gillisWeb-videoSixteenByNine3000.jpg\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# region_points = [(10, 200), (1080, 200)]                                      # line counting\n",
    "# region_points = [(0, 200), (800, 200)]                                     # line counting\n",
    "# region_points = [(0, 210), (1080, 210), (1080, 400), (0, 400)]  # rectangle region\n",
    "# region_points = [(0, 200), (1080, 200), (1080, 150), (0, 150), (0, 150)]   # polygon region\n",
    "\n",
    "\n",
    "# region_points = [(20, 400), (1080, 400)]                                      # line counting\n",
    "region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # rectangle region\n",
    "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]   # polygon region\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize object counter object\n",
    "counter = solutions.ObjectCounter(\n",
    "    show=True,  # display the output\n",
    "    region=region_points,  # pass region points\n",
    "    model=\"yolo11n-obb.pt\",  # model=\"yolo11n-obb.pt\" for object counting with OBB model.\n",
    "    # classes=[0, 2],  # count specific classes i.e. person and car with COCO pretrained model.\n",
    "    tracker=\"botsort.yaml\",  # choose trackers i.e \"bytetrack.yaml\"\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = counter(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo solutions trackzone source=\"path/to/video.mp4\" show=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620fc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/count.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# region_points = [(20, 400), (1080, 400)]                                      # line counting\n",
    "region_points = [(0, 200), (700, 200), (700, 260), (0, 260)]  # rectangle region\n",
    "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]   # polygon region\n",
    "# region_points = [(200, 0), (200, 420),(260, 420), (260, 0)]  # rectangle region\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize object counter object\n",
    "counter = solutions.ObjectCounter(\n",
    "    show=True,  # display the output\n",
    "    region=region_points,  # pass region points\n",
    "    model=\"./runs/detect/train2/weights/best.pt\",  # model=\"yolo11n-obb.pt\" for object counting with OBB model.\n",
    "    # classes=[0, 2],  # count specific classes i.e. person and car with COCO pretrained model.\n",
    "    classes=[47],  # count specific classes i.e. person and car with COCO pretrained model.\n",
    "    # tracker=\"botsort.yaml\",  # choose trackers i.e \"bytetrack.yaml\"\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = counter(im0)\n",
    "\n",
    "    print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd623495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "\n",
    "def count_objects_in_region(video_path, output_video_path, model_path):\n",
    "    \"\"\"Count objects in a specific region within a video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "    region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n",
    "    counter = solutions.ObjectCounter(show=True, region=region_points, model=model_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, im0 = cap.read()\n",
    "        if not success:\n",
    "            print(\"Video frame is empty or processing is complete.\")\n",
    "            break\n",
    "        results = counter(im0)\n",
    "        video_writer.write(results.plot_im)\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "count_objects_in_region(\"./Test-Video-And-Images/count.mp4\", \"output_video.avi\", \"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "from email_settings import password, from_email, to_email\n",
    "\n",
    "    \n",
    "    \n",
    "# create server\n",
    "server = smtplib.SMTP('smtp.gmail.com: 587')\n",
    "\n",
    "server.starttls()\n",
    "\n",
    "# Login Credentials for sending the mail\n",
    "server.login(from_email, password)\n",
    "\n",
    "    \n",
    "def send_email(to_email, from_email, people_detected=1):\n",
    " \n",
    "    message = MIMEMultipart()\n",
    "    message['From'] = from_email\n",
    "    message['To'] = to_email\n",
    "    message['Subject'] = \"Security Alert\"\n",
    "    # add in the message body\n",
    "    message.attach(MIMEText(f'ALERT - {people_detected} persons has been detected!!', 'plain'))\n",
    "    server.sendmail(from_email, to_email, message.as_string())\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "       \n",
    "        self.capture_index = capture_index\n",
    "        \n",
    "        self.email_sent = False\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        print(\"Using Device: \", self.device)\n",
    "        \n",
    "        self.model = self.load_model()\n",
    "        \n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "    \n",
    "        self.box_annotator = sv.BoxAnnotator(color=sv.ColorPalette.default(), thickness=3, text_thickness=3, text_scale=1.5)\n",
    "    \n",
    "\n",
    "    def load_model(self):\n",
    "       \n",
    "        model = YOLO(\"yolo11n.pt\")  # load a pretrained YOLOv8n model\n",
    "        model.fuse()\n",
    "    \n",
    "        return model\n",
    "\n",
    "\n",
    "    def predict(self, frame):\n",
    "       \n",
    "        results = self.model(frame)\n",
    "        \n",
    "        return results\n",
    "    \n",
    "\n",
    "    def plot_bboxes(self, results, frame):\n",
    "        \n",
    "        xyxys = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        \n",
    "        # Extract detections for person class\n",
    "        for result in results[0]:\n",
    "            class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "            \n",
    "            if class_id == 0:\n",
    "                \n",
    "                xyxys.append(result.boxes.xyxy.cpu().numpy())\n",
    "                confidences.append(result.boxes.conf.cpu().numpy())\n",
    "                class_ids.append(result.boxes.cls.cpu().numpy().astype(int))\n",
    "\n",
    "        \n",
    "            \n",
    "        # Setup detections for visualization\n",
    "        \n",
    "        detections = sv.Detections.from_ultralytics(results[0])\n",
    "    \n",
    "        frame = self.box_annotator.annotate(scene=frame, detections=detections)\n",
    "        \n",
    "        \n",
    "        return frame, class_ids\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __call__(self):\n",
    "\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        frame_count = 0\n",
    "      \n",
    "        while True:\n",
    "          \n",
    "            start_time = time()\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            assert ret\n",
    "            \n",
    "            results = self.predict(frame)\n",
    "            frame, class_ids = self.plot_bboxes(results, frame)\n",
    "            \n",
    "            if len(class_ids) > 0:\n",
    "                if not self.email_sent:  # Only send email if it hasn't been sent for the current detection\n",
    "                    send_email(to_email, from_email, len(class_ids))\n",
    "                    self.email_sent = True  # Set the flag to True after sending the email\n",
    "            else:\n",
    "                self.email_sent = False  # Reset the flag when no person is detected\n",
    "\n",
    "            \n",
    "            end_time = time()\n",
    "            fps = 1/np.round(end_time - start_time, 2)\n",
    "             \n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "            \n",
    "            cv2.imshow('YOLOv8 Detection', frame)\n",
    "            \n",
    "            frame_count += 1\n",
    " \n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                \n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        server.quit()\n",
    "        \n",
    "        \n",
    "    \n",
    "detector = ObjectDetection(capture_index=0)\n",
    "detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f78db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo solutions count show=True # for object counting\n",
    "\n",
    "!yolo solutions count show=True source=\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\" # specify video file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b74516",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img --weights \"path/to/yolov8n.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee615840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"Test-Video-And-Images/count.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"isegment_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize instance segmentation object\n",
    "isegment = solutions.InstanceSegmentation(\n",
    "    show=True,  # display the output\n",
    "    model=\"yolo11n-seg.pt\",  # model=\"yolo11n-seg.pt\" for object segmentation using YOLO11.\n",
    "    # classes=[0, 2],  # segment specific classes i.e, person and car with pretrained model.\n",
    "    line_width=2,  # thicker bounding box lines for better visibility\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    results = isegment(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Pass region as list\n",
    "region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n",
    "\n",
    "# Pass region as dictionary\n",
    "region_points = {\n",
    "    # \"region-01\": [(50, 50), (250, 50), (250, 250), (50, 250)],\n",
    "    # \"region-02\": [(640, 640), (780, 640), (780, 720), (640, 720)],\n",
    "}\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"region_counting.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize region counter object\n",
    "regioncounter = solutions.RegionCounter(\n",
    "    show=True,  # display the frame\n",
    "    region=region_points,  # pass region points\n",
    "    model=\"yolo11n.pt\",  # model for counting in regions i.e yolo11s.pt\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = regioncounter(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d841ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install inference-cli && inference server start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f818ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ba2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo export model=yolo11x.pt format=onnx      # export official model\n",
    "# !yolo export model=runs/detect/train2/weights/best.pt format=onnx # export custom trained model\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"yolo11x.pt\")  # load an official model\n",
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")  # load a custom trained model\n",
    "image = Image.open(requests.get('https://media.roboflow.com/notebooks/examples/dog.jpeg', stream=True).raw)\n",
    "result = model.predict(image, save=True, conf=0.25)[0]\n",
    "\n",
    "# # Export the model\n",
    "# model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb7f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a counting example\n",
    "# !yolo solutions count show=True\n",
    "\n",
    "# Pass a source video\n",
    "# !yolo solutions count source=\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\" show=True\n",
    "\n",
    "# Pass region coordinates\n",
    "!yolo solutions count region=\"[(0, 200), (1080, 200), (1080, 260), (0, 260)]\" source=\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\" show=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa265f29",
   "metadata": {},
   "source": [
    "## Draw bounding boxes on the detection results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8221b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.boxes.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.boxes.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e00fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.boxes.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b00c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "detections = sv.Detections.from_ultralytics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)\n",
    "\n",
    "annotated_image = image.copy()\n",
    "annotated_image = box_annotator.annotate(annotated_image, detections=detections)\n",
    "annotated_image = label_annotator.annotate(annotated_image, detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_image, size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f7064",
   "metadata": {},
   "source": [
    "## check face recognition between two images barackObama.jpg and tigerWoods.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "image = face_recognition.load_image_file(\"./Test-Video-And-Images/barackObama.jpg\")\n",
    "\n",
    "cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Face Detection\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "for (top, right, bottom, left) in face_locations:\n",
    "    # Draw a box around the face\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "# Facial Landmarks Detection\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "facial_features = [\n",
    "        'chin',\n",
    "        'left_eyebrow',\n",
    "        'right_eyebrow',\n",
    "        'nose_bridge',\n",
    "        'nose_tip',\n",
    "        'left_eye',\n",
    "        'right_eye',\n",
    "        'top_lip',\n",
    "        'bottom_lip']\n",
    "\n",
    "\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    for facial_feature in facial_features:\n",
    "        for point in face_landmarks[facial_feature]:\n",
    "            image = cv2.circle(image, point, 2, (255,60,170),2)\n",
    "            \n",
    "            \n",
    "cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Face Recognition\n",
    "original_image = face_recognition.load_image_file(\"./Test-Video-And-Images/barackObama.jpg\")\n",
    "unknown_image = face_recognition.load_image_file(\"./Test-Video-And-Images/tigerWoods.jpeg\")\n",
    "# Get face encodings\n",
    "try:\n",
    "    image_encoding = face_recognition.face_encodings(original_image)[0]\n",
    "    unknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
    "    \n",
    "    # Compare faces\n",
    "    results = face_recognition.compare_faces([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Calculate face distance for more detailed comparison\n",
    "    face_distance = face_recognition.face_distance([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Add text to unknown image\n",
    "    cv2.putText(unknown_image, f'Barack Obama: {results[0]}', (25, 75), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(unknown_image, f'Distance: {face_distance[0]:.2f}', (25, 125), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Barack Obama\", cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imshow(\"Unknown\", cv2.cvtColor(unknown_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "except IndexError:\n",
    "    print(\"No face found in one or both images\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62366813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load and display initial image\n",
    "image = face_recognition.load_image_file(\"./Images/Bappi/1.jpg\")\n",
    "\n",
    "# cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# Face Detection using face_recognition\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "for (top, right, bottom, left) in face_locations:\n",
    "    # Draw a box around the face\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow(\"Image with Face Detection\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Face Mesh Detection using mediapipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "# Reload original image for face mesh (without rectangles)\n",
    "image_mesh = face_recognition.load_image_file(\"./Images/Bappi/1.jpg\")\n",
    "\n",
    "# Convert image to RGB for mediapipe (face_recognition loads as RGB by default)\n",
    "results = face_mesh.process(image_mesh)\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        # # Draw the face mesh\n",
    "        # mp_drawing.draw_landmarks(\n",
    "        #     image_mesh,\n",
    "        #     face_landmarks,\n",
    "        #     mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        #     None,\n",
    "        #     mp_drawing_styles.get_default_face_mesh_contours_style()\n",
    "        # )\n",
    "        \n",
    "        # Optionally draw tesselation for full mesh\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_mesh,\n",
    "            face_landmarks,\n",
    "            mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            None,\n",
    "            mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "        )\n",
    "\n",
    "cv2.imshow(\"Face Mesh\", cv2.cvtColor(image_mesh, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# # Alternative: Manual face mesh drawing with custom points\n",
    "# image_manual_mesh = face_recognition.load_image_file(\"./barackObama.jpg\")\n",
    "\n",
    "# if results.multi_face_landmarks:\n",
    "#     for face_landmarks in results.multi_face_landmarks:\n",
    "#         # Convert mediapipe landmarks to pixel coordinates\n",
    "#         h, w, _ = image_manual_mesh.shape\n",
    "#         for lm in face_landmarks.landmark:\n",
    "#             x, y = int(lm.x * w), int(lm.y * h)\n",
    "#             cv2.circle(image_manual_mesh, (x, y), 1, (255, 60, 170), 1)\n",
    "\n",
    "# cv2.imshow(\"Manual Face Mesh Points\", cv2.cvtColor(image_manual_mesh, cv2.COLOR_RGB2BGR))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# Face Recognition\n",
    "original_image = face_recognition.load_image_file(\"./Images/Bappi/1.jpg\")\n",
    "unknown_image = face_recognition.load_image_file(\"./Test-Video-And-Images/barackObama.jpg\")\n",
    "\n",
    "# Get face encodings\n",
    "try:\n",
    "    image_encoding = face_recognition.face_encodings(original_image)[0]\n",
    "    unknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
    "    \n",
    "    # Compare faces\n",
    "    results = face_recognition.compare_faces([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Calculate face distance for more detailed comparison\n",
    "    face_distance = face_recognition.face_distance([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Add text to unknown image\n",
    "    cv2.putText(unknown_image, f'Answer is Obama: {results[0]}', (25, 75), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(unknown_image, f'Distance: {face_distance[0]:.2f}', (25, 125), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"answer is \", cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imshow(\"Unknown\", cv2.cvtColor(unknown_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "except IndexError:\n",
    "    print(\"No face found in one or both images\")\n",
    "\n",
    "# Clean up\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import smtplib\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Load known faces\n",
    "known_face_encodings = [...]  # Preloaded face encodings\n",
    "known_face_names = [...]       # Names of known people\n",
    "\n",
    "# Capture Video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    results = model(frame)  # YOLO detection\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy  # Extract bounding boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Face recognition\n",
    "            face_encodings = face_recognition.face_encodings(frame, [(y1, x2, y2, x1)])\n",
    "            if face_encodings:\n",
    "                match = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "                if not any(match):\n",
    "                    print(\"Unauthorized person detected!\")\n",
    "                    cv2.putText(frame, \"THREAT!\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                    \n",
    "                    # Send email alert\n",
    "                    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "                        server.starttls()\n",
    "                        server.login(\"your_email@gmail.com\", \"your_password\")\n",
    "                        server.sendmail(\"your_email@gmail.com\", \"security_team@gmail.com\", \"Threat Detected!\")\n",
    "\n",
    "    cv2.imshow(\"Security Feed\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=train model=yolo11n.pt data={dataset.location}/data.yaml epochs=10 lr0=0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model=\"./runs/detect/train2/weights/best.pt\" source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d725e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo train data=coco8.yaml model=\"./runs/detect/train2/weights/best.pt\" epochs=10 lr0=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3de451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate YOLO11n on COCO8 val with batch size 1\n",
    "!yolo val model=\"./runs/detect/train3/weights/best.pt\" data=coco8.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo predict model=\"./runs/detect/train3/weights/best.pt\" source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a731b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model=runs/detect/train2/weights/best.pt source=Test-Video-And-Images/count.mp4 show=True save=True imgsz=320 conf=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training from a pretrained *.pt model\n",
    "# !yolo obb train data=dota8.yaml model=yolo11n-obb.pt epochs=100 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15993b9c",
   "metadata": {},
   "source": [
    "# Security Alarm and Face Recognition\n",
    "\n",
    "This section implements an integrated security monitoring system that combines object detection, face recognition, and security alerts. The system offers:\n",
    "\n",
    "- Real-time object detection using YOLO model\n",
    "- Face recognition to identify known individuals\n",
    "- Motion tracking using MediaPipe\n",
    "- Security alarm triggering\n",
    "- Automated email alerts\n",
    "- Event logging\n",
    "- Performance optimization with frame processing intervals\n",
    "- FPS monitoring\n",
    "\n",
    "Features:\n",
    "- Detects 80+ object classes including persons, vehicles, and common objects\n",
    "- Recognizes faces and matches against known face database\n",
    "- Logs security events with timestamps\n",
    "- Plays audio alarm when unauthorized access detected\n",
    "- Sends email notifications with detection details\n",
    "- Displays real-time FPS and detection information\n",
    "\n",
    "Usage:\n",
    "- Add known faces to 'images' directory\n",
    "- Configure email settings for alerts\n",
    "- Press 'q' to quit monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"security_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "from_email = \"deveansari@gmail.com\"  # the sender email address\n",
    "password = \"nuzb vkot eauw qihl\"  # 16-digits password generated via: https://myaccount.google.com/apppasswords\n",
    "to_email = \"rahatansari.tpu@gmail.com\"  # the receiver email address\n",
    "\n",
    "# Initialize security alarm object\n",
    "securityalarm = solutions.SecurityAlarm(\n",
    "    show=True,  # display the output\n",
    "    model=\"yolo11n.pt\",  # i.e. yolo11s.pt, yolo11m.pt\n",
    "    records=1,  # total detections count to send an email\n",
    ")\n",
    "\n",
    "securityalarm.authenticate(from_email, password, to_email)  # authenticate the email server\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    results = securityalarm(im0)\n",
    "\n",
    "    print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719118b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f815ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a new model from YAML, transfer pretrained weights for face detection\n",
    "# !yolo detect train data=face_dataset.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640 single_cls=True name=face_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae974c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load models\n",
    "yolo_model = YOLO('yolo11n.pt')  # COCO-trained model\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # YOLO object detection\n",
    "    results = yolo_model(image)\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for result in results:\n",
    "        # Draw non-person objects\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls)\n",
    "            conf = float(box.conf)\n",
    "            label = yolo_model.names[cls_id]\n",
    "            \n",
    "            if label != \"person\":\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        # Face detection for person regions\n",
    "        person_boxes = [box.xyxy[0] for box in result.boxes if int(box.cls) == 0]\n",
    "        for person_box in person_boxes:\n",
    "            x1, y1, x2, y2 = map(int, person_box)\n",
    "            padding = 20  # Extra area around person\n",
    "            roi = image[max(0, y1-padding):min(y2+padding, image.shape[0]), \n",
    "                        max(0, x1-padding):min(x2+padding, image.shape[1])]\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, \n",
    "                scaleFactor=1.1, \n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30)\n",
    "            )\n",
    "            \n",
    "            # Draw face boxes\n",
    "            for (fx, fy, fw, fh) in faces:\n",
    "                # Convert to original image coordinates\n",
    "                abs_x = max(0, x1-padding) + fx\n",
    "                abs_y = max(0, y1-padding) + fy\n",
    "                cv2.rectangle(image, \n",
    "                            (abs_x, abs_y),\n",
    "                            (abs_x+fw, abs_y+fh),\n",
    "                            (0, 0, 255), 2)\n",
    "                cv2.putText(image, \"Face\", (abs_x, abs_y-5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Save results\n",
    "    output_path = image_path.replace('.jpg', '_result.jpg')\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_objects(\"./Test-Video-And-Images/20gillisWeb-videoSixteenByNine3000.jpg\")  # Example local image file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb49a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_model = YOLO('yolo11n.pt')\n",
    "\n",
    "# Load Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Verify the cascade loaded correctly\n",
    "if face_cascade.empty():\n",
    "    print(\"Error: Could not load Haar cascade file\")\n",
    "else:\n",
    "    print(\"Haar cascade loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caff61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_haar(image):\n",
    "    \"\"\"\n",
    "    Detect faces using Haar cascade and return bounding boxes\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    \n",
    "    face_boxes = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Convert to [x1, y1, x2, y2, confidence, class_name] format\n",
    "        face_boxes.append([x, y, x + w, y + h, 0.9, 'face'])  # Haar doesn't provide confidence, so we use 0.9\n",
    "    \n",
    "#  detect_faces_haar(\"d:\\\\Projects\\\\ultralytics\\\\face_person\\\\images\\\\happy-group-of-business-people-walking-in-front-of-the-office-G24CN2.jpg\")he-office-G24CN2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_yolo_haar_detection(image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Perform combined object detection (YOLO) and face detection (Haar Cascade)\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # YOLO object detection (all classes including person)\n",
    "    yolo_results = yolo_model.predict(\n",
    "        source=image,\n",
    "        conf=conf_threshold,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Face detection using Haar cascade\n",
    "    face_boxes = detect_faces_haar(image)\n",
    "    \n",
    "    return image, yolo_results, face_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524dcd20",
   "metadata": {},
   "source": [
    "## Face Person Detection from Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b29f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample image\n",
    "image_path = \"./bus.jpg\"  # Replace with your image path\n",
    "\n",
    "# Run detection\n",
    "def detect_objects_and_faces_haar(image_path, save_path, show_result=True):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    # Load YOLO model\n",
    "    model = YOLO('yolo11n.pt')\n",
    "    \n",
    "    # Load Haar cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # YOLO detection\n",
    "    results = model(img)\n",
    "    \n",
    "    # Get face detections\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    annotated_img = results[0].plot()\n",
    "    \n",
    "    # Draw face detections\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(annotated_img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Save result\n",
    "    cv2.imwrite(save_path, annotated_img)\n",
    "    \n",
    "    # Show result if requested\n",
    "    if show_result:\n",
    "        cv2.imshow('Result', annotated_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    return annotated_img\n",
    "\n",
    "result = detect_objects_and_faces_haar(\n",
    "    image_path=image_path,\n",
    "    save_path=\"result_yolo_haar.jpg\",\n",
    "    show_result=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5928ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load YOLO model for general object detection\n",
    "yolo_model = YOLO('yolo11n.pt')\n",
    "\n",
    "# Initialize MediaPipe face detection for accurate face detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117180eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_mediapipe(image):\n",
    "    \"\"\"\n",
    "    Detect faces using MediaPipe and return bounding boxes in YOLO format\n",
    "    \"\"\"\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "    \n",
    "    face_boxes = []\n",
    "    if results.detections:\n",
    "        h, w, _ = image.shape\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            # Convert to absolute coordinates\n",
    "            x1 = int(bbox.xmin * w)\n",
    "            y1 = int(bbox.ymin * h)\n",
    "            x2 = int((bbox.xmin + bbox.width) * w)\n",
    "            y2 = int((bbox.ymin + bbox.height) * h)\n",
    "            \n",
    "            # Store as [x1, y1, x2, y2, confidence, class_id]\n",
    "            confidence = detection.score[0]\n",
    "            face_boxes.append([x1, y1, x2, y2, confidence, 'face'])\n",
    "    \n",
    "    return face_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2660dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_detection(image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Perform combined object detection (YOLO) and face detection (MediaPipe)\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # YOLO object detection (excluding person class to avoid overlap)\n",
    "    yolo_results = yolo_model.predict(\n",
    "        source=image,\n",
    "        conf=conf_threshold,\n",
    "        classes=[i for i in range(80) if i != 0],  # All COCO classes except person (class 0)\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Face detection using MediaPipe\n",
    "    face_boxes = detect_faces_mediapipe(image)\n",
    "    print(image, yolo_results, face_boxes)\n",
    "    return image, yolo_results, face_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba51b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(image, yolo_results, face_boxes):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes for both YOLO detections and face detections\n",
    "    \"\"\"\n",
    "    # Create a copy of the image\n",
    "    result_image = image.copy()\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    if yolo_results and len(yolo_results[0].boxes) > 0:\n",
    "        for box in yolo_results[0].boxes:\n",
    "            # Get box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            \n",
    "            # Draw bounding box (blue for YOLO objects)\n",
    "            cv2.rectangle(result_image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            \n",
    "            # Draw label\n",
    "            label = f\"{class_name}: {confidence:.2f}\"\n",
    "            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "            cv2.rectangle(result_image, (x1, y1 - label_size[1] - 10), \n",
    "                         (x1 + label_size[0], y1), (255, 0, 0), -1)\n",
    "            cv2.putText(result_image, label, (x1, y1 - 5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw face detections\n",
    "    for face_box in face_boxes:\n",
    "        x1, y1, x2, y2, confidence, class_name = face_box\n",
    "        \n",
    "        # Draw bounding box (red for faces)\n",
    "        cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        label = f\"face: {confidence:.2f}\"\n",
    "        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "        cv2.rectangle(result_image, (x1, y1 - label_size[1] - 10), \n",
    "                     (x1 + label_size[0], y1), (0, 0, 255), -1)\n",
    "        cv2.putText(result_image, label, (x1, y1 - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_and_faces(image_path, save_path=None, show_result=True):\n",
    "    \"\"\"\n",
    "    Complete pipeline for object and face detection\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    \n",
    "    # Perform combined detection\n",
    "    image, yolo_results, face_boxes = combined_detection(image_path, conf_threshold=0.5)\n",
    "    \n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Draw detections\n",
    "    result_image = draw_detections(image, yolo_results, face_boxes)\n",
    "    \n",
    "    # Print detection summary\n",
    "    yolo_count = len(yolo_results[0].boxes) if yolo_results and yolo_results[0].boxes is not None else 0\n",
    "    face_count = len(face_boxes)\n",
    "    \n",
    "    print(f\"Detected {yolo_count} objects and {face_count} faces\")\n",
    "    \n",
    "    # Display detected classes\n",
    "    if yolo_results and len(yolo_results[0].boxes) > 0:\n",
    "        detected_classes = [yolo_model.names[int(box.cls[0])] for box in yolo_results[0].boxes]\n",
    "        print(f\"Objects detected: {set(detected_classes)}\")\n",
    "    \n",
    "    # Save result if path provided\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, result_image)\n",
    "        print(f\"Result saved to: {save_path}\")\n",
    "    \n",
    "    # Show result\n",
    "    if show_result:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Object Detection (Blue) + Face Detection (Red)')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for single image\n",
    "image_path = \"bus.jpg\"  # Replace with your image path\n",
    "result = detect_objects_and_faces(\n",
    "    image_path=image_path,\n",
    "    save_path=\"result_with_faces.jpg\",\n",
    "    show_result=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95208f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_detection(video_source, yolo_model):\n",
    "    \"\"\"\n",
    "    Real-time detection from webcam or video file\n",
    "    video_source: 0 for webcam, or path to video file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    yolo_model = YOLO(\"yolo11n.pt\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # YOLO detection (excluding person class)\n",
    "        yolo_results = yolo_model.predict(\n",
    "            source=frame,\n",
    "            conf=0.5,\n",
    "            classes=[i for i in range(80) if i != 0],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Face detection\n",
    "        face_boxes = detect_faces_mediapipe(frame)\n",
    "        \n",
    "        # Draw detections\n",
    "        result_frame = draw_detections(frame, yolo_results, face_boxes)\n",
    "        \n",
    "        # Display\n",
    "        cv2.imshow('Object + Face Detection', result_frame)\n",
    "        \n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Uncomment to run real-time detection\n",
    "real_time_detection(\"./Test-Video-And-Images/istockphoto-2207974714-640_adpp_is.mp4\", yolo_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6dd337d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Press 'q' to quit\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_faces_mediapipe(frame):\n",
    "    \"\"\"\n",
    "    Detect faces using MediaPipe\n",
    "    Returns list of face bounding boxes in format [x, y, w, h]\n",
    "    \"\"\"\n",
    "    face_boxes = []\n",
    "    \n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(rgb_frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            h, w, _ = frame.shape\n",
    "            for detection in results.detections:\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x = int(bbox.xmin * w)\n",
    "                y = int(bbox.ymin * h)\n",
    "                width = int(bbox.width * w)\n",
    "                height = int(bbox.height * h)\n",
    "                face_boxes.append([x, y, width, height])\n",
    "    \n",
    "    return face_boxes\n",
    "\n",
    "def draw_detections(frame, yolo_results, face_boxes):\n",
    "    \"\"\"\n",
    "    Draw YOLO detections and face detections on frame\n",
    "    \"\"\"\n",
    "    result_frame = frame.copy()\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    if yolo_results and len(yolo_results) > 0:\n",
    "        for result in yolo_results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                confidences = result.boxes.conf.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy()\n",
    "                \n",
    "                for box, conf, cls in zip(boxes, confidences, classes):\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    \n",
    "                    # Get class name\n",
    "                    class_name = result.names[int(cls)] if hasattr(result, 'names') else f\"Class {int(cls)}\"\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Draw label\n",
    "                    label = f\"{class_name}: {conf:.2f}\"\n",
    "                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                    cv2.rectangle(result_frame, (x1, y1 - label_size[1] - 10), \n",
    "                                (x1 + label_size[0], y1), (0, 255, 0), -1)\n",
    "                    cv2.putText(result_frame, label, (x1, y1 - 5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    \n",
    "    # Draw face detections\n",
    "    for face_box in face_boxes:\n",
    "        x, y, w, h = face_box\n",
    "        # Draw face bounding box in red\n",
    "        cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw face label\n",
    "        cv2.rectangle(result_frame, (x, y - 25), (x + 60, y), (0, 0, 255), -1)\n",
    "        cv2.putText(result_frame, \"Face\", (x + 5, y - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return result_frame\n",
    "\n",
    "def real_time_detection(video_source):\n",
    "    \"\"\"\n",
    "    Real-time detection from webcam or video file\n",
    "    video_source: 0 for webcam, or path to video file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {video_source}\")\n",
    "        return\n",
    "    \n",
    "    model = YOLO('yolo11n.pt')\n",
    "    \n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or failed to read frame\")\n",
    "            break\n",
    "        \n",
    "        # YOLO detection (excluding person class - class 0)\n",
    "        # Run YOLO detection, exclude person class (0)\n",
    "        yolo_results = model.predict(\n",
    "            source=frame,\n",
    "            conf=0.5,\n",
    "            classes=[i for i in range(100) if i != 1],  # Exclude person class       \n",
    "            # classes=[i for i in range(2) if i != 0],  # Exclude person class       \n",
    "            # classes=[0],  # Exclude person class\n",
    "            verbose=False\n",
    "        )\n",
    "        # Face detection will still detect person faces separately\n",
    "        \n",
    "        # Face detection\n",
    "        face_boxes = detect_faces_mediapipe(frame)\n",
    "        \n",
    "        # Draw detections\n",
    "        result_frame = draw_detections(frame, yolo_results, face_boxes)\n",
    "        \n",
    "        # Add frame info\n",
    "        cv2.putText(result_frame, f\"Objects: {len(yolo_results[0].boxes) if yolo_results[0].boxes is not None else 0}\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        cv2.putText(result_frame, f\"Faces: {len(face_boxes)}\", \n",
    "                   (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Display\n",
    "        cv2.imshow('Object + Face Detection', result_frame)\n",
    "        \n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    # video_file = \"./Test-Video-And-Images/istockphoto-1442184251-640_adpp_is.mp4\"  # Replace with your video path\n",
    "    video_file = \"./Test-Video-And-Images/istockphoto-1906861996-640_adpp_is.mp4\"  # Replace with your video path\n",
    "    real_time_detection(video_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "0e8f4e48",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Initializing model...\n",
      "Using CPU...\n"
     ]
    }
   ],
   "source": [
    "# Ultralytics  AGPL-3.0 License - https://ultralytics.com/license\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import LOGGER\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "enable_gpu = False  # Set True if running with CUDA\n",
    "model_file = \"yolo11n.pt\"  # Path to model file\n",
    "show_fps = True  # If True, shows current FPS in top-left corner\n",
    "show_conf = False  # Display or hide the confidence score\n",
    "save_video = True  # Set True to save output video\n",
    "video_output_path = \"interactive_tracker_output.avi\"  # Output video file name\n",
    "\n",
    "\n",
    "conf = 0.3  # Min confidence for object detection (lower = more detections, possibly more false positives)\n",
    "iou = 0.3  # IoU threshold for NMS (higher = less overlap allowed)\n",
    "max_det = 20  # Maximum objects per im (increase for crowded scenes)\n",
    "\n",
    "tracker = \"bytetrack.yaml\"  # Tracker config: 'bytetrack.yaml', 'botsort.yaml', etc.\n",
    "track_args = {\n",
    "    \"persist\": True,  # Keep frames history as a stream for continuous tracking\n",
    "    \"verbose\": False,  # Print debug info from tracker\n",
    "}\n",
    "\n",
    "window_name = \"Ultralytics YOLO Interactive Tracking\"  # Output window name\n",
    "\n",
    "LOGGER.info(\" Initializing model...\")\n",
    "if enable_gpu:\n",
    "    LOGGER.info(\"Using GPU...\")\n",
    "    model = YOLO(model_file)\n",
    "    model.to(\"cuda\")\n",
    "else:\n",
    "    LOGGER.info(\"Using CPU...\")\n",
    "    model = YOLO(model_file, task=\"detect\")\n",
    "\n",
    "classes = model.names  # Store model classes names\n",
    "\n",
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/vecteezy_tourists-walking-along-the-street-with-food-bar_28556756.mp4\")  # Replace with video path if needed\n",
    "\n",
    "# Initialize video writer\n",
    "vw = None\n",
    "if save_video:\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "    vw = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "selected_object_id = None\n",
    "selected_bbox = None\n",
    "selected_center = None\n",
    "\n",
    "\n",
    "def get_center(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Calculates the center point of a bounding box.\n",
    "\n",
    "    Args:\n",
    "        x1 (int): Top-left X coordinate.\n",
    "        y1 (int): Top-left Y coordinate.\n",
    "        x2 (int): Bottom-right X coordinate.\n",
    "        y2 (int): Bottom-right Y coordinate.\n",
    "\n",
    "    Returns:\n",
    "        (int, int): Center point (x, y) of the bounding box.\n",
    "    \"\"\"\n",
    "    return (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "\n",
    "def extend_line_from_edge(mid_x, mid_y, direction, img_shape):\n",
    "    \"\"\"\n",
    "    Calculates the endpoint to extend a line from the center toward an image edge.\n",
    "\n",
    "    Args:\n",
    "        mid_x (int): X-coordinate of the midpoint.\n",
    "        mid_y (int): Y-coordinate of the midpoint.\n",
    "        direction (str): Direction to extend ('left', 'right', 'up', 'down').\n",
    "        img_shape (tuple): Image shape in (height, width, channels).\n",
    "\n",
    "    Returns:\n",
    "        (int, int): Endpoint coordinate of the extended line.\n",
    "    \"\"\"\n",
    "    h, w = img_shape[:2]\n",
    "    if direction == \"left\":\n",
    "        return 0, mid_y\n",
    "    if direction == \"right\":\n",
    "        return w - 1, mid_y\n",
    "    if direction == \"up\":\n",
    "        return mid_x, 0\n",
    "    if direction == \"down\":\n",
    "        return mid_x, h - 1\n",
    "    return mid_x, mid_y\n",
    "\n",
    "\n",
    "def draw_tracking_scope(im, bbox, color):\n",
    "    \"\"\"\n",
    "    Draws tracking scope lines extending from the bounding box to image edges.\n",
    "\n",
    "    Args:\n",
    "        im (ndarray): Image array to draw on.\n",
    "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2).\n",
    "        color (tuple): Color in BGR format for drawing.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    mid_top = ((x1 + x2) // 2, y1)\n",
    "    mid_bottom = ((x1 + x2) // 2, y2)\n",
    "    mid_left = (x1, (y1 + y2) // 2)\n",
    "    mid_right = (x2, (y1 + y2) // 2)\n",
    "    cv2.line(im, mid_top, extend_line_from_edge(*mid_top, \"up\", im.shape), color, 2)\n",
    "    cv2.line(im, mid_bottom, extend_line_from_edge(*mid_bottom, \"down\", im.shape), color, 2)\n",
    "    cv2.line(im, mid_left, extend_line_from_edge(*mid_left, \"left\", im.shape), color, 2)\n",
    "    cv2.line(im, mid_right, extend_line_from_edge(*mid_right, \"right\", im.shape), color, 2)\n",
    "\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    \"\"\"\n",
    "    Handles mouse click events to select an object for focused tracking.\n",
    "\n",
    "    Args:\n",
    "        event (int): OpenCV mouse event type.\n",
    "        x (int): X-coordinate of the mouse event.\n",
    "        y (int): Y-coordinate of the mouse event.\n",
    "        flags (int): Any relevant flags passed by OpenCV.\n",
    "        param (any): Additional parameters (not used).\n",
    "    \"\"\"\n",
    "    global selected_object_id\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        detections = results[0].boxes.data if results[0].boxes is not None else []\n",
    "        if detections is not None:\n",
    "            min_area = float(\"inf\")\n",
    "            best_match = None\n",
    "            for track in detections:\n",
    "                track = track.tolist()\n",
    "                if len(track) >= 6:\n",
    "                    x1, y1, x2, y2 = map(int, track[:4])\n",
    "                    if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area < min_area:\n",
    "                            class_id = int(track[-1])\n",
    "                            track_id = int(track[4]) if len(track) == 7 else -1\n",
    "                            min_area = area\n",
    "                            best_match = (track_id, model.names[class_id])\n",
    "            if best_match:\n",
    "                selected_object_id, label = best_match\n",
    "                print(f\" TRACKING STARTED: {label} (ID {selected_object_id})\")\n",
    "\n",
    "\n",
    "cv2.namedWindow(window_name)\n",
    "cv2.setMouseCallback(window_name, click_event)\n",
    "\n",
    "fps_counter, fps_timer, fps_display = 0, time.time(), 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    results = model.track(im, conf=conf, iou=iou, max_det=max_det, tracker=tracker, **track_args)\n",
    "    annotator = Annotator(im)\n",
    "    detections = results[0].boxes.data if results[0].boxes is not None else []\n",
    "    detected_objects = []\n",
    "    for track in detections:\n",
    "        track = track.tolist()\n",
    "        if len(track) < 6:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, track[:4])\n",
    "        class_id = int(track[6]) if len(track) >= 7 else int(track[5])\n",
    "        track_id = int(track[4]) if len(track) == 7 else -1\n",
    "        color = colors(track_id, True)\n",
    "        txt_color = annotator.get_txt_color(color)\n",
    "        label = f\"{classes[class_id]} ID {track_id}\" + (f\" ({float(track[5]):.2f})\" if show_conf else \"\")\n",
    "        if track_id == selected_object_id:\n",
    "            draw_tracking_scope(im, (x1, y1, x2, y2), color)\n",
    "            center = get_center(x1, y1, x2, y2)\n",
    "            cv2.circle(im, center, 6, color, -1)\n",
    "\n",
    "            # Pulsing circle for attention\n",
    "            pulse_radius = 8 + int(4 * abs(time.time() % 1 - 0.5))\n",
    "            cv2.circle(im, center, pulse_radius, color, 2)\n",
    "\n",
    "            annotator.box_label([x1, y1, x2, y2], label=f\"ACTIVE: TRACK {track_id}\", color=color)\n",
    "        else:\n",
    "            # Draw dashed box for other objects\n",
    "            for i in range(x1, x2, 10):\n",
    "                cv2.line(im, (i, y1), (i + 5, y1), color, 3)\n",
    "                cv2.line(im, (i, y2), (i + 5, y2), color, 3)\n",
    "            for i in range(y1, y2, 10):\n",
    "                cv2.line(im, (x1, i), (x1, i + 5), color, 3)\n",
    "                cv2.line(im, (x2, i), (x2, i + 5), color, 3)\n",
    "            # Draw label text with background\n",
    "            (tw, th), bl = cv2.getTextSize(label, 0, 0.7, 2)\n",
    "            cv2.rectangle(im, (x1 + 5 - 5, y1 + 20 - th - 5), (x1 + 5 + tw + 5, y1 + 20 + bl), color, -1)\n",
    "            cv2.putText(im, label, (x1 + 5, y1 + 20), 0, 0.7, txt_color, 1, cv2.LINE_AA)\n",
    "\n",
    "    if show_fps:\n",
    "        fps_counter += 1\n",
    "        if time.time() - fps_timer >= 1.0:\n",
    "            fps_display = fps_counter\n",
    "            fps_counter = 0\n",
    "            fps_timer = time.time()\n",
    "\n",
    "        # Draw FPS text with background\n",
    "        fps_text = f\"FPS: {fps_display}\"\n",
    "        cv2.putText(im, fps_text, (10, 25), 0, 0.7, (255, 255, 255), 1)\n",
    "        (tw, th), bl = cv2.getTextSize(fps_text, 0, 0.7, 2)\n",
    "        cv2.rectangle(im, (10 - 5, 25 - th - 5), (10 + tw + 5, 25 + bl), (255, 255, 255), -1)\n",
    "        cv2.putText(im, fps_text, (10, 25), 0, 0.7, (104, 31, 17), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(window_name, im)\n",
    "    if save_video and vw is not None:\n",
    "        vw.write(im)\n",
    "    # Terminal logging\n",
    "    LOGGER.info(f\" DETECTED {len(detections)} OBJECT(S): {' | '.join(detected_objects)}\")\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    elif key == ord(\"c\"):\n",
    "        LOGGER.info(\" TRACKING RESET\")\n",
    "        selected_object_id = None\n",
    "\n",
    "cap.release()\n",
    "if save_video and vw is not None:\n",
    "    vw.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d247ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train specifically for face detection with custom dataset\n",
    "!yolo detect train data=face_dataset.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640 single_cls=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load models\n",
    "yolo_model = YOLO('yolo11n-cls.pt')  # COCO-trained model\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # YOLO object detection\n",
    "    results = yolo_model(image)\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for result in results:\n",
    "        # Draw non-person objects\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls)\n",
    "            conf = float(box.conf)\n",
    "            label = yolo_model.names[cls_id]\n",
    "            \n",
    "            if label != \"person\":\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        # Face detection for person regions\n",
    "        person_boxes = [box.xyxy[0] for box in result.boxes if int(box.cls) == 0]\n",
    "        for person_box in person_boxes:\n",
    "            x1, y1, x2, y2 = map(int, person_box)\n",
    "            padding = 20  # Extra area around person\n",
    "            roi = image[max(0, y1-padding):min(y2+padding, image.shape[0]), \n",
    "                        max(0, x1-padding):min(x2+padding, image.shape[1])]\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, \n",
    "                scaleFactor=1.1, \n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30))\n",
    "            \n",
    "            # Draw face boxes\n",
    "            for (fx, fy, fw, fh) in faces:\n",
    "                # Convert to original image coordinates\n",
    "                abs_x = max(0, x1-padding) + fx\n",
    "                abs_y = max(0, y1-padding) + fy\n",
    "                cv2.rectangle(image, \n",
    "                            (abs_x, abs_y),\n",
    "                            (abs_x+fw, abs_y+fh),\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(image, \"Face\", (abs_x, abs_y-5),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    # Save results in \"python detected images\" folder\n",
    "    output_dir = \"python detected images\"\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "    output_path = os.path.join(output_dir, os.path.basename(image_path).replace('.jpg', '_result.jpg'))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "# if __name__ == \"__main__\":\n",
    "    detect_objects(\"./happy-group-of-business-people-walking-in-front-of-the-office-G24CN2.jpg\")  # Replace with your image path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "def simple_video_detection(video_path):\n",
    "    \"\"\"\n",
    "    Simple video detection using YOLO's built-in capabilities\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = YOLO('yolo11n.pt')\n",
    "    \n",
    "    # Run detection on video\n",
    "    results = model.track(\n",
    "        source=video_path,\n",
    "        show=True,          # Display video\n",
    "        save=True,          # Save results\n",
    "        save_txt=True,      # Save detection coordinates\n",
    "        save_conf=True,     # Save confidence scores\n",
    "        line_width=2,       # Bounding box line width\n",
    "        show_labels=True,   # Show class labels\n",
    "        show_conf=True,     # Show confidence scores\n",
    "        iou=0.10,\n",
    "        conf=0.5,\n",
    "        classes=[0],  # Detect all classes\n",
    "    )\n",
    "    print(\"results====>\", results)\n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
    "    video_file = \"./Test-Video-And-Images/istockphoto-1423119278-640_adpp_is.mp4\"  # Replace with your video path\n",
    "    simple_video_detection(video_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "# import smtplib\n",
    "import pygame\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize YOLO model for object detection\n",
    "# Using YOLOv8n which can detect 80 different object classes\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Initialize MediaPipe for pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Load known faces\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Setup for known faces - replace with your implementation\n",
    "known_faces_dir = \"images\"  # Create this directory and add images\n",
    "if os.path.exists(known_faces_dir):\n",
    "    for person_name in os.listdir(known_faces_dir):\n",
    "        person_dir = os.path.join(known_faces_dir, person_name)\n",
    "        if os.path.isdir(person_dir):\n",
    "            for image_name in os.listdir(person_dir):\n",
    "                image_path = os.path.join(person_dir, image_name)\n",
    "                try:\n",
    "                    image = face_recognition.load_image_file(image_path)\n",
    "                    face_encodings = face_recognition.face_encodings(image)\n",
    "                    if face_encodings:\n",
    "                        known_face_encodings.append(face_encodings[0])\n",
    "                        known_face_names.append(person_name)\n",
    "                        print(f\"Loaded face: {person_name} from {image_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {image_path}: {e}\")\n",
    "\n",
    "# Print summary of loaded faces\n",
    "if known_face_encodings:\n",
    "    print(f\"Successfully loaded {len(known_face_encodings)} face encodings for {len(set(known_face_names))} people\")\n",
    "else:\n",
    "    print(\"Warning: No face encodings loaded. Face recognition will not work.\")\n",
    "\n",
    "# Setup alarm sound\n",
    "pygame.mixer.init()\n",
    "alarm_file = \"pols-aagyi-pols.mp3\"\n",
    "if os.path.exists(alarm_file):\n",
    "    pygame.mixer.music.load(alarm_file)\n",
    "else:\n",
    "    print(f\"Warning: Alarm file {alarm_file} not found\")\n",
    "\n",
    "# Create log directory\n",
    "log_dir = \"security_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def log_event(event_type, details=\"\"):\n",
    "    \"\"\"Log security events to file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_file = os.path.join(log_dir, f\"security_log_{datetime.now().strftime('%Y-%m-%d')}.txt\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"{timestamp} - {event_type}: {details}\\n\")\n",
    "\n",
    "def  send_email_alert(person_name=\"Unknown\", objects_detected=None):\n",
    "    \"\"\"Function to send email alert when a person is detected.\"\"\"\n",
    "    if objects_detected is None:\n",
    "        objects_detected = []\n",
    "    \n",
    "    objects_str = \", \".join(objects_detected) if objects_detected else \"None\"\n",
    "    log_event(\"ALERT_TRIGGERED\", f\"Person: {person_name}, Objects: {objects_str}\")\n",
    "    print(f\"Alert triggered: {person_name} detected with objects: {objects_str}\")\n",
    "\n",
    "# Start Video Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"./Test-Video-And-Images/istockphoto-457334206-640_adpp_is.mp4\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video capture device\")\n",
    "    exit()\n",
    "\n",
    "# Performance optimization variables\n",
    "frame_count = 0\n",
    "face_recognition_interval = 5  # Process face recognition every 5 frames\n",
    "last_alert_time = 0\n",
    "alert_cooldown = 10  # Seconds between alerts\n",
    "\n",
    "# Define objects of interest (subset of COCO classes that YOLO can detect)\n",
    "objects_of_interest = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\", \"mouse\"\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
    "    \"cell phone\", \"laptop\", \"book\", \"scissors\", \"knife\", \"face\"\n",
    "]\n",
    "\n",
    "print(\"Security monitoring started. Press 'q' to quit.\")\n",
    "log_event(\"SYSTEM_START\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Start a high-resolution timer to measure performance or track elapsed time using OpenCV's getTickCount() method\n",
    "        timer = cv2.getTickCount()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        # Determine whether to process faces based on frame count interval\n",
    "        # Helps optimize performance by reducing face recognition computations\n",
    "        process_faces = frame_count % face_recognition_interval == 0\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # YOLO Detection for all objects\n",
    "        results = model(frame)\n",
    "        \n",
    "        # Track detected objects in this frame\n",
    "        detected_objects = []\n",
    "        detected_persons = []\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            for i, box in enumerate(boxes):\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                \n",
    "                # Ensure coordinates are within frame boundaries\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "                \n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue  # Skip invalid boxes\n",
    "                \n",
    "                # Get class and confidence\n",
    "                cls = int(box.cls[0])\n",
    "                conf = float(box.conf[0])\n",
    "                class_name = result.names[cls]\n",
    "                \n",
    "                # Add to detected objects list if it's an object of interest\n",
    "                if class_name in objects_of_interest and class_name != \"person\":\n",
    "                    detected_objects.append(class_name)\n",
    "                    \n",
    "                    # Draw bounding box for object\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                    \n",
    "                    # Display object name and confidence\n",
    "                    label = f\"{class_name}: {conf:.2f}\"\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "                \n",
    "                # Process persons separately for face recognition\n",
    "                if class_name == \"person\":\n",
    "                    detected_persons.append((x1, y1, x2, y2))\n",
    "                    person_roi = frame[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Draw bounding box for person\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "\n",
    "                    # Face Recognition - only process every few frames\n",
    "                    # Conditionally process face recognition only at specified intervals and when a valid person region of interest (ROI) exists\n",
    "                    if process_faces and person_roi.size > 0:\n",
    "                        # Convert the person ROI to RGB format\n",
    "                        # Converts BGR (Blue, Green, Red) color space to RGB (Red, Green, Blue) color space\n",
    "                        # Used for face recognition and comparison\n",
    "                        rgb_small_frame = cv2.cvtColor(person_roi, cv2.COLOR_BGR2RGB)\n",
    "                       \n",
    "                        # Resize the person region of interest to a smaller scale for faster face recognition processing\n",
    "                        # Reduces image dimensions to 25% of the original size using bilinear interpolation\n",
    "                        # Helps improve performance by reducing computational complexity of face detection\n",
    "                        small_frame = cv2.resize(person_roi, (0, 0), fx=0.25, fy=0.25)\n",
    "                        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                                                \n",
    "                        \n",
    "                        # Detect face locations in a resized RGB frame using face_recognition library\n",
    "                        # Identifies and returns the bounding box coordinates of faces in the input image\n",
    "                        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "                        # Identifies face locations and generates corresponding face encodings\n",
    "                        \n",
    "                        if face_locations:\n",
    "                            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "                            \n",
    "                            for face_encoding in face_encodings:\n",
    "                                if known_face_encodings:  # Only compare if we have known faces\n",
    "                                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                                    \n",
    "                                    if any(matches):\n",
    "                                        # Find the name of the matched person\n",
    "                                        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                                        best_match_index = np.argmin(face_distances)\n",
    "                                        if matches[best_match_index]:\n",
    "                                            name = known_face_names[best_match_index]\n",
    "                                            \n",
    "                                            print(f\" Known Person Detected: {name}\")\n",
    "                                            cv2.putText(frame, f\"KNOWN: {name}\", (x1, y1 - 10), \n",
    "                                                      cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                                            \n",
    "                                            # Alert with cooldown\n",
    "                                            if current_time - last_alert_time > alert_cooldown:\n",
    "                                                if not pygame.mixer.music.get_busy():\n",
    "                                                    pygame.mixer.music.play()\n",
    "                                                send_email_alert(name, detected_objects)\n",
    "                                                log_event(\"KNOWN_PERSON\", f\"Detected: {name} with objects: {', '.join(detected_objects) if detected_objects else 'None'}\")\n",
    "                                                last_alert_time = current_time\n",
    "                                    else:\n",
    "                                        print(\" Unknown Person Detected!\")\n",
    "                                        cv2.putText(frame, \"UNKNOWN\", (x1, y1 - 10), \n",
    "                                                  cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                                        log_event(\"UNKNOWN_PERSON\", f\"With objects: {', '.join(detected_objects) if detected_objects else 'None'}\")\n",
    "        \n",
    "        # Display detected objects summary\n",
    "        if detected_objects:\n",
    "            objects_text = f\"Objects: {', '.join(set(detected_objects))}\"\n",
    "            cv2.putText(frame, objects_text, (20, 60), \n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "        \n",
    "        # Calculate and display FPS\n",
    "        fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "        cv2.putText(frame, f\"FPS: {int(fps)}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Security Monitoring', frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    log_event(\"SYSTEM_ERROR\", str(e))\n",
    "finally:\n",
    "    # Clean up resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pose.close()  # Close MediaPipe resources\n",
    "    pygame.mixer.quit()\n",
    "    log_event(\"SYSTEM_SHUTDOWN\")\n",
    "    print(\"Security monitoring stopped.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4488795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "# Open the video file\n",
    "cap = cv2.VideoCapture(\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\")\n",
    "# cap = cv2.VideoCapture(\"Test-Video-And-Images/istock-photos/istock-1326466261.mp4\")\n",
    "assert cap.isOpened(), \"Error: Could not open video file. Please check if the file path is correct and the file exists.\"\n",
    "# Get video properties: width, height, and frames per second (fps)\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define points for a line or region of interest in the video frame\n",
    "line_points = [(0, 100), (1080, 100)]  # Line coordinates near top of screen\n",
    "\n",
    "# Initialize the video writer to save the output video\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize the Object Counter with visualization options and other parameters\n",
    "counter = solutions.ObjectCounter(\n",
    "    show=True,  # Display the image during processing\n",
    "    region=line_points,  # Region of interest points\n",
    "    model=\"yolo11x.pt\",  # Ultralytics YOLO11 model file\n",
    "    line_width=2,  # Thickness of the lines and bounding boxes\n",
    ")\n",
    "\n",
    "# Process video frames in a loop\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    # Use the Object Counter to count objects in the frame and get the annotated image\n",
    "    results = counter(im0)\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12d307",
   "metadata": {},
   "source": [
    "### Run YOLO11 tracking on the frame, persisting tracks between frames intersecting not provide different IDs for tracking using iou=0.10 value for the intersection over union threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fff917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"Test-Video-And-Images/istockphoto-1139869319-640_adpp_is.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames intersecting not provide different IDs\n",
    "        results = model.track(\n",
    "            frame, \n",
    "            persist=True,\n",
    "            classes=[0], \n",
    "            iou=0.10, \n",
    "            conf=0.5\n",
    "            )\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89a2c783",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov11_ext",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
