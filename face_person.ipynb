{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23e37578",
   "metadata": {},
   "outputs": [],
   "source": [
    "pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec80774c",
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d62896e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bdf0139",
   "metadata": {},
   "outputs": [],
   "source": [
    "import dlib\n",
    "import face_recognition\n",
    "import torch\n",
    "import ultralytics\n",
    "print(f\"Dlib: {dlib.__version__}\")\n",
    "print(f\"Face-Recognition: {face_recognition.__version__}\")\n",
    "print(f\"PyTorch: {torch.__version__}\")\n",
    "print(f\"Ultralytics: {ultralytics.__version__}\")\n",
    "\n",
    "import mediapipe\n",
    "print(f\"mediapipe: {mediapipe.__version__}\")\n",
    "\n",
    "import pygame\n",
    "print(f\"pygame: {pygame.__version__}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e8e0d06",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Display current Ultralytics YOLO settings.\n",
    "\n",
    "This command shows the current configuration settings for the Ultralytics YOLO framework.\n",
    "\"\"\"\n",
    "!yolo settings\n",
    "\n",
    "# Reset settings to default values\n",
    "!yolo settings reset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ab5ffe",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import settings\n",
    "\n",
    "# View all settings\n",
    "print(settings)\n",
    "\n",
    "# Return a specific setting\n",
    "value = settings[\"datasets_dir\"]\n",
    "print(f\"Datasets directory: {value}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d90dbd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "print(f\"Python version: {sys.version}\")\n",
    "print(f\"Python path: {sys.executable}\")\n",
    "\n",
    "# Check CUDA version if available\n",
    "import torch\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
    "if torch.cuda.is_available():\n",
    "\tprint(f\"CUDA version: {torch.version.cuda}\")\n",
    "\tprint(f\"Current CUDA device: {torch.cuda.current_device()}\")\n",
    "\tprint(f\"Device name: {torch.cuda.get_device_name()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92887475",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import os\n",
    "# HOME = os.getcwd()\n",
    "# print(HOME)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d78f36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training from a pretrained *.pt model\n",
    "!yolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n",
    "# !yolo detect train data=coco8.yaml model=yolo11x.pt epochs=100 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9dac860e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# # model = YOLO(\"yolo11n.yaml\")  # build a new model from YAML\n",
    "# model = YOLO(\"yolo11n.pt\")  # load a pretrained model (recommended for training)\n",
    "# # model = YOLO(\"yolo11n.yaml\").load(\"yolo11n.pt\")  # build from YAML and transfer weights\n",
    "\n",
    "# # Train the model\n",
    "# results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fd821ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo detect val model=yolo11n.pt      # val official model\n",
    "# !yolo detect val model=path/to/best.pt # val custom model\n",
    "!yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640\n",
    "# !yolo val model=yolo11x.pt data=coco8.yaml batch=1 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d797408",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load a pretrained YOLO11n model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Train the model on COCO8\n",
    "results = model.train(data=\"coco8.yaml\", epochs=100, imgsz=640)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f01d5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "# # model = YOLO(\"path/to/best.pt\")  # load a custom model\n",
    "\n",
    "# # Validate the model\n",
    "# metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "# metrics.box.map  # map50-95\n",
    "# metrics.box.map50  # map50\n",
    "# metrics.box.map75  # map75\n",
    "# metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52500993",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from ultralytics import YOLO\n",
    "\n",
    "# # Load a model\n",
    "# model = YOLO(\"yolo11n.pt\")  # load an official model\n",
    "# model = YOLO(\"./runs/detect/train/weights/best.pt\")  # load a custom model\n",
    "\n",
    "# # Validate the model\n",
    "# metrics = model.val()  # no arguments needed, dataset and settings remembered\n",
    "# metrics.box.map  # map50-95\n",
    "# metrics.box.map50  # map50\n",
    "# metrics.box.map75  # map75\n",
    "# metrics.box.maps  # a list contains map50-95 of each category"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "803d6ea3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Build a new model from YAML and start training from scratch\n",
    "# !yolo detect train data=coco8.yaml model=yolo11n.yaml epochs=100 imgsz=640\n",
    "\n",
    "# # Start training from a pretrained *.pt model\n",
    "# !yolo detect train data=coco8.yaml model=yolo11n.pt epochs=100 imgsz=640\n",
    "\n",
    "# # Build a new model from YAML, transfer pretrained weights to it and start training\n",
    "# !yolo detect train data=coco8.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b56c84f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo train data=coco8.yaml model=yolo11n.pt epochs=10 lr0=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6597b39f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo val model=yolo11n.pt data=coco8.yaml batch=1 imgsz=640"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c1e07bf5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo detect val model=yolo11n.pt\n",
    "# !yolo detect val model=runs/detect/train/weights/best.pt # val custom model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11ee62f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# model = YOLO('yolo11n.pt')  # load a pretrained YOLO detection model\n",
    "model = YOLO('runs/detect/train/weights/best.pt')  # load a pretrained YOLO detection model\n",
    "# model.train(data='coco8.yaml', epochs=3)  # train the model\n",
    "# model('https://ultralytics.com/images/bus.jpg')  # predict on an image\n",
    "# results = model.predict(source=\"./testImages/vid1.mp4\", show=True)  \n",
    "# model.predict(\"https://media.roboflow.com/notebooks/examples/dog.jpeg\", save=True, imgsz=640, conf=0.25)\n",
    "model.predict(\"https://media.roboflow.com/notebooks/examples/dog.jpeg\", save=True, conf=0.25)\n",
    "# results = model.export(format='onnx')  # export the model to ONNX format# print(results)\n",
    "# cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5b84ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load YOLO11n, train it on COCO128 for 3 epochs and predict an image with it\n",
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "model = YOLO('yolo11m.pt')  # load a pretrained YOLO detection model\n",
    "# model = YOLO('runs/detect/train2/weights/best.pt')  # load a pretrained YOLO detection model\n",
    "# model.train(data='coco8.yaml', epochs=3)  # train the model\n",
    "\n",
    "# results = model.val()  # evaluate model performance on the validation set\n",
    "# results = model.predict('./testImages/vid1.mp4', show=True)  # predict on an image\n",
    "# results = model.predict('./testImages/vid1.mp4', show=True)  # predict on an image\n",
    "\n",
    "# results = model.export(format='onnx')  # export the model to ONNX format\n",
    "\n",
    "\n",
    "# Run inference on 'bus.jpg' with arguments\n",
<<<<<<< HEAD
    "# results = model.predict(\"./media_files/ice skatting object traking/computer_vision_object_and_detection_tracking_ice_skatting_object_traking_video_20250819_173636_10.mp4\", show=True, conf=0.25, iou=0.99, imgsz=640)\n",
    "results = model.predict(\"./media_files/animal_surveillance/goru-churi.mp4\", show=True, conf=0.25, iou=0.99, imgsz=640)\n",
=======
    "results = model.predict(\"./Test-Video-And-Images/istockphoto-2174886250-640_adpp_is.mp4\", show=True, conf=0.25, iou=0.99, imgsz=640)\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "for r in results:\n",
    "    print(\"Boxes:\", r.boxes)  # print the detection boxes\n",
    "    print(\"Masks:\", r.masks)  # print the masks\n",
    "    print(\"Keypoints:\", r.keypoints)  # print the keypoints\n",
    "    print(\"Probs:\", r.probs)  # print the probabilities\n",
    "    print(\"Speed:\", r.speed)  # print inference speed\n",
    "print(\"result =>\", results)\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "4f4edd65",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO('runs/detect/train2/weights/best.pt')  # Load your model\n",
    "video_path = './Test-Video-And-Images/istockphoto-2150199976-640_adpp_is.mp4'\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Get video's native resolution\n",
    "orig_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "orig_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "aspect_ratio = orig_width / orig_height\n",
    "\n",
    "# Set your base dimension (choose one)\n",
    "base_width = 800  # Fixed width, auto height\n",
    "# base_height = 600  # Fixed height, auto width\n",
    "\n",
    "# Calculate dimensions (choose one method)\n",
    "# Option 1: Fixed width, auto height\n",
    "display_width = base_width\n",
    "display_height = int(base_width / aspect_ratio)\n",
    "\n",
    "# Option 2: Fixed height, auto width\n",
    "# display_height = base_height\n",
    "# display_width = int(base_height * aspect_ratio)\n",
    "\n",
    "window_name = \"YOLOv11x Inference\"\n",
    "cv2.namedWindow(window_name, cv2.WINDOW_NORMAL)\n",
    "cv2.resizeWindow(window_name, display_width, display_height)\n",
    "\n",
    "while cap.isOpened():\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    # Run inference\n",
    "    results = model(frame, imgsz=640)\n",
    "    \n",
    "    # Get annotated frame\n",
    "    annotated_frame = results[0].plot()\n",
    "    print(f\"Annotated frame shape: {annotated_frame}\")\n",
    "    \n",
    "    # Show results\n",
    "    cv2.imshow(window_name, annotated_frame)\n",
    "    \n",
    "    if cv2.waitKey(1) == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
   "id": "404039be",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
<<<<<<< HEAD
=======
   "id": "15f3507e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"https://youtu.be/LNwODJXcvt4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Store the track history\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        result = model.track(frame, persist=True, conf=0.3, iou=0.5, tracker=\"botsort.yaml\")[0]\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        if result.boxes and result.boxes.is_track:\n",
    "            boxes = result.boxes.xywh.cpu()\n",
    "            track_ids = result.boxes.id.int().cpu().tolist()\n",
    "\n",
    "            # Visualize the result on the frame\n",
    "            frame = result.plot()\n",
    "\n",
    "            # Plot the tracks\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                x, y, w, h = box\n",
    "                track = track_history[track_id]\n",
    "                track.append((float(x), float(y)))  # x, y center point\n",
    "                if len(track) > 30:  # retain 30 tracks for 30 frames\n",
    "                    track.pop(0)\n",
    "\n",
    "                # Draw the tracking lines\n",
    "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
   "id": "67214ea7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "model = YOLO(\"yolo11n.pt\")\n",
<<<<<<< HEAD
    "video_path = \"/media_files/ice skatting object traking/computer_vision_object_and_detection_tracking_ice_skatting_object_traking_video_20250819_173636_8.mp4\"\n",
=======
    "video_path = \"./Test-Video-And-Images/istockphoto-1139869319-640_adpp_is.mp4\"\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "cap = cv2.VideoCapture(video_path)\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, frame = cap.read()\n",
    "    if success:\n",
<<<<<<< HEAD
    "        results = model.track(frame, show=True, persist=True, conf=0.5, iou=0.5, classes=0, tracker='botsort.yaml')\n",
=======
    "        results = model.track(frame, persist=True, conf=0.5, iou=0.5, classes=0, tracker='botsort.yaml')\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "        boxes = results[0].boxes.xywh.cpu()\n",
    "        track_ids = results[0].boxes.id.int().cpu().tolist()\n",
    "        annotated_frame = results[0].plot()\n",
    "        for box, track_id in zip(boxes, track_ids):\n",
    "            x, y, w, h = box\n",
    "            track = track_history[track_id]\n",
    "            track.append((float(x), float(y)))\n",
    "            if len(track) > 30:\n",
    "                track.pop(0)\n",
    "            points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "            cv2.polylines(annotated_frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aefbf083",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./media_files/apple counting/computer_vision_object_and_detection_tracking_apple_counting_video_20250819_173636_1.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define region points\n",
    "# region_points = [(150, 150), (1130, 150), (1130, 570), (150, 570)]\n",
    "region_points = [(0, 0), (1130, 0), (1130, 480), (0, 480)]\n",
    "\n",
    "# Video writer\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Init trackzone (object tracking in zones, not complete frame)\n",
    "trackzone = solutions.TrackZone(\n",
    "    show=True,  # display the output\n",
    "    region=region_points,  # pass region points\n",
    "    model=\"yolo11n.pt\",\n",
    "    classes=[0, 47],\n",
    "    conf=0.5,\n",
    "    iou=0.99,\n",
    "    tracker='bytetrack.yaml',\n",
    "    # track_high_thresh=0.5\n",
    "    \n",
    "    # tracker=\"bytetrack.yaml\",  # use ByteTrack for tracking\n",
    "    # track_high_thresh=0.5\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "    results = trackzone(im0)\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c411ebc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/media_files/ice skatting object traking/computer_vision_object_and_detection_tracking_ice_skatting_object_traking_video_20250819_173636_6.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Store the track history\n",
    "track_history = defaultdict(lambda: [])\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames\n",
    "        result = model.track(frame, show=True, persist=True, conf=0.5, iou=0.5, max_det=1000, classes=[0])[0]\n",
    "\n",
    "        # Get the boxes and track IDs\n",
    "        if result.boxes and result.boxes.is_track:\n",
    "            boxes = result.boxes.xywh.cpu()\n",
    "            track_ids = result.boxes.id.int().cpu().tolist()\n",
    "\n",
    "            # Visualize the result on the frame\n",
    "            frame = result.plot()\n",
    "\n",
    "            # Plot the tracks\n",
    "            for box, track_id in zip(boxes, track_ids):\n",
    "                x, y, w, h = box\n",
    "                track = track_history[track_id]\n",
    "                track.append((float(x), float(y)))  # x, y center point\n",
    "                if len(track) > 30:  # retain 30 tracks for 30 frames\n",
    "                    track.pop(0)\n",
    "\n",
    "                # Draw the tracking lines\n",
    "                points = np.hstack(track).astype(np.int32).reshape((-1, 1, 2))\n",
    "                cv2.polylines(frame, [points], isClosed=False, color=(230, 230, 230), thickness=10)\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c221c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"Test-Video-And-Images/istockphoto-1139869319-640_adpp_is.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"instance-segmentation.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Init InstanceSegmentation\n",
    "isegment = solutions.InstanceSegmentation(\n",
    "    show=True,  # display the output\n",
    "    model=\"yolo11n-seg.pt\",  # model=\"yolo11n-seg.pt\" for object segmentation using YOLO11.\n",
    "    classes=[0],\n",
    "    conf=0.5,\n",
    "    iou=0.99,\n",
    "    \n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "    results = isegment(im0)\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a726898b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "# cap = cv2.VideoCapture(\"./Test-Video-And-Images/test.jpeg\")\n",
    "video_path = \"./Test-Video-And-Images/bus.jpg\"  # Or any other valid video file\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "if not cap.isOpened():\n",
    "    print(f\"Error: Could not open video file: {video_path}\")\n",
    "    exit()\n",
    "\n",
    "# Initialize object cropper object\n",
    "cropper = solutions.ObjectCropper(\n",
    "    show=True,  # display the output with bounding boxes\n",
    "    model=\"runs/detect/train2/weights/best.pt\",  # use trained model\n",
    "    conf=0.45,  # confidence threshold\n",
    "    iou=0.99,  # intersection over union threshold\n",
    "    crop_dir=\"cropped-detections\",  # output directory for cropped detections \n",
    "    line_width=2,  # thicker bounding box lines for better visibility\n",
    "    show_labels=True,  # display labels on bounding boxes\n",
    "    \n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = cropper(im0)\n",
    "\n",
    "    print(results)  # access the output\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d03add51",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/20gillisWeb-videoSixteenByNine3000.jpg\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# region_points = [(10, 200), (1080, 200)]                                      # line counting\n",
    "# region_points = [(0, 200), (800, 200)]                                     # line counting\n",
    "# region_points = [(0, 210), (1080, 210), (1080, 400), (0, 400)]  # rectangle region\n",
    "# region_points = [(0, 200), (1080, 200), (1080, 150), (0, 150), (0, 150)]   # polygon region\n",
    "\n",
    "\n",
    "# region_points = [(20, 400), (1080, 400)]                                      # line counting\n",
    "region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]  # rectangle region\n",
    "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]   # polygon region\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize object counter object\n",
    "counter = solutions.ObjectCounter(\n",
    "    show=True,  # display the output\n",
    "    region=region_points,  # pass region points\n",
    "    model=\"yolo11n-obb.pt\",  # model=\"yolo11n-obb.pt\" for object counting with OBB model.\n",
    "    # classes=[0, 2],  # count specific classes i.e. person and car with COCO pretrained model.\n",
    "    tracker=\"botsort.yaml\",  # choose trackers i.e \"bytetrack.yaml\"\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = counter(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9fbb787",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo solutions trackzone source=\"path/to/video.mp4\" show=True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "620fc15b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./media_files/apple counting/computer_vision_object_and_detection_tracking_apple_counting_video_20250819_173636_1.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# region_points = [(20, 400), (1080, 400)]                                      # line counting\n",
    "region_points = [(0, 200), (700, 200), (700, 260), (0, 260)]  # rectangle region\n",
    "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360), (20, 400)]   # polygon region\n",
    "# region_points = [(200, 0), (200, 420),(260, 420), (260, 0)]  # rectangle region\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize object counter object\n",
    "counter = solutions.ObjectCounter(\n",
    "    show=True,  # display the output\n",
    "    region=region_points,  # pass region points\n",
    "    model=\"./runs/detect/train2/weights/best.pt\",  # model=\"yolo11n-obb.pt\" for object counting with OBB model.\n",
    "    # classes=[0, 2],  # count specific classes i.e. person and car with COCO pretrained model.\n",
    "    classes=[47],  # count specific classes i.e. person and car with COCO pretrained model.\n",
    "    # tracker=\"botsort.yaml\",  # choose trackers i.e \"bytetrack.yaml\"\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = counter(im0)\n",
    "\n",
    "    print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd623495",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "\n",
    "def count_objects_in_region(video_path, output_video_path, model_path):\n",
    "    \"\"\"Count objects in a specific region within a video.\"\"\"\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    assert cap.isOpened(), \"Error reading video file\"\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "    video_writer = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "    region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n",
    "    counter = solutions.ObjectCounter(show=True, region=region_points, model=model_path)\n",
    "\n",
    "    while cap.isOpened():\n",
    "        success, im0 = cap.read()\n",
    "        if not success:\n",
    "            print(\"Video frame is empty or processing is complete.\")\n",
    "            break\n",
    "        results = counter(im0)\n",
    "        video_writer.write(results.plot_im)\n",
    "\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "\n",
    "count_objects_in_region(\"./media_files/apple counting/computer_vision_object_and_detection_tracking_apple_counting_video_20250819_173636_1.mp4\", \"output_video.avi\", \"yolo11n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8276e9ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import cv2\n",
    "from time import time\n",
    "from ultralytics import YOLO\n",
    "\n",
    "import supervision as sv\n",
    "\n",
    "\n",
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "\n",
    "password = \"your_email_password\"\n",
    "from_email = \"rahatansari.tpu@gmail.com\"\n",
    "to_email = \"deveansari@gmail.com\"\n",
    "\n",
    "    \n",
    "    \n",
    "# create server\n",
    "server = smtplib.SMTP('smtp.gmail.com: 587')\n",
    "\n",
    "server.starttls()\n",
    "\n",
    "# Login Credentials for sending the mail\n",
    "server.login(from_email, \"nuzb vkot eauw qihl\")\n",
    "\n",
    "    \n",
    "def send_email(to_email, from_email, people_detected=1):\n",
    " \n",
    "    message = MIMEMultipart()\n",
    "    message['From'] = from_email\n",
    "    message['To'] = to_email\n",
    "    message['Subject'] = \"Security Alert\"\n",
    "    # add in the message body\n",
    "    message.attach(MIMEText(f'ALERT - {people_detected} persons has been detected!!', 'plain'))\n",
    "    server.sendmail(from_email, to_email, message.as_string())\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "class ObjectDetection:\n",
    "\n",
    "    def __init__(self, capture_index):\n",
    "       \n",
    "        self.capture_index = capture_index\n",
    "        \n",
    "        self.email_sent = False\n",
    "        \n",
    "        self.device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "        print(\"Using Device: \", self.device)\n",
    "        \n",
    "        self.model = self.load_model()\n",
    "        \n",
    "        self.CLASS_NAMES_DICT = self.model.model.names\n",
    "    \n",
    "        self.box_annotator = sv.BoxAnnotator(color=sv.ColorPalette.default(), thickness=3, text_thickness=3, text_scale=1.5)\n",
    "    \n",
    "\n",
    "    def load_model(self):\n",
    "        try:\n",
    "            model = YOLO(\"yolo11n.pt\")  # load a pretrained YOLOv8n model\n",
    "            model.fuse()\n",
    "            return model\n",
    "        except Exception as e:\n",
    "            print(f\"Error loading model: {e}\")\n",
    "            raise\n",
    "\n",
    "\n",
    "    def predict(self, frame):\n",
    "       \n",
    "        results = self.model(frame, show=True)\n",
    "        \n",
    "        return results\n",
    "    def plot_bboxes(self, results, frame):\n",
    "        \n",
    "        xyxys = []\n",
    "        confidences = []\n",
    "        class_ids = []\n",
    "        \n",
    "        if len(results) > 0:\n",
    "            # Extract detections for person class\n",
    "            for result in results[0]:\n",
    "                class_id = result.boxes.cls.cpu().numpy().astype(int)\n",
    "                \n",
    "                if class_id == 0:  # person class\n",
    "                    xyxys.append(result.boxes.xyxy.cpu().numpy())\n",
    "                    confidences.append(result.boxes.conf.cpu().numpy())\n",
    "                    class_ids.append(class_id)\n",
    "            \n",
    "            # Setup detections for visualization\n",
    "            detections = sv.Detections.from_ultralytics(results[0])\n",
    "            frame = self.box_annotator.annotate(scene=frame, detections=detections)\n",
    "        \n",
    "        return frame, class_ids\n",
    "        \n",
    "        return frame, class_ids\n",
    "    \n",
    "    \n",
    "    \n",
    "    def __call__(self):\n",
    "\n",
    "        cap = cv2.VideoCapture(self.capture_index)\n",
    "        assert cap.isOpened()\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, 640)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, 480)\n",
    "        \n",
    "        frame_count = 0\n",
    "      \n",
    "        while True:\n",
    "          \n",
    "            start_time = time()\n",
    "            \n",
    "            ret, frame = cap.read()\n",
    "            assert ret\n",
    "            \n",
    "            results = self.predict(frame)\n",
    "            frame, class_ids = self.plot_bboxes(results, frame)\n",
    "            \n",
    "            if len(class_ids) > 0:\n",
    "                if not self.email_sent:  # Only send email if it hasn't been sent for the current detection\n",
    "                    # send_email(to_email, from_email, len(class_ids))\n",
    "                    self.email_sent = True  # Set the flag to True after sending the email\n",
    "            else:\n",
    "                self.email_sent = False  # Reset the flag when no person is detected\n",
    "\n",
    "            \n",
    "            end_time = time()\n",
    "            fps = 1/np.round(end_time - start_time, 2)\n",
    "             \n",
    "            cv2.putText(frame, f'FPS: {int(fps)}', (20,70), cv2.FONT_HERSHEY_SIMPLEX, 1.5, (0,255,0), 2)\n",
    "            \n",
    "            cv2.imshow('YOLOv8 Detection', frame)\n",
    "            \n",
    "            frame_count += 1\n",
    " \n",
    "            if cv2.waitKey(5) & 0xFF == 27:\n",
    "                \n",
    "                break\n",
    "        \n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        server.quit()\n",
    "        try:\n",
    "            cap.release()\n",
    "            cv2.destroyAllWindows()\n",
    "            if server:\n",
    "                server.quit()\n",
    "        except Exception as e:\n",
    "            print(f\"Error during cleanup: {e}\")\n",
    "detector = ObjectDetection(capture_index=0)\n",
    "detector()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f78db0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo solutions count show=True # for object counting\n",
    "\n",
    "!yolo solutions count show=True source=\"./media_files/conveyer apple and bottle counting/istockphoto-1317593103-640_adpp_is.mp4\" # specify video file path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8b74516",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img --weights \"path/to/yolov8n.pt\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee615840",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./media_files/apple counting/computer_vision_object_and_detection_tracking_apple_counting_video_20250819_173636_1.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"isegment_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize instance segmentation object\n",
    "isegment = solutions.InstanceSegmentation(\n",
    "    show=True,  # display the output\n",
    "    model=\"yolo11n-seg.pt\",  # model=\"yolo11n-seg.pt\" for object segmentation using YOLO11.\n",
    "    # classes=[0, 2],  # segment specific classes i.e, person and car with pretrained model.\n",
    "    line_width=2,  # thicker bounding box lines for better visibility\n",
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    results = isegment(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d9b398",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(\"./media_files/people walking/102455-660253882_small.mp4\")\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Pass region as list\n",
    "# region_points = [(20, 400), (1080, 400), (1080, 360), (20, 360)]\n",
    "\n",
    "# Pass region as dictionary\n",
    "region_points = {\n",
    "    \"region-01\": [(50, 50), (250, 50), (250, 250), (50, 250)],\n",
    "    \"region-02\": [(640, 640), (780, 640), (780, 720), (640, 720)],\n",
    "}\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"region_counting.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize region counter object\n",
    "regioncounter = solutions.RegionCounter(\n",
    "    show=True,  # display the frame\n",
    "    region=region_points,  # pass region points\n",
<<<<<<< HEAD
    "    model=\"yolo11l.pt\",  # model for counting in regions i.e yolo11s.pt\n",
=======
    "    model=\"yolo11s.pt\",  # model for counting in regions i.e yolo11s.pt\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    ")\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or processing is complete.\")\n",
    "        break\n",
    "\n",
    "    results = regioncounter(im0)\n",
    "\n",
    "    # print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d841ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "!python yolov8_region_counter.py --source \"path/to/video.mp4\" --save-img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b67b282",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install inference-cli && inference server start"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5f818ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo export model=yolo11n-cls.pt format=onnx imgsz=224,128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d31ba2d7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %pip install -U ultralytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3b3df8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo export model=yolo11x.pt format=onnx      # export official model\n",
    "# !yolo export model=runs/detect/train2/weights/best.pt format=onnx # export custom trained model\n",
    "from ultralytics import YOLO\n",
    "from PIL import Image\n",
    "import requests\n",
    "\n",
    "# Load a model\n",
    "# model = YOLO(\"yolo11x.pt\")  # load an official model\n",
    "model = YOLO(\"runs/detect/train2/weights/best.pt\")  # load a custom trained model\n",
    "image = Image.open(requests.get('https://media.roboflow.com/notebooks/examples/dog.jpeg', stream=True).raw)\n",
    "result = model.predict(image, save=True, conf=0.25)[0]\n",
    "\n",
    "# Export the model\n",
    "model.export(format=\"onnx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6eb7f8f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run a counting example\n",
    "# !yolo solutions count show=True\n",
    "\n",
    "# Pass a source video\n",
    "# !yolo solutions count source=\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\" show=True\n",
    "\n",
    "# Pass region coordinates\n",
    "!yolo solutions count region=\"[(0, 200), (1080, 200), (1080, 260), (0, 260)]\" source=\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\" show=True"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa265f29",
   "metadata": {},
   "source": [
    "## Draw bounding boxes on the detection results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8221b17",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.boxes.xyxy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff4abf1",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.boxes.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90e00fe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "result.boxes.cls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bb6fd9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install supervision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6b00c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import supervision as sv\n",
    "\n",
    "detections = sv.Detections.from_ultralytics(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "901e8c94",
   "metadata": {},
   "outputs": [],
   "source": [
    "box_annotator = sv.BoxAnnotator()\n",
    "label_annotator = sv.LabelAnnotator(text_color=sv.Color.BLACK)\n",
    "\n",
    "annotated_image = image.copy()\n",
    "annotated_image = box_annotator.annotate(annotated_image, detections=detections)\n",
    "annotated_image = label_annotator.annotate(annotated_image, detections=detections)\n",
    "\n",
    "sv.plot_image(annotated_image, size=(10, 10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb0f7064",
   "metadata": {},
   "source": [
    "## check face recognition between two images barackObama.jpg and tigerWoods.jpg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db96a4a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "image = face_recognition.load_image_file(\"./images/obama/image1.jpg\")\n",
    "\n",
    "cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Face Detection\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "for (top, right, bottom, left) in face_locations:\n",
    "    # Draw a box around the face\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "\n",
    "cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "\n",
    "# Facial Landmarks Detection\n",
    "face_landmarks_list = face_recognition.face_landmarks(image)\n",
    "\n",
    "facial_features = [\n",
    "        'chin',\n",
    "        'left_eyebrow',\n",
    "        'right_eyebrow',\n",
    "        'nose_bridge',\n",
    "        'nose_tip',\n",
    "        'left_eye',\n",
    "        'right_eye',\n",
    "        'top_lip',\n",
    "        'bottom_lip']\n",
    "\n",
    "\n",
    "for face_landmarks in face_landmarks_list:\n",
    "    for facial_feature in facial_features:\n",
    "        for point in face_landmarks[facial_feature]:\n",
    "            image = cv2.circle(image, point, 2, (255,60,170),2)\n",
    "            \n",
    "            \n",
    "cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "\n",
    "# Face Recognition\n",
    "original_image = face_recognition.load_image_file(\"./images/obama/image1.jpg\")\n",
    "unknown_image = face_recognition.load_image_file(\"./images/messi/image2.jpg\")\n",
    "# Get face encodings\n",
    "try:\n",
    "    image_encoding = face_recognition.face_encodings(original_image)[0]\n",
    "    unknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
    "    \n",
    "    # Compare faces\n",
    "    results = face_recognition.compare_faces([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Calculate face distance for more detailed comparison\n",
    "    face_distance = face_recognition.face_distance([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Add text to unknown image\n",
    "    cv2.putText(unknown_image, f'Barack Obama: {results[0]}', (25, 75), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(unknown_image, f'Distance: {face_distance[0]:.2f}', (25, 125), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"Barack Obama\", cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imshow(\"Unknown\", cv2.cvtColor(unknown_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "except IndexError:\n",
    "    print(\"No face found in one or both images\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62366813",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "import numpy as np\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load and display initial image\n",
<<<<<<< HEAD
    "image = face_recognition.load_image_file(\"./images/Bappi/1.jpg\")\n",
=======
    "image = face_recognition.load_image_file(\"./Images/Robin/image2.jpg\")\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "\n",
    "# cv2.imshow(\"Image\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# Face Detection using face_recognition\n",
    "face_locations = face_recognition.face_locations(image)\n",
    "\n",
    "for (top, right, bottom, left) in face_locations:\n",
    "    # Draw a box around the face\n",
    "    cv2.rectangle(image, (left, top), (right, bottom), (0, 255, 0), 2)\n",
    "\n",
    "cv2.imshow(\"Image with Face Detection\", cv2.cvtColor(image, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# Face Mesh Detection using mediapipe\n",
    "mp_face_mesh = mp.solutions.face_mesh\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "mp_drawing_styles = mp.solutions.drawing_styles\n",
    "face_mesh = mp_face_mesh.FaceMesh(static_image_mode=True, max_num_faces=1, refine_landmarks=True)\n",
    "\n",
    "# Reload original image for face mesh (without rectangles)\n",
<<<<<<< HEAD
    "image_mesh = face_recognition.load_image_file(\"./images/Bappi/1.jpg\")\n",
=======
    "image_mesh = face_recognition.load_image_file(\"./Images/Robin/image2.jpg\")\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "\n",
    "# Convert image to RGB for mediapipe (face_recognition loads as RGB by default)\n",
    "results = face_mesh.process(image_mesh)\n",
    "\n",
    "if results.multi_face_landmarks:\n",
    "    for face_landmarks in results.multi_face_landmarks:\n",
    "        # # Draw the face mesh\n",
    "        # mp_drawing.draw_landmarks(\n",
    "        #     image_mesh,\n",
    "        #     face_landmarks,\n",
    "        #     mp_face_mesh.FACEMESH_CONTOURS,\n",
    "        #     None,\n",
    "        #     mp_drawing_styles.get_default_face_mesh_contours_style()\n",
    "        # )\n",
    "        \n",
    "        # Optionally draw tesselation for full mesh\n",
    "        mp_drawing.draw_landmarks(\n",
    "            image_mesh,\n",
    "            face_landmarks,\n",
    "            mp_face_mesh.FACEMESH_TESSELATION,\n",
    "            None,\n",
    "            mp_drawing_styles.get_default_face_mesh_tesselation_style()\n",
    "        )\n",
    "\n",
    "cv2.imshow(\"Face Mesh\", cv2.cvtColor(image_mesh, cv2.COLOR_RGB2BGR))\n",
    "cv2.waitKey(0)\n",
    "\n",
    "# # Alternative: Manual face mesh drawing with custom points\n",
    "# image_manual_mesh = face_recognition.load_image_file(\"./barackObama.jpg\")\n",
    "\n",
    "# if results.multi_face_landmarks:\n",
    "#     for face_landmarks in results.multi_face_landmarks:\n",
    "#         # Convert mediapipe landmarks to pixel coordinates\n",
    "#         h, w, _ = image_manual_mesh.shape\n",
    "#         for lm in face_landmarks.landmark:\n",
    "#             x, y = int(lm.x * w), int(lm.y * h)\n",
    "#             cv2.circle(image_manual_mesh, (x, y), 1, (255, 60, 170), 1)\n",
    "\n",
    "# cv2.imshow(\"Manual Face Mesh Points\", cv2.cvtColor(image_manual_mesh, cv2.COLOR_RGB2BGR))\n",
    "# cv2.waitKey(0)\n",
    "\n",
    "# Face Recognition\n",
<<<<<<< HEAD
    "original_image = face_recognition.load_image_file(\"./images/Bappi/1.jpg\")\n",
    "unknown_image = face_recognition.load_image_file(\"./images/Bappi/2.jpg\")\n",
=======
    "original_image = face_recognition.load_image_file(\"./Images/Robin/image1.jpg\")\n",
    "unknown_image = face_recognition.load_image_file(\"./Images/Robin/image2.jpg\")\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "\n",
    "# Get face encodings\n",
    "try:\n",
    "    image_encoding = face_recognition.face_encodings(original_image)[0]\n",
    "    unknown_encoding = face_recognition.face_encodings(unknown_image)[0]\n",
    "    \n",
    "    # Compare faces\n",
    "    results = face_recognition.compare_faces([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Calculate face distance for more detailed comparison\n",
    "    face_distance = face_recognition.face_distance([image_encoding], unknown_encoding)\n",
    "    \n",
    "    # Add text to unknown image\n",
    "    cv2.putText(unknown_image, f'Answer is Obama: {results[0]}', (25, 75), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    cv2.putText(unknown_image, f'Distance: {face_distance[0]:.2f}', (25, 125), cv2.FONT_HERSHEY_SIMPLEX, 1, (0, 255, 0), 2)\n",
    "    \n",
    "    cv2.imshow(\"answer is \", cv2.cvtColor(original_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.imshow(\"Unknown\", cv2.cvtColor(unknown_image, cv2.COLOR_RGB2BGR))\n",
    "    cv2.waitKey(0)\n",
    "    \n",
    "except IndexError:\n",
    "    print(\"No face found in one or both images\")\n",
    "\n",
    "# Clean up\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95cf3876",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import face_recognition\n",
    "from ultralytics import YOLO\n",
    "import numpy as np\n",
    "import smtplib\n",
    "\n",
    "# Load YOLO model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Load known faces\n",
    "known_face_encodings = [...]  # Preloaded face encodings\n",
    "known_face_names = [...]       # Names of known people\n",
    "\n",
    "# Capture Video\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    results = model(frame)  # YOLO detection\n",
    "\n",
    "    for result in results:\n",
    "        boxes = result.boxes.xyxy  # Extract bounding boxes\n",
    "        for box in boxes:\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            face = frame[y1:y2, x1:x2]\n",
    "\n",
    "            # Face recognition\n",
    "            face_encodings = face_recognition.face_encodings(frame, [(y1, x2, y2, x1)])\n",
    "            if face_encodings:\n",
    "                match = face_recognition.compare_faces(known_face_encodings, face_encodings[0])\n",
    "                if not any(match):\n",
    "                    print(\"Unauthorized person detected!\")\n",
    "                    cv2.putText(frame, \"THREAT!\", (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "                    \n",
    "                    # Send email alert\n",
    "                    with smtplib.SMTP(\"smtp.gmail.com\", 587) as server:\n",
    "                        server.starttls()\n",
    "                        # server.login(\"your_email@gmail.com\", \"your_password\")\n",
    "                        # server.sendmail(\"your_email@gmail.com\", \"security_team@gmail.com\", \"Threat Detected!\")\n",
    "\n",
    "    cv2.imshow(\"Security Feed\", frame)\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1844b23e",
   "metadata": {},
   "outputs": [],
   "source": [
    "%cd {HOME}\n",
    "\n",
    "!yolo task=detect mode=train model=yolo11n.pt data={dataset.location}/data.yaml epochs=10 lr0=0.01 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6565b594",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model=\"./runs/detect/train2/weights/best.pt\" source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d725e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo train data=coco8.yaml model=\"./runs/detect/train2/weights/best.pt\" epochs=10 lr0=0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec3de451",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate YOLO11n on COCO8 val with batch size 1\n",
    "!yolo val model=\"./runs/detect/train3/weights/best.pt\" data=coco8.yaml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1e247c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !yolo predict model=\"./runs/detect/train3/weights/best.pt\" source='https://ultralytics.com/images/zidane.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52a731b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "!yolo predict model=runs/detect/train2/weights/best.pt source=Test-Video-And-Images/count.mp4 show=True save=True imgsz=320 conf=0.25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1fc0bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Start training from a pretrained *.pt model\n",
    "# !yolo obb train data=dota8.yaml model=yolo11n-obb.pt epochs=100 imgsz=640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15993b9c",
   "metadata": {},
   "source": [
    "# Security Alarm and Face Recognition\n",
    "\n",
    "This section implements an integrated security monitoring system that combines object detection, face recognition, and security alerts. The system offers:\n",
    "\n",
    "- Real-time object detection using YOLO model\n",
    "- Face recognition to identify known individuals\n",
    "- Motion tracking using MediaPipe\n",
    "- Security alarm triggering\n",
    "- Automated email alerts\n",
    "- Event logging\n",
    "- Performance optimization with frame processing intervals\n",
    "- FPS monitoring\n",
    "\n",
    "Features:\n",
    "- Detects 80+ object classes including persons, vehicles, and common objects\n",
    "- Recognizes faces and matches against known face database\n",
    "- Logs security events with timestamps\n",
    "- Plays audio alarm when unauthorized access detected\n",
    "- Sends email notifications with detection details\n",
    "- Displays real-time FPS and detection information\n",
    "\n",
    "Usage:\n",
    "- Add known faces to 'images' directory\n",
    "- Configure email settings for alerts\n",
    "- Press 'q' to quit monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7501606c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "cap = cv2.VideoCapture(0)\n",
    "assert cap.isOpened(), \"Error reading video file\"\n",
    "\n",
    "# Video writer\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "video_writer = cv2.VideoWriter(\"security_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "from_email = \"deveansari@gmail.com\"  # the sender email address\n",
    "password = \"nuzb vkot eauw qihl\"  # 16-digits password generated via: https://myaccount.google.com/apppasswords\n",
    "to_email = \"rahatansari.tpu@gmail.com\"  # the receiver email address\n",
    "\n",
    "# Initialize security alarm object\n",
    "securityalarm = solutions.SecurityAlarm(\n",
    "    show=True,  # display the output\n",
    "    model=\"yolo11n.pt\",  # i.e. yolo11s.pt, yolo11m.pt\n",
    "    records=1,  # total detections count to send an email\n",
    ")\n",
    "\n",
    "securityalarm.authenticate(from_email, password, to_email)  # authenticate the email server\n",
    "\n",
    "# Process video\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    results = securityalarm(im0)\n",
    "\n",
    "    print(results)  # access the output\n",
    "\n",
    "    video_writer.write(results.plot_im)  # write the processed frame.\n",
    "\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "cv2.destroyAllWindows()  # destroy all opened windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5719118b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f815ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build a new model from YAML, transfer pretrained weights for face detection\n",
    "# !yolo detect train data=face_dataset.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640 single_cls=True name=face_detector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ae974c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "!curl -O https://raw.githubusercontent.com/opencv/opencv/master/data/haarcascades/haarcascade_frontalface_default.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d30f29e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load models\n",
    "yolo_model = YOLO('yolo11n.pt')  # COCO-trained model\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # YOLO object detection\n",
    "    results = yolo_model(image)\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for result in results:\n",
    "        # Draw non-person objects\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls)\n",
    "            conf = float(box.conf)\n",
    "            label = yolo_model.names[cls_id]\n",
    "            \n",
    "            if label != \"person\":\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        # Face detection for person regions\n",
    "        person_boxes = [box.xyxy[0] for box in result.boxes if int(box.cls) == 0]\n",
    "        for person_box in person_boxes:\n",
    "            x1, y1, x2, y2 = map(int, person_box)\n",
    "            padding = 20  # Extra area around person\n",
    "            roi = image[max(0, y1-padding):min(y2+padding, image.shape[0]), \n",
    "                        max(0, x1-padding):min(x2+padding, image.shape[1])]\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, \n",
    "                scaleFactor=1.1, \n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30)\n",
    "            )\n",
    "            \n",
    "            # Draw face boxes\n",
    "            for (fx, fy, fw, fh) in faces:\n",
    "                # Convert to original image coordinates\n",
    "                abs_x = max(0, x1-padding) + fx\n",
    "                abs_y = max(0, y1-padding) + fy\n",
    "                cv2.rectangle(image, \n",
    "                            (abs_x, abs_y),\n",
    "                            (abs_x+fw, abs_y+fh),\n",
    "                            (0, 0, 255), 2)\n",
    "                cv2.putText(image, \"Face\", (abs_x, abs_y-5),\n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "    \n",
    "    # Save results\n",
    "    output_path = image_path.replace('.jpg', '_result.jpg')\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "def process_video(video_path):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error loading video: {video_path}\")\n",
    "        return\n",
    "\n",
    "    # Get video properties for output\n",
    "    frame_width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    frame_height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    fps = int(cap.get(cv2.CAP_PROP_FPS))\n",
    "    \n",
    "    # Create VideoWriter object\n",
    "    output_path = video_path.replace('.mp4', '_result.mp4')\n",
    "    out = cv2.VideoWriter(output_path, \n",
    "                         cv2.VideoWriter_fourcc(*'mp4v'), \n",
    "                         fps, (frame_width, frame_height))\n",
    "\n",
    "    while cap.isOpened():\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "\n",
    "        # YOLO object detection\n",
    "        results = yolo_model(frame)\n",
    "        \n",
    "        # Process YOLO detections\n",
    "        for result in results:\n",
    "            # Draw non-person objects\n",
    "            for box in result.boxes:\n",
    "                cls_id = int(box.cls)\n",
    "                conf = float(box.conf)\n",
    "                label = yolo_model.names[cls_id]\n",
    "                \n",
    "                if label != \"person\":\n",
    "                    x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    cv2.putText(frame, f\"{label} {conf:.2f}\", (x1, y1-10), \n",
    "                                cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "            \n",
    "            # Face detection for person regions\n",
    "            person_boxes = [box.xyxy[0] for box in result.boxes if int(box.cls) == 0]\n",
    "            for person_box in person_boxes:\n",
    "                x1, y1, x2, y2 = map(int, person_box)\n",
    "                padding = 20\n",
    "                roi = frame[max(0, y1-padding):min(y2+padding, frame.shape[0]), \n",
    "                            max(0, x1-padding):min(x2+padding, frame.shape[1])]\n",
    "                \n",
    "                if roi.size > 0:  # Check if ROI is not empty\n",
    "                    gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "                    faces = face_cascade.detectMultiScale(\n",
    "                        gray, \n",
    "                        scaleFactor=1.1, \n",
    "                        minNeighbors=5,\n",
    "                        minSize=(30, 30)\n",
    "                    )\n",
    "                    \n",
    "                    for (fx, fy, fw, fh) in faces:\n",
    "                        abs_x = max(0, x1-padding) + fx\n",
    "                        abs_y = max(0, y1-padding) + fy\n",
    "                        cv2.rectangle(frame, \n",
    "                                    (abs_x, abs_y),\n",
    "                                    (abs_x+fw, abs_y+fh),\n",
    "                                    (0, 0, 255), 2)\n",
    "                        cv2.putText(frame, \"Face\", (abs_x, abs_y-5),\n",
    "                                    cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 0, 255), 2)\n",
    "\n",
    "        out.write(frame)\n",
    "        \n",
    "        # Display the frame (optional)\n",
    "        cv2.imshow('Frame', frame)\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "\n",
    "    cap.release()\n",
    "    out.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    process_video(\"./media_files/conveyer apple and bottle counting/istockphoto-2154825392-640_adpp_is.mp4\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdb49a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "\n",
    "# Load YOLO model\n",
    "yolo_model = YOLO('yolo11n.pt')\n",
    "\n",
    "# Load Haar cascade for face detection\n",
    "face_cascade = cv2.CascadeClassifier('./haarcascade_frontalface_default.xml')\n",
    "\n",
    "# Verify the cascade loaded correctly\n",
    "if face_cascade.empty():\n",
    "    print(\"Error: Could not load Haar cascade file\")\n",
    "else:\n",
    "    print(\"Haar cascade loaded successfully\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8caff61c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_haar(image):\n",
    "    \"\"\"\n",
    "    Detect faces using Haar cascade and return bounding boxes\n",
    "    \"\"\"\n",
    "    # Convert to grayscale for face detection\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    # Detect faces\n",
    "    faces = face_cascade.detectMultiScale(\n",
    "        gray,\n",
    "        scaleFactor=1.1,\n",
    "        minNeighbors=5,\n",
    "        minSize=(30, 30),\n",
    "        flags=cv2.CASCADE_SCALE_IMAGE\n",
    "    )\n",
    "    \n",
    "    face_boxes = []\n",
    "    for (x, y, w, h) in faces:\n",
    "        # Convert to [x1, y1, x2, y2, confidence, class_name] format\n",
    "        face_boxes.append([x, y, x + w, y + h, 0.9, 'face'])  # Haar doesn't provide confidence, so we use 0.9\n",
    "    \n",
    "#  detect_faces_haar(\"d:\\\\Projects\\\\ultralytics\\\\face_person\\\\images\\\\happy-group-of-business-people-walking-in-front-of-the-office-G24CN2.jpg\")he-office-G24CN2.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17d6c5e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_yolo_haar_detection(image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Perform combined object detection (YOLO) and face detection (Haar Cascade)\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # YOLO object detection (all classes including person)\n",
    "    yolo_results = yolo_model.predict(\n",
    "        source=image,\n",
    "        conf=conf_threshold,\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Face detection using Haar cascade\n",
    "    face_boxes = detect_faces_haar(image)\n",
    "    \n",
    "    return image, yolo_results, face_boxes\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "524dcd20",
   "metadata": {},
   "source": [
    "## Face Person Detection from Images "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16b29f0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test with a sample image\n",
    "image_path = \"./bus.jpg\"  # Replace with your image path\n",
    "\n",
    "# Run detection\n",
    "def detect_objects_and_faces_haar(image_path, save_path, show_result=True):\n",
    "    import cv2\n",
    "    import numpy as np\n",
    "    from ultralytics import YOLO\n",
    "    \n",
    "    # Load YOLO model\n",
    "    model = YOLO('yolo11n.pt')\n",
    "    \n",
    "    # Load Haar cascade for face detection\n",
    "    face_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_frontalface_default.xml')\n",
    "    \n",
    "    # Read image\n",
    "    img = cv2.imread(image_path)\n",
    "    \n",
    "    # YOLO detection\n",
    "    results = model(img)\n",
    "    \n",
    "    # Get face detections\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    faces = face_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    annotated_img = results[0].plot()\n",
    "    \n",
    "    # Draw face detections\n",
    "    for (x, y, w, h) in faces:\n",
    "        cv2.rectangle(annotated_img, (x, y), (x+w, y+h), (255, 0, 0), 2)\n",
    "    \n",
    "    # Save result\n",
    "    cv2.imwrite(save_path, annotated_img)\n",
    "    \n",
    "    # Show result if requested\n",
    "    if show_result:\n",
    "        cv2.imshow('Result', annotated_img)\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()\n",
    "    \n",
    "    return annotated_img\n",
    "\n",
    "result = detect_objects_and_faces_haar(\n",
    "    image_path=image_path,\n",
    "    save_path=\"result_yolo_haar.jpg\",\n",
    "    show_result=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac5928ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "\n",
    "# Load YOLO model for general object detection\n",
    "yolo_model = YOLO('yolo11n.pt')\n",
    "\n",
    "# Initialize MediaPipe face detection for accurate face detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "face_detection = mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "117180eb",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_faces_mediapipe(image):\n",
    "    \"\"\"\n",
    "    Detect faces using MediaPipe and return bounding boxes in YOLO format\n",
    "    \"\"\"\n",
    "    rgb_image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
    "    results = face_detection.process(rgb_image)\n",
    "    \n",
    "    face_boxes = []\n",
    "    if results.detections:\n",
    "        h, w, _ = image.shape\n",
    "        for detection in results.detections:\n",
    "            bbox = detection.location_data.relative_bounding_box\n",
    "            # Convert to absolute coordinates\n",
    "            x1 = int(bbox.xmin * w)\n",
    "            y1 = int(bbox.ymin * h)\n",
    "            x2 = int((bbox.xmin + bbox.width) * w)\n",
    "            y2 = int((bbox.ymin + bbox.height) * h)\n",
    "            \n",
    "            # Store as [x1, y1, x2, y2, confidence, class_id]\n",
    "            confidence = detection.score[0]\n",
    "            face_boxes.append([x1, y1, x2, y2, confidence, 'face'])\n",
    "    \n",
    "    return face_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2660dda",
   "metadata": {},
   "outputs": [],
   "source": [
    "def combined_detection(image_path, conf_threshold=0.5):\n",
    "    \"\"\"\n",
    "    Perform combined object detection (YOLO) and face detection (MediaPipe)\n",
    "    \"\"\"\n",
    "    # Read image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error: Could not load image from {image_path}\")\n",
    "        return None, None, None\n",
    "    \n",
    "    # YOLO object detection (excluding person class to avoid overlap)\n",
    "    yolo_results = yolo_model.predict(\n",
    "        source=image,\n",
    "        conf=conf_threshold,\n",
    "        classes=[i for i in range(80) if i != 0],  # All COCO classes except person (class 0)\n",
    "        verbose=False\n",
    "    )\n",
    "    \n",
    "    # Face detection using MediaPipe\n",
    "    face_boxes = detect_faces_mediapipe(image)\n",
    "    print(image, yolo_results, face_boxes)\n",
    "    return image, yolo_results, face_boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba51b29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def draw_detections(image, yolo_results, face_boxes):\n",
    "    \"\"\"\n",
    "    Draw bounding boxes for both YOLO detections and face detections\n",
    "    \"\"\"\n",
    "    # Create a copy of the image\n",
    "    result_image = image.copy()\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    if yolo_results and len(yolo_results[0].boxes) > 0:\n",
    "        for box in yolo_results[0].boxes:\n",
    "            # Get box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            confidence = float(box.conf[0])\n",
    "            class_id = int(box.cls[0])\n",
    "            class_name = yolo_model.names[class_id]\n",
    "            \n",
    "            # Draw bounding box (blue for YOLO objects)\n",
    "            cv2.rectangle(result_image, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "            \n",
    "            # Draw label\n",
    "            label = f\"{class_name}: {confidence:.2f}\"\n",
    "            label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "            cv2.rectangle(result_image, (x1, y1 - label_size[1] - 10), \n",
    "                         (x1 + label_size[0], y1), (255, 0, 0), -1)\n",
    "            cv2.putText(result_image, label, (x1, y1 - 5), \n",
    "                       cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    # Draw face detections\n",
    "    for face_box in face_boxes:\n",
    "        x1, y1, x2, y2, confidence, class_name = face_box\n",
    "        \n",
    "        # Draw bounding box (red for faces)\n",
    "        cv2.rectangle(result_image, (x1, y1), (x2, y2), (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw label\n",
    "        label = f\"face: {confidence:.2f}\"\n",
    "        label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "        cv2.rectangle(result_image, (x1, y1 - label_size[1] - 10), \n",
    "                     (x1 + label_size[0], y1), (0, 0, 255), -1)\n",
    "        cv2.putText(result_image, label, (x1, y1 - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34cd364e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_objects_and_faces(image_path, save_path=None, show_result=True):\n",
    "    \"\"\"\n",
    "    Complete pipeline for object and face detection\n",
    "    \"\"\"\n",
    "    print(f\"Processing: {image_path}\")\n",
    "    \n",
    "    # Perform combined detection\n",
    "    image, yolo_results, face_boxes = combined_detection(image_path, conf_threshold=0.5)\n",
    "    \n",
    "    if image is None:\n",
    "        return None\n",
    "    \n",
    "    # Draw detections\n",
    "    result_image = draw_detections(image, yolo_results, face_boxes)\n",
    "    \n",
    "    # Print detection summary\n",
    "    yolo_count = len(yolo_results[0].boxes) if yolo_results and yolo_results[0].boxes is not None else 0\n",
    "    face_count = len(face_boxes)\n",
    "    \n",
    "    print(f\"Detected {yolo_count} objects and {face_count} faces\")\n",
    "    \n",
    "    # Display detected classes\n",
    "    if yolo_results and len(yolo_results[0].boxes) > 0:\n",
    "        detected_classes = [yolo_model.names[int(box.cls[0])] for box in yolo_results[0].boxes]\n",
    "        print(f\"Objects detected: {set(detected_classes)}\")\n",
    "    \n",
    "    # Save result if path provided\n",
    "    if save_path:\n",
    "        cv2.imwrite(save_path, result_image)\n",
    "        print(f\"Result saved to: {save_path}\")\n",
    "    \n",
    "    # Show result\n",
    "    if show_result:\n",
    "        plt.figure(figsize=(12, 8))\n",
    "        plt.imshow(cv2.cvtColor(result_image, cv2.COLOR_BGR2RGB))\n",
    "        plt.title('Object Detection (Blue) + Face Detection (Red)')\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    \n",
    "    return result_image\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f4f7477",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example usage for single image\n",
    "image_path = \"bus.jpg\"  # Replace with your image path\n",
    "result = detect_objects_and_faces(\n",
    "    image_path=image_path,\n",
    "    save_path=\"result_with_faces.jpg\",\n",
    "    show_result=True\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95208f37",
   "metadata": {},
   "outputs": [],
   "source": [
    "def real_time_detection(video_source, yolo_model):\n",
    "    \"\"\"\n",
    "    Real-time detection from webcam or video file\n",
    "    video_source: 0 for webcam, or path to video file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    yolo_model = YOLO(\"yolo11m.pt\")\n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            break\n",
    "        \n",
    "        # YOLO detection (excluding person class)\n",
    "        yolo_results = yolo_model.predict(\n",
    "            source=frame,\n",
    "            show=True,\n",
    "            conf=0.5,\n",
    "            # classes=[i for i in range(80) if i != 0],\n",
    "            verbose=False\n",
    "        )\n",
    "        \n",
    "        # Face detection\n",
    "        face_boxes = detect_faces_mediapipe(frame)\n",
    "        \n",
    "        # Draw detections\n",
    "        result_frame = draw_detections(frame, yolo_results, face_boxes)\n",
    "        \n",
    "        # Display\n",
    "        cv2.imshow('Object + Face Detection', result_frame)\n",
    "        \n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Uncomment to run real-time detection\n",
<<<<<<< HEAD
    "real_time_detection(\"./media_files/people walking/computer_vision_object_and_detection_tracking_people_walking_video_20250819_173636_1.mp4\", yolo_model)\n"
=======
    "real_time_detection(\"./Test-Video-And-Images/istockphoto-1442184251-640_adpp_is.mp4\", yolo_model)\n"
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dd337d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "from ultralytics import YOLO\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "import mediapipe as mp\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_faces_mediapipe(frame):\n",
    "    \"\"\"\n",
    "    Detect faces using MediaPipe\n",
    "    Returns list of face bounding boxes in format [x, y, w, h]\n",
    "    \"\"\"\n",
    "    face_boxes = []\n",
    "    \n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(rgb_frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            h, w, _ = frame.shape\n",
    "            for detection in results.detections:\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x = int(bbox.xmin * w)\n",
    "                y = int(bbox.ymin * h)\n",
    "                width = int(bbox.width * w)\n",
    "                height = int(bbox.height * h)\n",
    "                face_boxes.append([x, y, width, height])\n",
    "    \n",
    "    return face_boxes\n",
    "\n",
    "def draw_detections(frame, yolo_results, face_boxes):\n",
    "    \"\"\"\n",
    "    Draw YOLO detections and face detections on frame\n",
    "    \"\"\"\n",
    "    result_frame = frame.copy()\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    if yolo_results and len(yolo_results) > 0:\n",
    "        for result in yolo_results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                confidences = result.boxes.conf.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy()\n",
    "                \n",
    "                for box, conf, cls in zip(boxes, confidences, classes):\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    \n",
    "                    # Get class name\n",
    "                    class_name = result.names[int(cls)] if hasattr(result, 'names') else f\"Class {int(cls)}\"\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Draw label\n",
    "                    label = f\"{class_name}: {conf:.2f}\"\n",
    "                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                    cv2.rectangle(result_frame, (x1, y1 - label_size[1] - 10), \n",
    "                                (x1 + label_size[0], y1), (0, 255, 0), -1)\n",
    "                    cv2.putText(result_frame, label, (x1, y1 - 5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    \n",
    "    # Draw face detections\n",
    "    # for face_box in face_boxes:\n",
    "    #     x, y, w, h = face_box\n",
    "    #     # Draw face bounding box in red\n",
    "    #     cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "    #     # Draw face label\n",
    "    #     cv2.rectangle(result_frame, (x, y - 25), (x + 60, y), (0, 0, 255), -1)\n",
    "    #     cv2.putText(result_frame, \"Face\", (x + 5, y - 5), \n",
    "    #                cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return result_frame\n",
    "\n",
    "def real_time_detection(video_source):\n",
    "    \"\"\"\n",
    "    Real-time detection from webcam or video file\n",
    "    video_source: 0 for webcam, or path to video file\n",
    "    \"\"\"\n",
    "    cap = cv2.VideoCapture(video_source)\n",
    "    \n",
    "    # Check if video opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error: Could not open video source {video_source}\")\n",
    "        return\n",
    "    \n",
    "    model = YOLO('yolo11m.pt')  # Load YOLO model\n",
    "    # model = YOLO('runs/detect/train2/weights/best.pt')  # Load YOLO model\n",
    "    \n",
    "    print(\"Press 'q' to quit\")\n",
    "    \n",
    "    while True:\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"End of video or failed to read frame\")\n",
    "            break\n",
    "        \n",
    "        # YOLO detection (excluding person class - class 0)\n",
    "        # Run YOLO detection, exclude person class (0)\n",
    "        yolo_results = model.predict(\n",
    "            source=frame,\n",
    "            conf=0.5,\n",
<<<<<<< HEAD
    "            # classes=[i for i in range(100) if i != 0],  # Exclude person class       \n",
=======
    "            classes=[i for i in range(100) if i != 0],  # Exclude person class       \n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "            # classes=[i for i in range(2) if i != 0],  # Exclude person class       \n",
    "            # classes=[0],  # Exclude person class\n",
    "            verbose=False,\n",
    "            boxes=True,                   # draw the rectangles\n",
    "            show_labels=True\n",
    "        )\n",
    "        # Face detection will still detect person faces separately\n",
    "        \n",
    "        # Face detection\n",
    "        face_boxes = detect_faces_mediapipe(frame)\n",
    "        \n",
    "        # Draw detections\n",
    "        result_frame = draw_detections(frame, yolo_results, face_boxes)\n",
    "        \n",
    "        # Add frame info\n",
    "        cv2.putText(result_frame, f\"Objects: {len(yolo_results[0].boxes) if yolo_results[0].boxes is not None else 0}\", \n",
    "                   (10, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        # cv2.putText(result_frame, f\"Faces: {len(face_boxes)}\", \n",
    "        #            (10, 60), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (255, 255, 255), 2)\n",
    "        \n",
    "        # Display\n",
    "        cv2.imshow('Object + Face Detection', result_frame)\n",
    "        \n",
    "        # Break on 'q' key press\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "    \n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
<<<<<<< HEAD
    "#  video_file = \"./media_files/conveyer apple and bottle counting/istockphoto-2154825392-640_adpp_is.mp4\"  # Replace with your video path\n",
    " video_file = \"./media_files/animal_surveillance/goru-churi.mp4\"  # Replace with your video path\n",
=======
    " video_file = \"./Test-Video-And-Images/istockphoto-2174886250-640_adpp_is.mp4\"  # Replace with your video path\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    " # video_file = \"./Test-Video-And-Images/855564-hd_1920_1080_24fps.mp4\"  # Replace with your video path\n",
    " real_time_detection(video_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e8f4e48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ultralytics  AGPL-3.0 License - https://ultralytics.com/license\n",
    "\n",
    "import time\n",
    "\n",
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "from ultralytics.utils import LOGGER\n",
    "from ultralytics.utils.plotting import Annotator, colors\n",
    "\n",
    "enable_gpu = True  # Set True if running with CUDA\n",
    "# model_file = \"runs/detect/train2/weights/best.pt\"  # Path to model file\n",
    "model_file = \"yolo11n.pt\"  # Path to model file\n",
    "show_fps = True  # If True, shows current FPS in top-left corner\n",
    "show_conf = True  # Display or hide the confidence score\n",
    "save_video = True  # Set True to save output video\n",
    "video_output_path = \"interactive_tracker_output.avi\"  # Output video file name\n",
    "\n",
    "\n",
<<<<<<< HEAD
    "conf = 0.5  # Min confidence for object detection (lower = more detections, possibly more false positives)\n",
    "iou = 0.10  # IoU threshold for NMS (higher = less overlap allowed)\n",
=======
    "conf = 0.3  # Min confidence for object detection (lower = more detections, possibly more false positives)\n",
    "iou = 0.99  # IoU threshold for NMS (higher = less overlap allowed)\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "max_det = 20  # Maximum objects per im (increase for crowded scenes)\n",
    "\n",
    "tracker = \"bytetrack.yaml\"  # Tracker config: 'bytetrack.yaml', 'botsort.yaml', etc.\n",
    "track_args = {\n",
    "    \"persist\": True,  # Keep frames history as a stream for continuous tracking\n",
    "    \"verbose\": False,  # Print debug info from tracker\n",
    "}\n",
    "\n",
    "window_name = \"Ultralytics YOLO Interactive Tracking\"  # Output window name\n",
    "\n",
    "LOGGER.info(\" Initializing model...\")\n",
    "if enable_gpu:\n",
    "    LOGGER.info(\"Using GPU...\")\n",
    "    model = YOLO(model_file)\n",
    "    model.to(\"cuda\")\n",
    "else:\n",
    "    LOGGER.info(\"Using CPU...\")\n",
    "    model = YOLO(model_file, task=\"detect\")\n",
    "\n",
    "classes = model.names  # Store model classes names\n",
    "\n",
<<<<<<< HEAD
    "# cap = cv2.VideoCapture(\"./media_files/people walking/102455-660253882_small.mp4\")  # Replace with video path if needed\n",
    "cap = cv2.VideoCapture(\"./media_files/animal_surveillance/goru-churi.mp4\")  # Replace with video path if needed\n",
=======
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/istockphoto-472860965-640_adpp_is.mp4\")  # Replace with video path if needed\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "\n",
    "# Initialize video writer\n",
    "vw = None\n",
    "if save_video:\n",
    "    w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "    vw = cv2.VideoWriter(video_output_path, cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "selected_object_id = None\n",
    "selected_bbox = None\n",
    "selected_center = None\n",
    "\n",
    "\n",
    "def get_center(x1, y1, x2, y2):\n",
    "    \"\"\"\n",
    "    Calculates the center point of a bounding box.\n",
    "\n",
    "    Args:\n",
    "        x1 (int): Top-left X coordinate.\n",
    "        y1 (int): Top-left Y coordinate.\n",
    "        x2 (int): Bottom-right X coordinate.\n",
    "        y2 (int): Bottom-right Y coordinate.\n",
    "\n",
    "    Returns:\n",
    "        (int, int): Center point (x, y) of the bounding box.\n",
    "    \"\"\"\n",
    "    return (x1 + x2) // 2, (y1 + y2) // 2\n",
    "\n",
    "\n",
    "def extend_line_from_edge(mid_x, mid_y, direction, img_shape):\n",
    "    \"\"\"\n",
    "    Calculates the endpoint to extend a line from the center toward an image edge.\n",
    "\n",
    "    Args:\n",
    "        mid_x (int): X-coordinate of the midpoint.\n",
    "        mid_y (int): Y-coordinate of the midpoint.\n",
    "        direction (str): Direction to extend ('left', 'right', 'up', 'down').\n",
    "        img_shape (tuple): Image shape in (height, width, channels).\n",
    "\n",
    "    Returns:\n",
    "        (int, int): Endpoint coordinate of the extended line.\n",
    "    \"\"\"\n",
    "    h, w = img_shape[:2]\n",
    "    if direction == \"left\":\n",
    "        return 0, mid_y\n",
    "    if direction == \"right\":\n",
    "        return w - 1, mid_y\n",
    "    if direction == \"up\":\n",
    "        return mid_x, 0\n",
    "    if direction == \"down\":\n",
    "        return mid_x, h - 1\n",
    "    return mid_x, mid_y\n",
    "\n",
    "\n",
    "def draw_tracking_scope(im, bbox, color):\n",
    "    \"\"\"\n",
    "    Draws tracking scope lines extending from the bounding box to image edges.\n",
    "\n",
    "    Args:\n",
    "        im (ndarray): Image array to draw on.\n",
    "        bbox (tuple): Bounding box coordinates (x1, y1, x2, y2).\n",
    "        color (tuple): Color in BGR format for drawing.\n",
    "    \"\"\"\n",
    "    x1, y1, x2, y2 = bbox\n",
    "    mid_top = ((x1 + x2) // 2, y1)\n",
    "    mid_bottom = ((x1 + x2) // 2, y2)\n",
    "    mid_left = (x1, (y1 + y2) // 2)\n",
    "    mid_right = (x2, (y1 + y2) // 2)\n",
    "    cv2.line(im, mid_top, extend_line_from_edge(*mid_top, \"up\", im.shape), color, 2)\n",
    "    cv2.line(im, mid_bottom, extend_line_from_edge(*mid_bottom, \"down\", im.shape), color, 2)\n",
    "    cv2.line(im, mid_left, extend_line_from_edge(*mid_left, \"left\", im.shape), color, 2)\n",
    "    cv2.line(im, mid_right, extend_line_from_edge(*mid_right, \"right\", im.shape), color, 2)\n",
    "\n",
    "\n",
    "def click_event(event, x, y, flags, param):\n",
    "    \"\"\"\n",
    "    Handles mouse click events to select an object for focused tracking.\n",
    "\n",
    "    Args:\n",
    "        event (int): OpenCV mouse event type.\n",
    "        x (int): X-coordinate of the mouse event.\n",
    "        y (int): Y-coordinate of the mouse event.\n",
    "        flags (int): Any relevant flags passed by OpenCV.\n",
    "        param (any): Additional parameters (not used).\n",
    "    \"\"\"\n",
    "    global selected_object_id\n",
    "    if event == cv2.EVENT_LBUTTONDOWN:\n",
    "        detections = results[0].boxes.data if results[0].boxes is not None else []\n",
    "        if detections is not None:\n",
    "            min_area = float(\"inf\")\n",
    "            best_match = None\n",
    "            for track in detections:\n",
    "                track = track.tolist()\n",
    "                if len(track) >= 6:\n",
    "                    x1, y1, x2, y2 = map(int, track[:4])\n",
    "                    if x1 <= x <= x2 and y1 <= y <= y2:\n",
    "                        area = (x2 - x1) * (y2 - y1)\n",
    "                        if area < min_area:\n",
    "                            class_id = int(track[-1])\n",
    "                            track_id = int(track[4]) if len(track) == 7 else -1\n",
    "                            min_area = area\n",
    "                            best_match = (track_id, model.names[class_id])\n",
    "            if best_match:\n",
    "                selected_object_id, label = best_match\n",
    "                print(f\" TRACKING STARTED: {label} (ID {selected_object_id})\")\n",
    "\n",
    "\n",
    "cv2.namedWindow(window_name)\n",
    "cv2.setMouseCallback(window_name, click_event)\n",
    "\n",
    "fps_counter, fps_timer, fps_display = 0, time.time(), 0\n",
    "\n",
    "while cap.isOpened():\n",
    "    success, im = cap.read()\n",
    "    if not success:\n",
    "        break\n",
    "\n",
    "    results = model.track(im, conf=conf, iou=iou, max_det=max_det, tracker=tracker, **track_args)\n",
    "    annotator = Annotator(im)\n",
    "    detections = results[0].boxes.data if results[0].boxes is not None else []\n",
    "    detected_objects = []\n",
    "    for track in detections:\n",
    "        track = track.tolist()\n",
    "        if len(track) < 6:\n",
    "            continue\n",
    "        x1, y1, x2, y2 = map(int, track[:4])\n",
    "        class_id = int(track[6]) if len(track) >= 7 else int(track[5])\n",
    "        track_id = int(track[4]) if len(track) == 7 else -1\n",
    "        color = colors(track_id, True)\n",
    "        txt_color = annotator.get_txt_color(color)\n",
    "        label = f\"{classes[class_id]} ID {track_id}\" + (f\" ({float(track[5]):.2f})\" if show_conf else \"\")\n",
    "        if track_id == selected_object_id:\n",
    "            draw_tracking_scope(im, (x1, y1, x2, y2), color)\n",
    "            center = get_center(x1, y1, x2, y2)\n",
    "            cv2.circle(im, center, 6, color, -1)\n",
    "\n",
    "            # Pulsing circle for attention\n",
    "            pulse_radius = 8 + int(4 * abs(time.time() % 1 - 0.5))\n",
    "            cv2.circle(im, center, pulse_radius, color, 2)\n",
    "\n",
    "            annotator.box_label([x1, y1, x2, y2], label=f\"ACTIVE: TRACK {track_id}\", color=color)\n",
    "        else:\n",
    "            # Draw dashed box for other objects\n",
    "            for i in range(x1, x2, 10):\n",
    "                cv2.line(im, (i, y1), (i + 5, y1), color, 3)\n",
    "                cv2.line(im, (i, y2), (i + 5, y2), color, 3)\n",
    "            for i in range(y1, y2, 10):\n",
    "                cv2.line(im, (x1, i), (x1, i + 5), color, 3)\n",
    "                cv2.line(im, (x2, i), (x2, i + 5), color, 3)\n",
    "            # Draw label text with background\n",
    "            (tw, th), bl = cv2.getTextSize(label, 0, 0.7, 2)\n",
    "            cv2.rectangle(im, (x1 + 5 - 5, y1 + 20 - th - 5), (x1 + 5 + tw + 5, y1 + 20 + bl), color, -1)\n",
    "            cv2.putText(im, label, (x1 + 5, y1 + 20), 0, 0.7, txt_color, 1, cv2.LINE_AA)\n",
    "\n",
    "    if show_fps:\n",
    "        fps_counter += 1\n",
    "        if time.time() - fps_timer >= 1.0:\n",
    "            fps_display = fps_counter\n",
    "            fps_counter = 0\n",
    "            fps_timer = time.time()\n",
    "\n",
    "        # Draw FPS text with background\n",
    "        fps_text = f\"FPS: {fps_display}\"\n",
    "        cv2.putText(im, fps_text, (10, 25), 0, 0.7, (255, 255, 255), 1)\n",
    "        (tw, th), bl = cv2.getTextSize(fps_text, 0, 0.7, 2)\n",
    "        cv2.rectangle(im, (10 - 5, 25 - th - 5), (10 + tw + 5, 25 + bl), (255, 255, 255), -1)\n",
    "        cv2.putText(im, fps_text, (10, 25), 0, 0.7, (104, 31, 17), 1, cv2.LINE_AA)\n",
    "\n",
    "    cv2.imshow(window_name, im)\n",
    "    if save_video and vw is not None:\n",
    "        vw.write(im)\n",
    "    # Terminal logging\n",
    "    LOGGER.info(f\" DETECTED {len(detections)} OBJECT(S): {' | '.join(detected_objects)}\")\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "    if key == ord(\"q\"):\n",
    "        break\n",
    "    elif key == ord(\"c\"):\n",
    "        LOGGER.info(\" TRACKING RESET\")\n",
    "        selected_object_id = None\n",
    "\n",
    "cap.release()\n",
    "if save_video and vw is not None:\n",
    "    vw.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d247ae9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train specifically for face detection with custom dataset\n",
    "!yolo detect train data=face_dataset.yaml model=yolo11n.yaml pretrained=yolo11n.pt epochs=100 imgsz=640 single_cls=True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b21c2b42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "from ultralytics import YOLO\n",
    "import os\n",
    "\n",
    "# Load models\n",
    "yolo_model = YOLO('yolo11n-cls.pt')  # COCO-trained model\n",
    "face_cascade = cv2.CascadeClassifier('haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_objects(image_path):\n",
    "    # Load image\n",
    "    image = cv2.imread(image_path)\n",
    "    if image is None:\n",
    "        print(f\"Error loading image: {image_path}\")\n",
    "        return\n",
    "    \n",
    "    # YOLO object detection\n",
    "    results = yolo_model(image, show=True)\n",
    "    \n",
    "    # Process YOLO detections\n",
    "    for result in results:\n",
    "        # Draw non-person objects\n",
    "        for box in result.boxes:\n",
    "            cls_id = int(box.cls)\n",
    "            conf = float(box.conf)\n",
    "            label = yolo_model.names[cls_id]\n",
    "            \n",
    "            if label != \"person\":\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                cv2.putText(image, f\"{label} {conf:.2f}\", (x1, y1-10), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.9, (0, 255, 0), 2)\n",
    "        \n",
    "        # Face detection for person regions\n",
    "        person_boxes = [box.xyxy[0] for box in result.boxes if int(box.cls) == 0]\n",
    "        for person_box in person_boxes:\n",
    "            x1, y1, x2, y2 = map(int, person_box)\n",
    "            padding = 20  # Extra area around person\n",
    "            roi = image[max(0, y1-padding):min(y2+padding, image.shape[0]), \n",
    "                        max(0, x1-padding):min(x2+padding, image.shape[1])]\n",
    "            \n",
    "            # Convert to grayscale for face detection\n",
    "            gray = cv2.cvtColor(roi, cv2.COLOR_BGR2GRAY)\n",
    "            faces = face_cascade.detectMultiScale(\n",
    "                gray, \n",
    "                scaleFactor=1.1, \n",
    "                minNeighbors=5,\n",
    "                minSize=(30, 30))\n",
    "            \n",
    "            # Draw face boxes\n",
    "            for (fx, fy, fw, fh) in faces:\n",
    "                # Convert to original image coordinates\n",
    "                abs_x = max(0, x1-padding) + fx\n",
    "                abs_y = max(0, y1-padding) + fy\n",
    "                cv2.rectangle(image, \n",
    "                            (abs_x, abs_y),\n",
    "                            (abs_x+fw, abs_y+fh),\n",
    "                            (255, 255, 255), 2)\n",
    "                cv2.putText(image, \"Face\", (abs_x, abs_y-5),\n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 255), 2)\n",
    "    \n",
    "    # Save results in \"python detected images\" folder\n",
    "    output_dir = \"python detected images\"\n",
    "    if not os.path.exists(output_dir):\n",
    "      os.makedirs(output_dir)\n",
    "    output_path = os.path.join(output_dir, os.path.basename(image_path).replace('.jpg', '_result.jpg'))\n",
    "    cv2.imwrite(output_path, image)\n",
    "    print(f\"Results saved to: {output_path}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    detect_objects(\"bus.png\")  # Replace with your image path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9164cc0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "\n",
    "def simple_video_detection(video_path):\n",
    "    \"\"\"\n",
    "    Simple video detection using YOLO's built-in capabilities\n",
    "    \"\"\"\n",
    "    # Load model\n",
    "    model = YOLO('yolo11n.pt')\n",
    "    # model = YOLO('best_fc_model.pt')\n",
    "    \n",
    "    # Run detection on video\n",
    "    results = model.track(\n",
    "        source=video_path,\n",
    "        show=True,          # Display video\n",
    "        # save=True,          # Save results\n",
    "        save_txt=True,      # Save detection coordinates\n",
    "        save_conf=True,     # Save confidence scores\n",
    "        line_width=2,       # Bounding box line width\n",
    "        show_labels=True,   # Show class labels\n",
    "        show_conf=True,     # Show confidence scores\n",
    "        iou=0.10,\n",
    "        conf=0.5,\n",
    "        # classes=[0],  # Detect all classes\n",
<<<<<<< HEAD
    "        imgsz=640,\n",
    "        # stream=True\n",
=======
    "        imgsz=640\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "    )\n",
    "    print(\"results====>\", results)\n",
    "    return results\n",
    "\n",
    "# Usage\n",
    "if __name__ == \"__main__\":\n",
<<<<<<< HEAD
    "    video_file = \"./media_files/animal_surveillance/goru-churi.mp4\"  # Replace with your video path\n",
=======
    "    video_file = \"./Test-Video-And-Images/855564-hd_1920_1080_24fps.mp4\"  # Replace with your video path\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "    simple_video_detection(video_file)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1367758b",
   "metadata": {},
   "source": [
    "# detect face with mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28c4772e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "import smtplib\n",
    "import pygame\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_faces_mediapipe(frame):\n",
    "    \"\"\"\n",
    "    Detect faces using MediaPipe\n",
    "    Returns list of face bounding boxes in format [x, y, w, h]\n",
    "    \"\"\"\n",
    "    face_boxes = []\n",
    "    \n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(rgb_frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            h, w, _ = frame.shape\n",
    "            for detection in results.detections:\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x = int(bbox.xmin * w)\n",
    "                y = int(bbox.ymin * h)\n",
    "                width = int(bbox.width * w)\n",
    "                height = int(bbox.height * h)\n",
    "                face_boxes.append([x, y, width, height])\n",
    "    \n",
    "    return face_boxes\n",
    "\n",
    "def draw_detections(frame, yolo_results, face_boxes):\n",
    "    \"\"\"\n",
    "    Draw YOLO detections and face detections on frame\n",
    "    \"\"\"\n",
    "    result_frame = frame.copy()\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    if yolo_results and len(yolo_results) > 0:\n",
    "        for result in yolo_results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                confidences = result.boxes.conf.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy()\n",
    "                \n",
    "                for box, conf, cls in zip(boxes, confidences, classes):\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    \n",
    "                    # Get class name\n",
    "                    class_name = result.names[int(cls)] if hasattr(result, 'names') else f\"Class {int(cls)}\"\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Draw label\n",
    "                    label = f\"{class_name}: {conf:.2f}\"\n",
    "                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                    cv2.rectangle(result_frame, (x1, y1 - label_size[1] - 10), \n",
    "                                (x1 + label_size[0], y1), (0, 255, 0), -1)\n",
    "                    cv2.putText(result_frame, label, (x1, y1 - 5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    \n",
    "    # Draw face detections\n",
    "    for face_box in face_boxes:\n",
    "        x, y, w, h = face_box\n",
    "        # Draw face bounding box in red\n",
    "        cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw face label\n",
    "        cv2.rectangle(result_frame, (x, y - 25), (x + 60, y), (0, 0, 255), -1)\n",
    "        cv2.putText(result_frame, \"Face\", (x + 5, y - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return result_frame\n",
    "\n",
    "# Initialize YOLO model for object detection\n",
    "# Using YOLOv8n which can detect 80 different object classes\n",
    "# model = YOLO(\"runs/detect/train2/weights/best.pt\")\n",
    "model = YOLO(\"yolo11m.pt\")\n",
    "\n",
    "# Initialize MediaPipe for pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Load known faces\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Setup for known faces - replace with your implementation\n",
    "# known_faces_dir = \"images\"  # Create this directory and add images\n",
    "known_faces_dir = \"family_members\"  # Create this directory and add images\n",
    "if os.path.exists(known_faces_dir):\n",
    "    for person_name in os.listdir(known_faces_dir):\n",
    "        person_dir = os.path.join(known_faces_dir, person_name)\n",
    "        if os.path.isdir(person_dir):\n",
    "            for image_name in os.listdir(person_dir):\n",
    "                image_path = os.path.join(person_dir, image_name)\n",
    "                try:\n",
    "                    image = face_recognition.load_image_file(image_path)\n",
    "                    face_encodings = face_recognition.face_encodings(image)\n",
    "                    if face_encodings:\n",
    "                        known_face_encodings.append(face_encodings[0])\n",
    "                        known_face_names.append(person_name)\n",
    "                        print(f\"Loaded face: {person_name} from {image_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {image_path}: {e}\")\n",
    "\n",
    "# Print summary of loaded faces\n",
    "if known_face_encodings:\n",
    "    print(f\"Successfully loaded {len(known_face_encodings)} face encodings for {len(set(known_face_names))} people\")\n",
    "else:\n",
    "    print(\"Warning: No face encodings loaded. Face recognition will not work.\")\n",
    "\n",
    "# Setup alarm sound\n",
    "pygame.mixer.init()\n",
    "alarm_file = \"pols-aagyi-pols.mp3\"\n",
    "if os.path.exists(alarm_file):\n",
    "    pygame.mixer.music.load(alarm_file)\n",
    "else:\n",
    "    print(f\"Warning: Alarm file {alarm_file} not found\")\n",
    "\n",
    "# Create log directory\n",
    "log_dir = \"security_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def log_event(event_type, details=\"\"):\n",
    "    \"\"\"Log security events to file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_file = os.path.join(log_dir, f\"security_log_{datetime.now().strftime('%Y-%m-%d')}.txt\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"{timestamp} - {event_type}: {details}\\n\")\n",
    "\n",
    "def  send_email_alert(person_name=\"Unknown\", objects_detected=None):\n",
    "    \"\"\"Function to send email alert when a person is detected.\"\"\"\n",
    "    if objects_detected is None:\n",
    "        objects_detected = []\n",
    "    \n",
    "    objects_str = \", \".join(objects_detected) if objects_detected else \"None\"\n",
    "    log_event(\"ALERT_TRIGGERED\", f\"Person: {person_name}, Objects: {objects_str}\")\n",
    "    print(f\"Alert triggered: {person_name} detected with objects: {objects_str}\")\n",
    "\n",
    "# Start Video Capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "# cap = cv2.VideoCapture(\"./Test-Video-And-Images/istockphoto-2174886250-640_adpp_is.mp4\")\n",
    "# cap = cv2.VideoCapture(\"https://www.youtube.com/watch?v=wswxrDiSiHI\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video capture device\")\n",
    "    exit()\n",
    "\n",
    "# Performance optimization variables\n",
    "frame_count = 0\n",
    "face_recognition_interval = 5  # Process face recognition every 5 frames\n",
    "last_alert_time = 0\n",
    "alert_cooldown = 10  # Seconds between alerts\n",
    "\n",
    "# Define objects of interest (subset of COCO classes that YOLO can detect)\n",
    "objects_of_interest = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\", \"mouse\",\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
    "    \"cell phone\", \"laptop\", \"book\", \"scissors\", \"knife\"\n",
    "]\n",
    "\n",
    "print(\"Security monitoring started. Press 'q' to quit.\")\n",
    "log_event(\"SYSTEM_START\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Start a high-resolution timer to measure performance or track elapsed time using OpenCV's getTickCount() method\n",
    "        timer = cv2.getTickCount()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        # Determine whether to process faces based on frame count interval\n",
    "        # Helps optimize performance by reducing face recognition computations\n",
    "        process_faces = frame_count % face_recognition_interval == 0\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # YOLO Detection for all objects\n",
    "        # results = model.predict(frame, conf=0.5, classes=[i for i in range(100) if i != 0], verbose=False)\n",
<<<<<<< HEAD
    "        # results = model(frame, conf=0.5, verbose=False)        \n",
=======
    "        results = model(frame, conf=0.5, verbose=False)        \n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "        \n",
    "        # Face Detection using MediaPipe\n",
    "        # results = model.predict(frame, conf=0.5, classes=[i for i in range(100) if i != 0], verbose=False\n",
    "        # results = model.predict(frame, conf=0.5, classes=[i for i in range(100) if i != 0], verbose=False)\n",
<<<<<<< HEAD
    "        results = detect_faces_mediapipe(frame)\n",
    "        \n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n",
=======
    "        # results = detect_faces_mediapipe(frame)\n",
    "        \n",
    "        # Visualize the results on the frame\n",
    "        # annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        # cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "        \n",
    "        # Track detected objects in this frame\n",
    "        detected_objects = []\n",
    "        detected_persons = []\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            for i, box in enumerate(boxes):\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                \n",
    "                # Ensure coordinates are within frame boundaries\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "                \n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue  # Skip invalid boxes\n",
    "                \n",
    "                # Get class and confidence\n",
    "                cls = int(box.cls[0])\n",
    "                conf = float(box.conf[0])\n",
    "                class_name = result.names[cls]\n",
    "                \n",
    "                # Add to detected objects list if it's an object of interest\n",
    "                if class_name in objects_of_interest and class_name != \"person\":\n",
    "                    detected_objects.append(class_name)\n",
    "                    \n",
    "                    # Draw bounding box for object\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                    \n",
    "                    # Display object name and confidence\n",
    "                    label = f\"{class_name}: {conf:.2f}\"\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "                \n",
    "                # Process persons separately for face recognition\n",
    "                # Process persons separately for face recognition\n",
    "                if class_name == \"person\":\n",
    "                    detected_persons.append((x1, y1, x2, y2))\n",
    "                    person_roi = frame[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Only proceed if person ROI is valid\n",
    "                    if person_roi.size > 0 and person_roi.shape[0] > 0 and person_roi.shape[1] > 0:\n",
    "                        # Face detection on person ROI\n",
    "                        face_boxes = detect_faces_mediapipe(person_roi)\n",
    "                        \n",
    "                        if face_boxes:\n",
    "                            # Draw bounding boxes around faces (not person)\n",
    "                            for face_box in face_boxes:\n",
    "                                fx, fy, fw, fh = face_box\n",
    "                                # Adjust face coordinates to original frame coordinates\n",
    "                                face_x1 = x1 + fx\n",
    "                                face_y1 = y1 + fy\n",
    "                                face_x2 = face_x1 + fw\n",
    "                                face_y2 = face_y1 + fh\n",
    "                                \n",
    "                                # Draw face bounding box in red\n",
    "                                cv2.rectangle(frame, (face_x1, face_y1), (face_x2, face_y2), (0, 0, 255), 2)\n",
    "                                \n",
    "                                # Add face label\n",
    "                                # cv2.rectangle(frame, (face_x1, face_y1 - 25), (face_x1 + 60, face_y1), (0, 0, 255), -1)\n",
    "                                cv2.putText(frame, \"Face\", (face_x1 + 5, face_y1 - 5), \n",
    "                                          cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                                # cv2.putText(frame, \"KNOWN: {name}\",(face_x1 + 20, face_y1 - 20), \n",
    "                                #           cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "                                \n",
<<<<<<< HEAD
    "                        else:\n",
    "                            # If no face detected in person, draw person box with \"No Face\" label\n",
    "                            cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                            cv2.putText(frame, \"Person - No Face\", (x1, y1 - 10), \n",
    "                                      cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
=======
    "                        # else:\n",
    "                        #     # If no face detected in person, draw person box with \"No Face\" label\n",
    "                        #     cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                        #     cv2.putText(frame, \"Person - No Face\", (x1, y1 - 10), \n",
    "                        #               cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 255), 2)\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "\n",
    "                    # Face Recognition - only process every few frames\n",
    "                    # Conditionally process face recognition only at specified intervals and when a valid person region of interest (ROI) exists\n",
    "                    if process_faces and person_roi.size > 0:\n",
    "                        # Convert the person ROI to RGB format\n",
    "                        # Converts BGR (Blue, Green, Red) color space to RGB (Red, Green, Blue) color space\n",
    "                        # Used for face recognition and comparison\n",
    "                        rgb_small_frame = cv2.cvtColor(person_roi, cv2.COLOR_BGR2RGB)\n",
    "                       \n",
    "                        # Resize the person region of interest to a smaller scale for faster face recognition processing\n",
    "                        # Reduces image dimensions to 25% of the original size using bilinear interpolation\n",
    "                        # Helps improve performance by reducing computational complexity of face detection\n",
    "                        small_frame = cv2.resize(person_roi, (0, 0), fx=0.25, fy=0.25)\n",
    "                        rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "                        \n",
    "                                                \n",
    "                        \n",
    "                        # Detect face locations in a resized RGB frame using face_recognition library\n",
    "                        # Identifies and returns the bounding box coordinates of faces in the input image\n",
    "                        face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "                        # Identifies face locations and generates corresponding face encodings\n",
    "                        \n",
    "                        if face_locations:\n",
    "                            face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "                            \n",
    "                            for face_encoding in face_encodings:\n",
    "                                if known_face_encodings:  # Only compare if we have known faces\n",
    "                                    matches = face_recognition.compare_faces(known_face_encodings, face_encoding)\n",
    "                                    \n",
    "                                    if any(matches):\n",
    "                                        # Find the name of the matched person\n",
    "                                        face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                                        best_match_index = np.argmin(face_distances)\n",
    "                                        if matches[best_match_index]:\n",
    "                                            name = known_face_names[best_match_index]\n",
    "                                            cv2.putText(frame, name, (x1, y1 - 10), \n",
    "                                                      cv2.FONT_HERSHEY_SIMPLEX, 0.4, (0, 255, 0), 2)\n",
    "                                            print(f\" Known Person Detected: {name}\")\n",
    "                                            # cv2.putText(frame, f\"KNOWN: {name}\", (x1, y1 - 10), \n",
    "                                            #           cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                                            \n",
    "                                            # Alert with cooldown\n",
    "                                            if current_time - last_alert_time > alert_cooldown:\n",
    "                                                if not pygame.mixer.music.get_busy():\n",
    "                                                    pygame.mixer.music.play()\n",
    "                                                # send_email_alert(name, detected_objects)\n",
    "                                                log_event(\"KNOWN_PERSON\", f\"Detected: {name} with objects: {', '.join(detected_objects) if detected_objects else 'None'}\")\n",
    "                                                last_alert_time = current_time\n",
    "                                    else:\n",
    "                                        print(\" Unknown Person Detected!\")\n",
    "                                        cv2.putText(frame, \"UNKNOWN\", (x1, y1 - 10), \n",
    "                                                  cv2.FONT_HERSHEY_SIMPLEX, 0., (0, 0, 255), 2)\n",
    "                                        log_event(\"UNKNOWN_PERSON\", f\"With objects: {', '.join(detected_objects) if detected_objects else 'None'}\")\n",
    "        \n",
    "        \n",
    "                # Display detected objects summary\n",
    "                if detected_objects:\n",
    "                    objects_text = f\"Objects: {', '.join(set(detected_objects))}\"\n",
    "                    cv2.putText(frame, objects_text, (20, 60), \n",
    "                            cv2.FONT_HERSHEY_SIMPLEX, 0.6, (255, 255, 0), 2)\n",
    "                \n",
    "                # Calculate and display FPS\n",
    "                fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "                cv2.putText(frame, f\"FPS: {int(fps)}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "                \n",
    "            \n",
    "                # Display the resulting frame\n",
    "                cv2.imshow('Security Monitoring', frame)\n",
    "                \n",
    "                # Break the loop if 'q' is pressed\n",
    "                if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "                    break\n",
    "                    \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    log_event(\"SYSTEM_ERROR\", str(e))\n",
    "finally:\n",
    "    # Clean up resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pose.close()  # Close MediaPipe resources\n",
    "    pygame.mixer.quit()\n",
    "    log_event(\"SYSTEM_SHUTDOWN\")\n",
    "    print(\"Security monitoring stopped.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ef353ed",
   "metadata": {},
   "source": [
    "# facial detection and recognition without mediapipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eff05f82",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import mediapipe as mp\n",
    "from ultralytics import YOLO\n",
    "# import smtplib\n",
    "import pygame\n",
    "import os\n",
    "import time\n",
    "from datetime import datetime\n",
    "\n",
    "# Initialize MediaPipe Face Detection\n",
    "mp_face_detection = mp.solutions.face_detection\n",
    "mp_drawing = mp.solutions.drawing_utils\n",
    "\n",
    "def detect_faces_mediapipe(frame):\n",
    "    \"\"\"\n",
    "    Detect faces using MediaPipe\n",
    "    Returns list of face bounding boxes in format [x, y, w, h]\n",
    "    \"\"\"\n",
    "    face_boxes = []\n",
    "    \n",
    "    with mp_face_detection.FaceDetection(model_selection=0, min_detection_confidence=0.5) as face_detection:\n",
    "        # Convert BGR to RGB for MediaPipe\n",
    "        rgb_frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "        results = face_detection.process(rgb_frame)\n",
    "        \n",
    "        if results.detections:\n",
    "            h, w, _ = frame.shape\n",
    "            for detection in results.detections:\n",
    "                bbox = detection.location_data.relative_bounding_box\n",
    "                x = int(bbox.xmin * w)\n",
    "                y = int(bbox.ymin * h)\n",
    "                width = int(bbox.width * w)\n",
    "                height = int(bbox.height * h)\n",
    "                face_boxes.append([x, y, width, height])\n",
    "    \n",
    "    return face_boxes\n",
    "\n",
<<<<<<< HEAD
    "def draw_detections(frame, yolo_results, face_boxes):\n",
    "    \"\"\"\n",
    "    Draw YOLO detections and face detections on frame\n",
    "    \"\"\"\n",
    "    result_frame = frame.copy()\n",
    "    \n",
    "    # Draw YOLO detections\n",
    "    if yolo_results and len(yolo_results) > 0:\n",
    "        for result in yolo_results:\n",
    "            if result.boxes is not None:\n",
    "                boxes = result.boxes.xyxy.cpu().numpy()\n",
    "                confidences = result.boxes.conf.cpu().numpy()\n",
    "                classes = result.boxes.cls.cpu().numpy()\n",
    "                \n",
    "                for box, conf, cls in zip(boxes, confidences, classes):\n",
    "                    x1, y1, x2, y2 = map(int, box)\n",
    "                    \n",
    "                    # Get class name\n",
    "                    class_name = result.names[int(cls)] if hasattr(result, 'names') else f\"Class {int(cls)}\"\n",
    "                    \n",
    "                    # Draw bounding box\n",
    "                    cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "                    # Draw label\n",
    "                    label = f\"{class_name}: {conf:.2f}\"\n",
    "                    label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "                    cv2.rectangle(result_frame, (x1, y1 - label_size[1] - 10), \n",
    "                                (x1 + label_size[0], y1), (0, 255, 0), -1)\n",
    "                    cv2.putText(result_frame, label, (x1, y1 - 5), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    \n",
    "    # Draw face detections\n",
    "    for face_box in face_boxes:\n",
    "        x, y, w, h = face_box\n",
    "        # Draw face bounding box in red\n",
    "        cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "        # Draw face label\n",
    "        cv2.rectangle(result_frame, (x, y - 25), (x + 60, y), (0, 0, 255), -1)\n",
    "        cv2.putText(result_frame, \"Face\", (x + 5, y - 5), \n",
    "                   cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "    return result_frame\n",
=======
    "# def draw_detections(frame, yolo_results, face_boxes):\n",
    "#     \"\"\"\n",
    "#     Draw YOLO detections and face detections on frame\n",
    "#     \"\"\"\n",
    "#     result_frame = frame.copy()\n",
    "    \n",
    "#     # Draw YOLO detections\n",
    "#     if yolo_results and len(yolo_results) > 0:\n",
    "#         for result in yolo_results:\n",
    "#             if result.boxes is not None:\n",
    "#                 boxes = result.boxes.xyxy.cpu().numpy()\n",
    "#                 confidences = result.boxes.conf.cpu().numpy()\n",
    "#                 classes = result.boxes.cls.cpu().numpy()\n",
    "                \n",
    "#                 for box, conf, cls in zip(boxes, confidences, classes):\n",
    "#                     x1, y1, x2, y2 = map(int, box)\n",
    "                    \n",
    "#                     # Get class name\n",
    "#                     class_name = result.names[int(cls)] if hasattr(result, 'names') else f\"Class {int(cls)}\"\n",
    "                    \n",
    "#                     # Draw bounding box\n",
    "#                     cv2.rectangle(result_frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "                    \n",
    "#                     # Draw label\n",
    "#                     label = f\"{class_name}: {conf:.2f}\"\n",
    "#                     label_size = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 2)[0]\n",
    "#                     cv2.rectangle(result_frame, (x1, y1 - label_size[1] - 10), \n",
    "#                                 (x1 + label_size[0], y1), (0, 255, 0), -1)\n",
    "#                     cv2.putText(result_frame, label, (x1, y1 - 5), \n",
    "#                               cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 2)\n",
    "    \n",
    "#     # Draw face detections\n",
    "#     for face_box in face_boxes:\n",
    "#         x, y, w, h = face_box\n",
    "#         # Draw face bounding box in red\n",
    "#         cv2.rectangle(result_frame, (x, y), (x + w, y + h), (0, 0, 255), 2)\n",
    "        \n",
    "#         # Draw face label\n",
    "#         cv2.rectangle(result_frame, (x, y - 25), (x + 60, y), (0, 0, 255), -1)\n",
    "#         cv2.putText(result_frame, \"Face\", (x + 5, y - 5), \n",
    "#                    cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n",
    "    \n",
    "#     return result_frame\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "\n",
    "# Initialize YOLO model for object detection\n",
    "# Using YOLOv8n which can detect 80 different object classes\n",
    "# model = YOLO(\"runs/detect/train2/weights/best.pt\")\n",
    "model = YOLO(\"yolo11m.pt\")\n",
    "\n",
    "# Initialize MediaPipe for pose detection\n",
    "mp_pose = mp.solutions.pose\n",
    "pose = mp_pose.Pose(min_detection_confidence=0.5, min_tracking_confidence=0.5)\n",
    "\n",
    "# Load known faces\n",
    "known_face_encodings = []\n",
    "known_face_names = []\n",
    "\n",
    "# Setup for known faces - replace with your implementation\n",
<<<<<<< HEAD
    "# known_faces_dir = \"images\"  # Create this directory and add images\n",
    "known_faces_dir = \"family_members\"  # Create this directory and add images\n",
=======
    "known_faces_dir = \"images\"  # Create this directory and add images\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "if os.path.exists(known_faces_dir):\n",
    "    for person_name in os.listdir(known_faces_dir):\n",
    "        person_dir = os.path.join(known_faces_dir, person_name)\n",
    "        if os.path.isdir(person_dir):\n",
    "            for image_name in os.listdir(person_dir):\n",
    "                image_path = os.path.join(person_dir, image_name)\n",
    "                try:\n",
    "                    image = face_recognition.load_image_file(image_path)\n",
    "                    face_encodings = face_recognition.face_encodings(image)\n",
    "                    if face_encodings:\n",
    "                        known_face_encodings.append(face_encodings[0])\n",
    "                        known_face_names.append(person_name)\n",
    "                        print(f\"Loaded face: {person_name} from {image_name}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"Error loading {image_path}: {e}\")\n",
    "\n",
    "# Print summary of loaded faces\n",
    "if known_face_encodings:\n",
    "    print(f\"Successfully loaded {len(known_face_encodings)} face encodings for {len(set(known_face_names))} people\")\n",
    "else:\n",
    "    print(\"Warning: No face encodings loaded. Face recognition will not work.\")\n",
    "\n",
    "# Setup alarm sound\n",
    "pygame.mixer.init()\n",
    "alarm_file = \"pols-aagyi-pols.mp3\"\n",
    "if os.path.exists(alarm_file):\n",
    "    pygame.mixer.music.load(alarm_file)\n",
    "else:\n",
    "    print(f\"Warning: Alarm file {alarm_file} not found\")\n",
    "\n",
    "# Create log directory\n",
    "log_dir = \"security_logs\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "def log_event(event_type, details=\"\"):\n",
    "    \"\"\"Log security events to file\"\"\"\n",
    "    timestamp = datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    log_file = os.path.join(log_dir, f\"security_log_{datetime.now().strftime('%Y-%m-%d')}.txt\")\n",
    "    with open(log_file, \"a\") as f:\n",
    "        f.write(f\"{timestamp} - {event_type}: {details}\\n\")\n",
    "\n",
    "def  send_email_alert(person_name=\"Unknown\", objects_detected=None):\n",
    "    \"\"\"Function to send email alert when a person is detected.\"\"\"\n",
    "    if objects_detected is None:\n",
    "        objects_detected = []\n",
    "    \n",
    "    objects_str = \", \".join(objects_detected) if objects_detected else \"None\"\n",
    "    log_event(\"ALERT_TRIGGERED\", f\"Person: {person_name}, Objects: {objects_str}\")\n",
    "    print(f\"Alert triggered: {person_name} detected with objects: {objects_str}\")\n",
    "\n",
    "# Start Video Capture\n",
<<<<<<< HEAD
    "cap = cv2.VideoCapture(\"./media_files/WIN_20251103_14_11_20_Pro.mp4\")\n",
    "# cap = cv2.VideoCapture(\"./media_files/animal_surveillance/goru-churi.mp4\")\n",
=======
    "# cap = cv2.VideoCapture(0)\n",
    "cap = cv2.VideoCapture(\"./Test-Video-And-Images/istockphoto-2174886250-640_adpp_is.mp4\")\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "# cap = cv2.VideoCapture(\"./Test-Video-And-Images/istockphoto-1456638008-640_adpp_is.mp4\")\n",
    "# cap = cv2.VideoCapture(\"https://www.youtube.com/watch?v=wswxrDiSiHI\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video capture device\")\n",
    "    exit()\n",
    "\n",
    "# Performance optimization variables\n",
    "frame_count = 0\n",
    "face_recognition_interval = 5  # Process face recognition every 5 frames\n",
    "last_alert_time = 0\n",
    "alert_cooldown = 10  # Seconds between alerts\n",
    "\n",
    "# Define objects of interest (subset of COCO classes that YOLO can detect)\n",
    "objects_of_interest = [\n",
    "    \"person\", \"bicycle\", \"car\", \"motorcycle\", \"bus\", \"truck\", \"mouse\",\n",
    "    \"backpack\", \"umbrella\", \"handbag\", \"tie\", \"suitcase\",\n",
    "    \"cell phone\", \"laptop\", \"book\", \"scissors\", \"knife\", \"face\"\n",
    "]\n",
    "\n",
    "print(\"Security monitoring started. Press 'q' to quit.\")\n",
    "log_event(\"SYSTEM_START\")\n",
    "\n",
    "try:\n",
    "    while True:\n",
    "        # Start a high-resolution timer to measure performance or track elapsed time using OpenCV's getTickCount() method\n",
    "        timer = cv2.getTickCount()\n",
    "        ret, frame = cap.read()\n",
    "        if not ret:\n",
    "            print(\"Failed to grab frame\")\n",
    "            break\n",
    "            \n",
    "        frame_count += 1\n",
    "        # Determine whether to process faces based on frame count interval\n",
    "        # Helps optimize performance by reducing face recognition computations\n",
    "        process_faces = frame_count % face_recognition_interval == 0\n",
    "        current_time = time.time()\n",
    "        \n",
    "        # YOLO Detection for all objects\n",
    "        results = model.track(frame, device='cuda:0', persist=True)\n",
    "        \n",
    "        # Track detected objects in this frame\n",
    "        detected_objects = []\n",
    "        detected_persons = []\n",
    "\n",
    "        for result in results:\n",
    "            boxes = result.boxes\n",
    "            \n",
    "            for i, box in enumerate(boxes):\n",
    "                # Get box coordinates\n",
    "                x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "                \n",
    "                # Ensure coordinates are within frame boundaries\n",
    "                x1, y1 = max(0, x1), max(0, y1)\n",
    "                x2, y2 = min(frame.shape[1], x2), min(frame.shape[0], y2)\n",
    "                \n",
    "                if x2 <= x1 or y2 <= y1:\n",
    "                    continue  # Skip invalid boxes\n",
    "                \n",
    "                # Get class and confidence\n",
    "                cls = int(box.cls[0])\n",
    "                conf = float(box.conf[0])\n",
    "                class_name = result.names[cls]\n",
    "                \n",
    "                # Handle NON-PERSON objects - draw bounding boxes as usual\n",
    "                if class_name in objects_of_interest and class_name != \"person\":\n",
    "                    detected_objects.append(class_name)\n",
    "                    \n",
    "                    # Draw bounding box for object\n",
    "                    cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 255), 2)\n",
    "                    \n",
    "                    # Display object name and confidence\n",
    "                    label = f\"{class_name}: {conf:.2f}\"\n",
    "                    cv2.putText(frame, label, (x1, y1 - 10), \n",
    "                              cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 255, 255), 2)\n",
    "                \n",
    "                # Handle PERSON detection with face recognition\n",
    "                elif class_name == \"person\":\n",
    "                    detected_persons.append((x1, y1, x2, y2))\n",
    "                    person_roi = frame[y1:y2, x1:x2]\n",
    "                    \n",
    "                    # Initialize variables for this person\n",
    "                    person_name = \"UNKNOWN\"\n",
    "                    face_detected = False\n",
    "                    \n",
    "                    # Only proceed if person ROI is valid\n",
    "                    if person_roi.size > 0 and person_roi.shape[0] > 0 and person_roi.shape[1] > 0:\n",
    "                        \n",
    "                        # Face Recognition - only process every few frames for performance\n",
    "                        if process_faces:\n",
    "                            # Resize for faster processing\n",
    "                            small_frame = cv2.resize(person_roi, (0, 0), fx=0.25, fy=0.25)\n",
    "                            rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "                            \n",
    "                            # Detect face locations using face_recognition library\n",
    "                            face_locations = face_recognition.face_locations(rgb_small_frame, model=\"cnn\")\n",
    "                            \n",
    "                            if face_locations:\n",
    "                                face_detected = True\n",
    "                                face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "                                \n",
    "                                # Process each detected face\n",
    "                                for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "                                    # Scale back up face locations since the frame we detected in was scaled to 1/4 size\n",
    "                                    top *= 4\n",
    "                                    right *= 4\n",
    "                                    bottom *= 4\n",
    "                                    left *= 4\n",
    "                                    \n",
    "                                    # Adjust face coordinates to original frame coordinates\n",
    "                                    face_x1 = x1 + left\n",
    "                                    face_y1 = y1 + top\n",
    "                                    face_x2 = x1 + right\n",
    "                                    face_y2 = y1 + bottom\n",
    "                                    \n",
    "                                    # Ensure face coordinates are within person ROI bounds\n",
    "                                    face_x1 = max(x1, min(face_x1, x2))\n",
    "                                    face_y1 = max(y1, min(face_y1, y2))\n",
    "                                    face_x2 = max(x1, min(face_x2, x2))\n",
    "                                    face_y2 = max(y1, min(face_y2, y2))\n",
    "                                    \n",
    "                                    # Compare with known faces if available\n",
    "                                    if known_face_encodings:\n",
    "                                        matches = face_recognition.compare_faces(known_face_encodings, face_encoding, tolerance=0.6)\n",
    "                                        \n",
    "                                        if any(matches):\n",
    "                                            # Find the best match\n",
    "                                            face_distances = face_recognition.face_distance(known_face_encodings, face_encoding)\n",
    "                                            best_match_index = np.argmin(face_distances)\n",
    "                                            if matches[best_match_index]:\n",
    "                                                person_name = known_face_names[best_match_index]\n",
    "                                                print(f\" Known Person Detected: {person_name}\")\n",
    "                                                \n",
    "                                                # Alert with cooldown for known person\n",
    "                                                if current_time - last_alert_time > alert_cooldown:\n",
    "                                                    if not pygame.mixer.music.get_busy():\n",
    "                                                        pygame.mixer.music.play()\n",
    "                                                    log_event(\"KNOWN_PERSON\", f\"Detected: {person_name} with objects: {', '.join(detected_objects) if detected_objects else 'None'}\")\n",
    "                                                    last_alert_time = current_time\n",
    "                                        else:\n",
    "                                            person_name = \"UNKNOWN\"\n",
    "                                            print(\" Unknown Person Detected!\")\n",
    "                                            log_event(\"UNKNOWN_PERSON\", f\"With objects: {', '.join(detected_objects) if detected_objects else 'None'}\")\n",
    "                                    \n",
    "                                    # Draw face bounding box\n",
    "                                    face_color = (0, 255, 0) if person_name != \"UNKNOWN\" else (0, 0, 255)\n",
    "                                    cv2.rectangle(frame, (face_x1, face_y1), (face_x2, face_y2), face_color, 2)\n",
    "                                    \n",
    "                                    # Draw face label with name\n",
    "                                    face_label = f\"Face: {person_name}\"\n",
    "                                    label_size = cv2.getTextSize(face_label, cv2.FONT_HERSHEY_SIMPLEX, 0.4, 2)[0]\n",
    "                                    cv2.rectangle(frame, (face_x1, face_y1 - label_size[1] - 10), \n",
    "                                                (face_x1 + label_size[0], face_y1), face_color, -1)\n",
    "                                    cv2.putText(frame, face_label, (face_x1, face_y1 - 5), \n",
    "                                              cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 2)\n",
    "                                    \n",
    "                        \n",
    "                        # Alternative: Use MediaPipe for face detection if face_recognition didn't find faces\n",
    "                        if not face_detected:\n",
    "                            face_boxes = detect_faces_mediapipe(person_roi)\n",
    "                            if face_boxes:\n",
    "                                face_detected = True\n",
    "                                for face_box in face_boxes:\n",
    "                                    fx, fy, fw, fh = face_box\n",
    "                                    # Adjust face coordinates to original frame coordinates\n",
    "                                    face_x1 = x1 + fx\n",
    "                                    face_y1 = y1 + fy\n",
    "                                    face_x2 = face_x1 + fw\n",
    "                                    face_y2 = face_y1 + fh\n",
    "                                    \n",
    "                                    # Draw face bounding box in red for unknown (MediaPipe detection)\n",
    "                                    cv2.rectangle(frame, (face_x1, face_y1), (face_x2, face_y2), (0, 0, 255), 2)\n",
    "                                    cv2.putText(frame, \"Face\", (face_x1, face_y1 - 5), \n",
    "                                              cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 255), 2)\n",
    "                    \n",
    "                    # Draw person bounding box only if no face was detected\n",
<<<<<<< HEAD
    "                    if not face_detected:\n",
    "                        cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                        cv2.putText(frame, \"Person - No Face\", (x1, y1 - 10), \n",
    "                                  cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 2)\n",
=======
    "                    # if not face_detected:\n",
    "                    #     cv2.rectangle(frame, (x1, y1), (x2, y2), (255, 0, 0), 2)\n",
    "                    #     cv2.putText(frame, \"Person - No Face\", (x1, y1 - 10), \n",
    "                    #               cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 0, 0), 2)\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "        \n",
    "        # Display detected objects summary (optional)\n",
    "        if detected_objects:\n",
    "            objects_text = f\"Objects: {', '.join(set(detected_objects))}\"\n",
    "            cv2.putText(frame, objects_text, (20, 60), \n",
    "                      cv2.FONT_HERSHEY_SIMPLEX, 0.4, (255, 255, 0), 2)\n",
    "        \n",
    "        # Calculate and display FPS (optional)\n",
    "        fps = cv2.getTickFrequency() / (cv2.getTickCount() - timer)\n",
    "        cv2.putText(frame, f\"FPS: {int(fps)}\", (20, 30), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2)\n",
    "        \n",
    "        # Display the resulting frame\n",
    "        cv2.imshow('Security Monitoring', frame)\n",
    "        \n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            break\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f\"An error occurred: {e}\")\n",
    "    log_event(\"SYSTEM_ERROR\", str(e))\n",
    "finally:\n",
    "    # Clean up resources\n",
    "    cap.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    pose.close()  # Close MediaPipe resources\n",
    "    pygame.mixer.quit()\n",
    "    log_event(\"SYSTEM_SHUTDOWN\")\n",
    "    print(\"Security monitoring stopped.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8299cea8",
   "metadata": {},
   "source": [
    "# Tracking people with YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4488795",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import solutions\n",
    "\n",
    "# Open the video file\n",
<<<<<<< HEAD
    "cap = cv2.VideoCapture(\"./media_files/people walking/computer_vision_object_and_detection_tracking_people_walking_video_20250819_173636_1.mp4\")\n",
=======
    "cap = cv2.VideoCapture(\"Test-Video-And-Images/istockphoto-655785208-640_adpp_is.mp4\")\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    "# cap = cv2.VideoCapture(\"Test-Video-And-Images/istockphoto-1496613432-640_adpp_is (2).mp4\")\n",
    "assert cap.isOpened(), \"Error: Could not open video file. Please check if the file path is correct and the file exists.\"\n",
    "# Get video properties: width, height, and frames per second (fps)\n",
    "w, h, fps = (int(cap.get(x)) for x in (cv2.CAP_PROP_FRAME_WIDTH, cv2.CAP_PROP_FRAME_HEIGHT, cv2.CAP_PROP_FPS))\n",
    "\n",
    "# Define points for a line or region of interest in the video frame\n",
    "line_points = [(0, 250), (1080, 250)]  # Line coordinates near top of screen\n",
    "\n",
    "# Initialize the video writer to save the output video\n",
    "video_writer = cv2.VideoWriter(\"object_counting_output.avi\", cv2.VideoWriter_fourcc(*\"mp4v\"), fps, (w, h))\n",
    "\n",
    "# Initialize the Object Counter with visualization options and other parameters\n",
    "counter = solutions.ObjectCounter(\n",
    "    show=True,  # Display the image during processing\n",
    "    region=line_points,  # Region of interest points\n",
    "    model=\"yolo11m.pt\",  # Ultralytics YOLO11 model file\n",
    "    line_width=2,  # Thickness of the lines and bounding boxes\n",
    "    conf=0.5,  # Confidence threshold for detections\n",
<<<<<<< HEAD
    "    iou=0.10\n",
=======
    "    iou=0.99\n",
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
    ")\n",
    "\n",
    "# Process video frames in a loop\n",
    "while cap.isOpened():\n",
    "    success, im0 = cap.read()\n",
    "    if not success:\n",
    "        print(\"Video frame is empty or video processing has been successfully completed.\")\n",
    "        break\n",
    "\n",
    "    # Use the Object Counter to count objects in the frame and get the annotated image\n",
    "    results = counter(im0)\n",
    "    print(f\"Objects counted: {results}\")  # Print the counts of objects detected\n",
    "\n",
    "    # Write the annotated frame to the output video\n",
    "    video_writer.write(results.plot_im)\n",
    "\n",
    "# Release the video capture and writer objects\n",
    "cap.release()\n",
    "video_writer.release()\n",
    "\n",
    "# Close all OpenCV windows\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf12d307",
   "metadata": {},
   "source": [
    "### Run YOLO11 tracking on the frame, persisting tracks between frames intersecting not provide different IDs for tracking using iou=0.10 value for the intersection over union threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8fff917",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Open the video file\n",
    "video_path = \"/media_files/animal survilance/    __     720p.mp4\"\n",
    "cap = cv2.VideoCapture(video_path)\n",
    "\n",
    "# Loop through the video frames\n",
    "while cap.isOpened():\n",
    "    # Read a frame from the video\n",
    "    success, frame = cap.read()\n",
    "\n",
    "    if success:\n",
    "        # Run YOLO11 tracking on the frame, persisting tracks between frames intersecting not provide different IDs\n",
    "        results = model.track(\n",
    "            frame, \n",
    "            persist=True,\n",
    "            show=True,\n",
    "            classes=[0], \n",
    "            iou=0.10, \n",
    "            conf=0.5\n",
    "            )\n",
    "\n",
    "        # Visualize the results on the frame\n",
    "        annotated_frame = results[0].plot()\n",
    "\n",
    "        # Display the annotated frame\n",
    "        cv2.imshow(\"YOLO11 Tracking\", annotated_frame)\n",
    "\n",
    "        # Break the loop if 'q' is pressed\n",
    "        if cv2.waitKey(1) & 0xFF == ord(\"q\"):\n",
    "            break\n",
    "    else:\n",
    "        # Break the loop if the end of the video is reached\n",
    "        break\n",
    "\n",
    "# Release the video capture object and close the display window\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "148b1f33",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install --no-cache-dir --upgrade \"tf_keras\" \"sng4onnx>=1.0.1\" \"onnx_graphsurgeon>=0.3.26\" \"ai-edge-litert>=1.2.0\" \"onnx2tf>=1.26.3\" \"onnxruntime>=1.15.0\" \"protobuf>=5\" --extra-index-url https://pypi.ngc.nvidia.com\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7812ac0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a YOLO11n PyTorch model to ONNX format\n",
    "!yolo export model=yolo11n.pt format=onnx # creates 'yolo11n.onnx'\n",
    "\n",
    "# Run inference with the exported model\n",
<<<<<<< HEAD
    "!yolo predict model=yolo11n.onnx source='https://ultralytics.com/images/bus.jpg' imgsz=320"
=======
    "!yolo predict model=yolo11n.onnx source='https://ultralytics.com/images/bus.jpg'"
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65d10d61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Export the model to TF.js format\n",
    "model.export(format=\"tfjs\")  # creates '/yolo11n_web_model'\n",
    "\n",
    "# # Load the exported TF.js model\n",
    "# tfjs_model = YOLO(\"./yolo11n_web_model\")\n",
    "\n",
    "# # Run inference\n",
    "# results = tfjs_model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a36c7925",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Export the model to TF.js format\n",
    "model.export(format=\"tfjs\")  # creates '/yolo11n_web_model'\n",
    "\n",
    "# # Load the exported TF.js model\n",
    "# tfjs_model = YOLO(\"./yolo11n_web_model\")\n",
    "\n",
    "# # Run inference\n",
    "# results = tfjs_model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "300c149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Export the model to TF.js format\n",
    "model.export(format=\"tfjs\")  # creates '/yolo11n_web_model'\n",
    "\n",
    "# # Load the exported TF.js model\n",
    "# tfjs_model = YOLO(\"./yolo11n_web_model\")\n",
    "\n",
    "# # Run inference\n",
    "# results = tfjs_model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fc2c336",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export a YOLO11n PyTorch model to ONNX format\n",
    "!yolo export model=yolo11n.pt format=onnx # creates 'yolo11n.onnx'\n",
    "\n",
    "# Run inference with the exported model\n",
    "!yolo predict model=yolo11n.onnx source='https://ultralytics.com/images/bus.jpg'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "760b2a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Export the model to TF.js format\n",
    "model.export(format=\"tfjs\")  # creates '/yolo11n_web_model'\n",
    "\n",
    "# # Load the exported TF.js model\n",
    "# tfjs_model = YOLO(\"./yolo11n_web_model\")\n",
    "\n",
    "# # Run inference\n",
    "# results = tfjs_model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69664436",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "\n",
    "# Load the YOLO11 model\n",
    "model = YOLO(\"yolo11n.pt\")\n",
    "\n",
    "# Export the model to TF.js format\n",
    "model.export(format=\"tfjs\")  # creates '/yolo11n_web_model'\n",
    "\n",
    "# # Load the exported TF.js model\n",
    "# tfjs_model = YOLO(\"./yolo11n_web_model\")\n",
    "\n",
    "# # Run inference\n",
    "# results = tfjs_model(\"https://ultralytics.com/images/bus.jpg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfa55a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install wandb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84751b10",
   "metadata": {},
   "outputs": [],
   "source": [
    "!wandb --version"
   ]
<<<<<<< HEAD
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ecc8537",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ...existing code...\n",
    "class FaceRecognitionAlarm(solutions.SecurityAlarm):\n",
    "    # ...existing code...\n",
    "\n",
    "    def __init__(self, face_data_path, **kwargs):\n",
    "        # ...existing code...\n",
    "        # New tunables\n",
    "        self.face_tolerance = kwargs.get(\"face_tolerance\", 0.55)      # L2 distance threshold\n",
    "        self.recognition_min_votes = kwargs.get(\"recognition_min_votes\", 2)\n",
    "        self.recognition_window = kwargs.get(\"recognition_window\", 3) # consecutive frames to confirm\n",
    "        self.recognition_interval = kwargs.get(\"recognition_interval\", 1)  # compute encodings every N frames\n",
    "        self._frame_counter = 0\n",
    "        # per-track persistent recognition state: {track_id: {'name': str, 'votes': int, 'last_seen': int}}\n",
    "        self._track_recognition = {}\n",
    "        # ...existing code...\n",
    "\n",
    "    def __call__(self, im0, results=None, annotator=None):\n",
    "        # ...existing code until face detection setup ...\n",
    "\n",
    "        # Use explicit resize scale\n",
    "        scale = 0.25\n",
    "        try:\n",
    "            small_frame = cv2.resize(im0, (0, 0), fx=scale, fy=scale)\n",
    "            rgb_small_frame = cv2.cvtColor(small_frame, cv2.COLOR_BGR2RGB)\n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"Error preparing frame for face recognition: {e}\")\n",
    "            rgb_small_frame = None\n",
    "\n",
    "        face_locations = []\n",
    "        face_encodings = []\n",
    "        if rgb_small_frame is not None and (self._frame_counter % self.recognition_interval == 0):\n",
    "            try:\n",
    "                face_locations = face_recognition.face_locations(rgb_small_frame)\n",
    "                face_encodings = face_recognition.face_encodings(rgb_small_frame, face_locations)\n",
    "            except Exception as e:\n",
    "                LOGGER.error(f\"Face recognition error: {e}\")\n",
    "                face_locations = []\n",
    "                face_encodings = []\n",
    "        self._frame_counter += 1\n",
    "\n",
    "        # Local counter of unknown persons this frame (do not mutate parent's clss)\n",
    "        unknown_person_count = 0\n",
    "\n",
    "        # If tracker IDs exist, use them; otherwise fallback to index-based ephemeral ids\n",
    "        use_tracks = bool(getattr(self, \"track_ids\", None))\n",
    "        person_cls_id = 0\n",
    "\n",
    "        for i, box in enumerate(boxes_list):\n",
    "            # safe class id\n",
    "            cls_id = classes_list[i] if i < len(classes_list) else None\n",
    "            label = names_map.get(cls_id, str(cls_id)) if isinstance(names_map, dict) else str(cls_id)\n",
    "            color = colors(cls_id, True) if isinstance(names_map, dict) else (255, 255, 255)\n",
    "\n",
    "            if cls_id != person_cls_id and cls_id is not None:\n",
    "                # draw non-person boxes normally\n",
    "                try:\n",
    "                    annotator.box_label(box, label=label, color=color)\n",
    "                except Exception:\n",
    "                    pass\n",
    "                continue\n",
    "\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            track_id = self.track_ids[i] if (use_tracks and i < len(self.track_ids)) else f\"idx_{i}\"\n",
    "\n",
    "            # default state for this detection\n",
    "            confirmed_name = None\n",
    "            face_found = False\n",
    "\n",
    "            # Associate faces by center point (scale back to original using scale)\n",
    "            for (top, right, bottom, left), face_encoding in zip(face_locations, face_encodings):\n",
    "                # scale face coords back to original size\n",
    "                top  = int(top  / scale)\n",
    "                right= int(right/ scale)\n",
    "                bottom=int(bottom/scale)\n",
    "                left = int(left / scale)\n",
    "\n",
    "                cx = (left + right) // 2\n",
    "                cy = (top + bottom) // 2\n",
    "                if x1 <= cx <= x2 and y1 <= cy <= y2:\n",
    "                    face_found = True\n",
    "                    if self.known_face_encodings:\n",
    "                        # compute distances and pick best\n",
    "                        dists = face_recognition.face_distance(self.known_face_encodings, face_encoding)\n",
    "                        best_idx = int(np.argmin(dists))\n",
    "                        best_dist = float(dists[best_idx])\n",
    "                        if best_dist <= self.face_tolerance:\n",
    "                            candidate_name = self.known_face_names[best_idx]\n",
    "                        else:\n",
    "                            candidate_name = \"Unknown\"\n",
    "                    else:\n",
    "                        candidate_name = \"Unknown\"\n",
    "\n",
    "                    # update per-track recognition votes\n",
    "                    tr = self._track_recognition.setdefault(track_id, {\"name\": None, \"votes\": 0, \"frames\": 0})\n",
    "                    # simple voting: if same candidate as last frame increment votes, else reset\n",
    "                    if tr[\"name\"] == candidate_name:\n",
    "                        tr[\"votes\"] = min(tr[\"votes\"] + 1, self.recognition_window)\n",
    "                    else:\n",
    "                        tr[\"name\"] = candidate_name\n",
    "                        tr[\"votes\"] = 1\n",
    "                    tr[\"frames\"] += 1\n",
    "                    tr[\"last_seen\"] = 0  # reset last_seen since we just saw\n",
    "                    # confirm once votes >= recognition_min_votes\n",
    "                    if tr[\"name\"] != \"Unknown\" and tr[\"votes\"] >= self.recognition_min_votes:\n",
    "                        confirmed_name = tr[\"name\"]\n",
    "                    elif tr[\"name\"] == \"Unknown\" and tr[\"votes\"] >= 1:\n",
    "                        # unknown can be counted quickly; tune this if too sensitive\n",
    "                        confirmed_name = \"Unknown\"\n",
    "                    break  # matched face for this person box\n",
    "\n",
    "            # if no face found, decrement last_seen counters for track state (aging)\n",
    "            if not face_found and track_id in self._track_recognition:\n",
    "                tr = self._track_recognition[track_id]\n",
    "                tr[\"last_seen\"] = tr.get(\"last_seen\", 0) + 1\n",
    "                # optionally decay votes over time\n",
    "                if tr[\"last_seen\"] > (self.recognition_window * 2):\n",
    "                    tr[\"votes\"] = max(0, tr[\"votes\"] - 1)\n",
    "\n",
    "            # decide label/color\n",
    "            if confirmed_name is None:\n",
    "                # not yet confirmed; keep \"No Face\" or original label\n",
    "                if face_found:\n",
    "                    label = \"Unknown (checking...)\"\n",
    "                    color = (0, 0, 255)\n",
    "                else:\n",
    "                    label = f\"{names_map.get(cls_id, 'person')} (No Face)\"\n",
    "                    color = (200, 200, 200)\n",
    "            elif confirmed_name == \"Unknown\":\n",
    "                label = \"Unknown (ALARM!)\"\n",
    "                color = (0, 0, 255)\n",
    "                unknown_person_count += 1\n",
    "            else:\n",
    "                label = f\"{confirmed_name} (Known)\"\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "            # Draw\n",
    "            try:\n",
    "                annotator.box_label(box, label=label, color=color)\n",
    "            except Exception:\n",
    "                x1_, y1_, x2_, y2_ = x1, y1, x2, y2\n",
    "                cv2.rectangle(im0, (x1_, y1_), (x2_, y2_), color, 2)\n",
    "                if label:\n",
    "                    cv2.putText(im0, label, (x1_ , y1_ - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "        # Alarm control (do not mutate parent's detection lists)\n",
    "        if unknown_person_count >= self.records:\n",
    "            if not self.email_sent:\n",
    "                # self.send_email(im0, unknown_person_count)  # uncomment if available\n",
    "                self.email_sent = True\n",
    "                LOGGER.info(f\" Email alert condition met for {unknown_person_count} unknown person(s).\")\n",
    "            if not self.sound_played:\n",
    "                if pygame.mixer.get_init() and not pygame.mixer.music.get_busy():\n",
    "                    try:\n",
    "                        pygame.mixer.music.play()\n",
    "                        self.sound_played = True\n",
    "                        LOGGER.info(\" Playing security alarm!\")\n",
    "                    except Exception as e:\n",
    "                        LOGGER.error(f\"Failed to play alarm sound: {e}\")\n",
    "        else:\n",
    "            # reset\n",
    "            if self.sound_played:\n",
    "                try:\n",
    "                    pygame.mixer.music.stop()\n",
    "                except Exception:\n",
    "                    pass\n",
    "            self.email_sent = False\n",
    "            self.sound_played = False\n",
    "\n",
    "        plot_im = annotator.result()\n",
    "        try:\n",
    "            self.display_output(plot_im)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        # age / cleanup track_recognition entries occasionally\n",
    "        # remove entries not seen for a while\n",
    "        to_del = []\n",
    "        for tid, tr in self._track_recognition.items():\n",
    "            if tr.get(\"last_seen\", 0) > 150:\n",
    "                to_del.append(tid)\n",
    "            else:\n",
    "                tr[\"last_seen\"] = tr.get(\"last_seen\", 0) + 1\n",
    "        for tid in to_del:\n",
    "            del self._track_recognition[tid]\n",
    "\n",
    "        return SolutionResults(plot_im=plot_im, im0=im0, total_tracks=len(getattr(self, \"track_ids\", [])), email_sent=self.email_sent, sound_played=self.sound_played)\n",
    "# ...existing code..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3e433",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pygame 2.6.1 (SDL 2.28.4, Python 3.12.8)\n",
      "Hello from the pygame community. https://www.pygame.org/contribute.html\n",
      "Ultralytics Solutions:  {'source': None, 'model': 'yolo11m.pt', 'classes': None, 'show_conf': True, 'show_labels': True, 'region': None, 'colormap': 21, 'show_in': True, 'show_out': True, 'up_angle': 145.0, 'down_angle': 90, 'kpts': [6, 8, 10], 'analytics_type': 'line', 'figsize': (12.8, 7.2), 'blur_ratio': 0.5, 'vision_point': (20, 20), 'crop_dir': 'cropped-detections', 'json_file': None, 'line_width': 2, 'records': 1, 'fps': 30.0, 'max_hist': 5, 'meter_per_pixel': 0.05, 'max_speed': 120, 'show': True, 'iou': 0.7, 'conf': 0.5, 'device': None, 'max_det': 300, 'half': False, 'tracker': 'botsort.yaml', 'verbose': True, 'data': 'images'}\n",
      "--- Starting Video Processing. Press 'q' to terminate. ---\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 76.7ms\n",
      "Speed: 2.6ms preprocess, 76.7ms inference, 130.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 46.6ms\n",
      "Speed: 3.3ms preprocess, 46.6ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 46.8ms\n",
      "Speed: 2.5ms preprocess, 46.8ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.9ms\n",
      "Speed: 2.8ms preprocess, 47.9ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 47.9ms\n",
      "Speed: 2.3ms preprocess, 47.9ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 48.6ms\n",
      "Speed: 2.9ms preprocess, 48.6ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 47.2ms\n",
      "Speed: 2.8ms preprocess, 47.2ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 47.6ms\n",
      "Speed: 2.6ms preprocess, 47.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 47.6ms\n",
      "Speed: 3.0ms preprocess, 47.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 couch, 1 potted plant, 1 laptop, 46.3ms\n",
      "Speed: 2.7ms preprocess, 46.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 52.2ms\n",
      "Speed: 3.3ms preprocess, 52.2ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 45.7ms\n",
      "Speed: 2.5ms preprocess, 45.7ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 49.6ms\n",
      "Speed: 2.2ms preprocess, 49.6ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 46.0ms\n",
      "Speed: 3.0ms preprocess, 46.0ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 45.9ms\n",
      "Speed: 2.9ms preprocess, 45.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 52.8ms\n",
      "Speed: 2.8ms preprocess, 52.8ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 47.6ms\n",
      "Speed: 2.5ms preprocess, 47.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 47.5ms\n",
      "Speed: 2.6ms preprocess, 47.5ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 3.0ms preprocess, 47.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.0ms preprocess, 47.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.8ms\n",
      "Speed: 2.9ms preprocess, 47.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.5ms\n",
      "Speed: 2.8ms preprocess, 47.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.1ms\n",
      "Speed: 3.1ms preprocess, 50.1ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.6ms\n",
      "Speed: 2.7ms preprocess, 47.6ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 48.3ms\n",
      "Speed: 3.0ms preprocess, 48.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.8ms preprocess, 47.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 51.8ms\n",
      "Speed: 2.6ms preprocess, 51.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 53.7ms\n",
      "Speed: 2.7ms preprocess, 53.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 47.5ms\n",
      "Speed: 2.7ms preprocess, 47.5ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 47.6ms\n",
      "Speed: 2.7ms preprocess, 47.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.7ms\n",
      "Speed: 2.9ms preprocess, 52.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.5ms\n",
      "Speed: 2.6ms preprocess, 47.5ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.9ms preprocess, 47.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.4ms\n",
      "Speed: 2.3ms preprocess, 47.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.4ms preprocess, 47.7ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 laptop, 47.3ms\n",
      "Speed: 2.8ms preprocess, 47.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.9ms\n",
      "Speed: 3.0ms preprocess, 47.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.8ms\n",
      "Speed: 2.8ms preprocess, 47.8ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.8ms\n",
      "Speed: 2.8ms preprocess, 47.8ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.5ms\n",
      "Speed: 3.6ms preprocess, 47.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.6ms preprocess, 47.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.8ms\n",
      "Speed: 2.8ms preprocess, 47.8ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 laptop, 47.6ms\n",
      "Speed: 2.8ms preprocess, 47.6ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.6ms preprocess, 47.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.9ms\n",
      "Speed: 2.9ms preprocess, 47.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.4ms\n",
      "Speed: 3.1ms preprocess, 47.4ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 47.5ms\n",
      "Speed: 2.5ms preprocess, 47.5ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.5ms\n",
      "Speed: 3.5ms preprocess, 47.5ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 48.4ms\n",
      "Speed: 2.9ms preprocess, 48.4ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.5ms\n",
      "Speed: 2.9ms preprocess, 47.5ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.8ms\n",
      "Speed: 2.6ms preprocess, 47.8ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.9ms\n",
      "Speed: 3.3ms preprocess, 47.9ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.9ms\n",
      "Speed: 2.8ms preprocess, 47.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.0ms\n",
      "Speed: 2.4ms preprocess, 49.0ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 couch, 1 potted plant, 1 laptop, 47.9ms\n",
      "Speed: 3.1ms preprocess, 47.9ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.6ms\n",
      "Speed: 2.7ms preprocess, 47.6ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.8ms preprocess, 47.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.5ms\n",
      "Speed: 2.8ms preprocess, 47.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 47.7ms\n",
      "Speed: 3.0ms preprocess, 47.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 47.9ms\n",
      "Speed: 3.3ms preprocess, 47.9ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.5ms preprocess, 47.7ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.7ms\n",
      "Speed: 2.7ms preprocess, 47.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.9ms\n",
      "Speed: 2.7ms preprocess, 47.9ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 47.6ms\n",
      "Speed: 2.6ms preprocess, 47.6ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.3ms\n",
      "Speed: 2.7ms preprocess, 49.3ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 48.5ms\n",
      "Speed: 3.2ms preprocess, 48.5ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 48.7ms\n",
      "Speed: 2.3ms preprocess, 48.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.5ms\n",
      "Speed: 2.7ms preprocess, 50.5ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 laptop, 49.3ms\n",
      "Speed: 2.9ms preprocess, 49.3ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 55.3ms\n",
      "Speed: 2.8ms preprocess, 55.3ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 53.9ms\n",
      "Speed: 2.7ms preprocess, 53.9ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 laptop, 49.7ms\n",
      "Speed: 3.4ms preprocess, 49.7ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 48.8ms\n",
      "Speed: 3.2ms preprocess, 48.8ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 50.0ms\n",
      "Speed: 2.9ms preprocess, 50.0ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 48.3ms\n",
      "Speed: 2.8ms preprocess, 48.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 49.3ms\n",
      "Speed: 2.4ms preprocess, 49.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 50.1ms\n",
      "Speed: 3.1ms preprocess, 50.1ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 53.4ms\n",
      "Speed: 3.1ms preprocess, 53.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 51.3ms\n",
      "Speed: 3.4ms preprocess, 51.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 laptop, 49.4ms\n",
      "Speed: 2.4ms preprocess, 49.4ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.2ms\n",
      "Speed: 2.4ms preprocess, 50.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.4ms\n",
      "Speed: 3.6ms preprocess, 50.4ms inference, 5.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 49.5ms\n",
      "Speed: 2.2ms preprocess, 49.5ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.3ms\n",
      "Speed: 2.2ms preprocess, 52.3ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 50.4ms\n",
      "Speed: 2.9ms preprocess, 50.4ms inference, 5.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 50.7ms\n",
      "Speed: 2.1ms preprocess, 50.7ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 50.0ms\n",
      "Speed: 2.5ms preprocess, 50.0ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 50.0ms\n",
      "Speed: 3.3ms preprocess, 50.0ms inference, 5.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 49.6ms\n",
      "Speed: 2.3ms preprocess, 49.6ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 52.2ms\n",
      "Speed: 2.9ms preprocess, 52.2ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.3ms\n",
      "Speed: 3.1ms preprocess, 52.3ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 51.7ms\n",
      "Speed: 2.4ms preprocess, 51.7ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 51.9ms\n",
      "Speed: 2.3ms preprocess, 51.9ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.1ms\n",
      "Speed: 2.7ms preprocess, 50.1ms inference, 6.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.5ms\n",
      "Speed: 3.0ms preprocess, 52.5ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 52.5ms\n",
      "Speed: 3.0ms preprocess, 52.5ms inference, 3.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.6ms\n",
      "Speed: 2.0ms preprocess, 49.6ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.7ms\n",
      "Speed: 3.0ms preprocess, 49.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.4ms\n",
      "Speed: 2.7ms preprocess, 52.4ms inference, 5.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 laptop, 50.3ms\n",
      "Speed: 3.1ms preprocess, 50.3ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.7ms\n",
      "Speed: 2.5ms preprocess, 52.7ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.1ms\n",
      "Speed: 3.2ms preprocess, 52.1ms inference, 3.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 51.7ms\n",
      "Speed: 2.8ms preprocess, 51.7ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 53.2ms\n",
      "Speed: 2.6ms preprocess, 53.2ms inference, 4.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 50.1ms\n",
      "Speed: 3.6ms preprocess, 50.1ms inference, 3.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 49.7ms\n",
      "Speed: 2.1ms preprocess, 49.7ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.5ms\n",
      "Speed: 3.1ms preprocess, 52.5ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.3ms\n",
      "Speed: 2.7ms preprocess, 50.3ms inference, 4.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.5ms\n",
      "Speed: 2.3ms preprocess, 49.5ms inference, 5.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.7ms\n",
      "Speed: 2.4ms preprocess, 49.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.2ms\n",
      "Speed: 2.8ms preprocess, 50.2ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.8ms\n",
      "Speed: 2.7ms preprocess, 49.8ms inference, 6.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 potted plant, 1 laptop, 53.3ms\n",
      "Speed: 2.7ms preprocess, 53.3ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.3ms\n",
      "Speed: 2.8ms preprocess, 50.3ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 50.2ms\n",
      "Speed: 4.1ms preprocess, 50.2ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 laptop, 51.8ms\n",
      "Speed: 2.8ms preprocess, 51.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 51.4ms\n",
      "Speed: 2.5ms preprocess, 51.4ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 couch, 1 potted plant, 1 laptop, 50.4ms\n",
      "Speed: 2.3ms preprocess, 50.4ms inference, 3.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 51.8ms\n",
      "Speed: 2.4ms preprocess, 51.8ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.8ms\n",
      "Speed: 2.8ms preprocess, 52.8ms inference, 2.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.1ms\n",
      "Speed: 2.9ms preprocess, 52.1ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 49.4ms\n",
      "Speed: 3.6ms preprocess, 49.4ms inference, 4.5ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 49.2ms\n",
      "Speed: 2.4ms preprocess, 49.2ms inference, 4.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 49.3ms\n",
      "Speed: 3.0ms preprocess, 49.3ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 50.2ms\n",
      "Speed: 2.6ms preprocess, 50.2ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 48.9ms\n",
      "Speed: 2.2ms preprocess, 48.9ms inference, 4.6ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 couch, 1 laptop, 49.5ms\n",
      "Speed: 3.0ms preprocess, 49.5ms inference, 3.3ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 49.3ms\n",
      "Speed: 3.8ms preprocess, 49.3ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 54.7ms\n",
      "Speed: 3.0ms preprocess, 54.7ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 50.4ms\n",
      "Speed: 2.7ms preprocess, 50.4ms inference, 2.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 50.8ms\n",
      "Speed: 3.2ms preprocess, 50.8ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 49.1ms\n",
      "Speed: 2.1ms preprocess, 49.1ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 49.0ms\n",
      "Speed: 2.4ms preprocess, 49.0ms inference, 5.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 50.3ms\n",
      "Speed: 2.6ms preprocess, 50.3ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 50.8ms\n",
      "Speed: 3.5ms preprocess, 50.8ms inference, 3.9ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 48.8ms\n",
      "Speed: 2.7ms preprocess, 48.8ms inference, 3.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 53.7ms\n",
      "Speed: 2.4ms preprocess, 53.7ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 48.9ms\n",
      "Speed: 2.0ms preprocess, 48.9ms inference, 3.2ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 potted plant, 1 laptop, 55.2ms\n",
      "Speed: 3.2ms preprocess, 55.2ms inference, 4.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 cup, 1 chair, 1 laptop, 51.4ms\n",
      "Speed: 2.9ms preprocess, 51.4ms inference, 3.7ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 potted plant, 1 laptop, 52.7ms\n",
      "Speed: 2.7ms preprocess, 52.7ms inference, 4.4ms postprocess per image at shape (1, 3, 384, 640)\n",
      "\n",
      "0: 384x640 1 person, 1 chair, 1 laptop, 52.4ms\n",
      "Speed: 3.5ms preprocess, 52.4ms inference, 4.0ms postprocess per image at shape (1, 3, 384, 640)\n"
     ]
    }
   ],
   "source": [
    "# ...existing code...\n",
    "\"\"\"\n",
    "security_alarm.py\n",
    "FaceRecognitionAlarm: extends ultralytics.solutions.SecurityAlarm with robust face-recognition logic\n",
    "to avoid false alarms for known family members by using:\n",
    " - single resized-frame face detection + encodings per interval\n",
    " - per-track voting / persistence\n",
    " - configurable distance threshold (face_tolerance) and vote thresholds\n",
    " - alarm cooldown and non-destructive alarm logic (does not mutate parent's detection lists)\n",
    "\"\"\"\n",
    "import os\n",
    "import cv2\n",
    "import time\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import List, Tuple, Dict, Optional\n",
    "\n",
    "import numpy as np\n",
    "import face_recognition\n",
    "import pygame\n",
    "\n",
    "from ultralytics import solutions\n",
    "from ultralytics.solutions.solutions import SolutionAnnotator, SolutionResults\n",
    "from ultralytics.utils.plotting import colors\n",
    "from ultralytics.utils import LOGGER\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "KNOWN_FACE_DIR = \"family_members\"\n",
    "# initialize pygame mixer once\n",
    "try:\n",
    "    pygame.mixer.init()\n",
    "except Exception:\n",
    "    logger.debug(\"pygame.mixer.init() failed or already initialized\")\n",
    "\n",
    "class FaceRecognitionAlarm(solutions.SecurityAlarm):\n",
    "    \"\"\"\n",
    "    FaceRecognitionAlarm integrates face_recognition with the base SecurityAlarm.\n",
    "    Key behaviors:\n",
    "      - Loads known faces from 'face_data_path' structured as: face_data_path/person_name/image.jpg\n",
    "      - Runs face detection/encoding on a resized frame every `recognition_interval` frames\n",
    "      - Associates detected faces to YOLO person boxes by face-center-in-box test\n",
    "      - Maintains per-track recognition state: votes, last_seen age, confirmed identity\n",
    "      - Triggers alarms only when confirmed unknown-person count >= records threshold\n",
    "      - Does NOT mutate parent's detection lists (self.clss/self.boxes) for alarm decisions\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "        self,\n",
    "        face_data_path: str,\n",
    "        face_tolerance: float = 0.25,\n",
    "        recognition_min_votes: int = 2,\n",
    "        recognition_window: int = 3,\n",
    "        recognition_interval: int = 2,\n",
    "        alarm_sound_path: Optional[str] = None,\n",
    "        alarm_cooldown_s: float = 10.0,\n",
    "        *args,\n",
    "        **kwargs\n",
    "    ):\n",
    "        super().__init__(*args, **kwargs)\n",
    "\n",
    "        self.face_data_path = face_data_path\n",
    "        self.face_tolerance = float(face_tolerance)\n",
    "        self.recognition_min_votes = int(recognition_min_votes)\n",
    "        self.recognition_window = int(recognition_window)\n",
    "        self.recognition_interval = int(recognition_interval)\n",
    "        self._frame_counter = 0\n",
    "\n",
    "        # per-track persistent recognition state:\n",
    "        # track_id -> {\"name\": Optional[str], \"votes\": int, \"frames\": int, \"last_seen\": int}\n",
    "        self._track_recognition: Dict[str, Dict] = {}\n",
    "\n",
    "        # alarm sound and cooldown\n",
    "        self.sound_played = False\n",
    "        self.email_sent = False\n",
    "        self.alarm_sound_path = alarm_sound_path\n",
    "        self.alarm_last_time = 0.0\n",
    "        self.alarm_cooldown_s = float(alarm_cooldown_s)\n",
    "\n",
    "        # load known faces\n",
    "        self.known_face_encodings: List[np.ndarray] = []\n",
    "        self.known_face_names: List[str] = []\n",
    "        self._load_known_faces(self.face_data_path)\n",
    "\n",
    "        # load alarm sound if provided\n",
    "        if self.alarm_sound_path and Path(self.alarm_sound_path).exists():\n",
    "            try:\n",
    "                pygame.mixer.music.load(self.alarm_sound_path)\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to load alarm sound: {e}\")\n",
    "\n",
    "    def _load_known_faces(self, face_data_path: str):\n",
    "        if not face_data_path:\n",
    "            logger.warning(\"No face_data_path provided; all faces will be treated as unknown.\")\n",
    "            return\n",
    "        if not os.path.exists(face_data_path):\n",
    "            logger.warning(f\"Face data path '{face_data_path}' does not exist.\")\n",
    "            return\n",
    "\n",
    "        loaded = 0\n",
    "        for root, dirs, files in os.walk(face_data_path):\n",
    "            # parent directory name used as label\n",
    "            person_name = os.path.basename(root) if root != face_data_path else None\n",
    "            for fname in files:\n",
    "                if not fname.lower().endswith(('.jpg', '.jpeg', '.png')):\n",
    "                    continue\n",
    "                im_path = os.path.join(root, fname)\n",
    "                try:\n",
    "                    img = face_recognition.load_image_file(im_path)\n",
    "                    encs = face_recognition.face_encodings(img)\n",
    "                    if encs:\n",
    "                        self.known_face_encodings.append(encs[0])\n",
    "                        # if person_name is None or equals the top folder, set name from file stem\n",
    "                        if person_name and person_name != \"\":\n",
    "                            self.known_face_names.append(person_name)\n",
    "                        else:\n",
    "                            self.known_face_names.append(Path(fname).stem)\n",
    "                        loaded += 1\n",
    "                except Exception as e:\n",
    "                    logger.debug(f\"Failed to load face from {im_path}: {e}\")\n",
    "        logger.info(f\"Loaded {loaded} known face encodings from '{face_data_path}'\")\n",
    "\n",
    "    def _maybe_play_alarm(self):\n",
    "        now = time.time()\n",
    "        if (now - self.alarm_last_time) < self.alarm_cooldown_s:\n",
    "            return\n",
    "        if self.alarm_sound_path and Path(self.alarm_sound_path).exists():\n",
    "            try:\n",
    "                if not pygame.mixer.music.get_busy():\n",
    "                    pygame.mixer.music.play()\n",
    "                    self.sound_played = True\n",
    "                    self.alarm_last_time = now\n",
    "                    logger.info(\"Playing alarm sound\")\n",
    "            except Exception as e:\n",
    "                logger.warning(f\"Failed to play alarm sound: {e}\")\n",
    "        else:\n",
    "            # fallback: set flag and log\n",
    "            self.sound_played = True\n",
    "            self.alarm_last_time = now\n",
    "            logger.info(\"Alarm triggered (no sound file)\")\n",
    "\n",
    "    def _reset_alarm(self):\n",
    "        if self.sound_played:\n",
    "            try:\n",
    "                pygame.mixer.music.stop()\n",
    "            except Exception:\n",
    "                pass\n",
    "        self.sound_played = False\n",
    "        self.email_sent = False\n",
    "\n",
    "    def _get_face_encodings_for_frame(self, im0: np.ndarray, scale: float = 0.25) -> Tuple[List[Tuple[int,int,int,int]], List[np.ndarray], float]:\n",
    "        \"\"\"Run face detection + encodings on a resized frame. Returns face_locations (top,right,bottom,left) scaled to original and encodings.\"\"\"\n",
    "        # Returns empty if called on non-interval frames\n",
    "        if (self._frame_counter % max(1, self.recognition_interval)) != 0:\n",
    "            self._frame_counter += 1\n",
    "            return [], [], scale\n",
    "\n",
    "        small = cv2.resize(im0, (0, 0), fx=scale, fy=scale)\n",
    "        rgb_small = cv2.cvtColor(small, cv2.COLOR_BGR2RGB)\n",
    "        try:\n",
    "            face_locs = face_recognition.face_locations(rgb_small)\n",
    "            encodings = face_recognition.face_encodings(rgb_small, face_locs)\n",
    "        except Exception as e:\n",
    "            logger.debug(f\"face_recognition failed: {e}\")\n",
    "            face_locs, encodings = [], []\n",
    "\n",
    "        # scale back to original coordinates\n",
    "        scaled_locs = []\n",
    "        for (top, right, bottom, left) in face_locs:\n",
    "            scaled_locs.append((int(top/scale), int(right/scale), int(bottom/scale), int(left/scale)))\n",
    "        self._frame_counter += 1\n",
    "        return scaled_locs, encodings, scale\n",
    "\n",
    "    def __call__(self, im0: np.ndarray) -> SolutionResults:\n",
    "        \"\"\"\n",
    "        Process a frame: run base extractor, run face recognition (interval),\n",
    "        associate faces to person boxes, maintain vote/persistence, and trigger alarms only for confirmed unknowns.\n",
    "        \"\"\"\n",
    "        # 1) run base extractor/tracker to populate self.boxes, self.clss, self.confs, self.track_ids, self.names\n",
    "        self.extract_tracks(im0)\n",
    "\n",
    "        annotator = SolutionAnnotator(im0, line_width=self.line_width)\n",
    "\n",
    "        # get face detections/encodings every recognition_interval frames\n",
    "        face_locations, face_encodings, scale = self._get_face_encodings_for_frame(im0)\n",
    "\n",
    "        unknown_person_count = 0\n",
    "        person_cls_id = 0  # COCO person id\n",
    "\n",
    "        boxes = getattr(self, \"boxes\", [])\n",
    "        classes = getattr(self, \"clss\", [])\n",
    "        confs = getattr(self, \"confs\", [])\n",
    "        track_ids = getattr(self, \"track_ids\", [])\n",
    "\n",
    "        use_tracks = bool(track_ids) and len(track_ids) == len(boxes)\n",
    "\n",
    "        # iterate detections\n",
    "        for idx, box in enumerate(boxes):\n",
    "            cls = int(classes[idx]) if idx < len(classes) else None\n",
    "            conf = float(confs[idx]) if idx < len(confs) else 0.0\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            # default label/color\n",
    "            label = self.names.get(cls, str(cls)) if isinstance(self.names, dict) else (self.names[cls] if cls is not None and cls < len(self.names) else str(cls))\n",
    "            color = colors(cls, True) if cls is not None else (255, 255, 255)\n",
    "\n",
    "            if cls != person_cls_id:\n",
    "                # non-person: draw as normal\n",
    "                try:\n",
    "                    annotator.box_label(box, label=label, color=color)\n",
    "                except Exception:\n",
    "                    cv2.rectangle(im0, (x1, y1), (x2, y2), color, 2)\n",
    "                    cv2.putText(im0, str(label), (x1, y1-6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "                continue\n",
    "\n",
    "            # determine an id for this detection (track_id fallback to index)\n",
    "            track_id = track_ids[idx] if use_tracks else f\"idx_{idx}\"\n",
    "\n",
    "            # find a face whose center lies inside the person box\n",
    "            matched_candidate = None\n",
    "            face_found = False\n",
    "            for floc, fenc in zip(face_locations, face_encodings):\n",
    "                top, right, bottom, left = floc\n",
    "                cx = (left + right) // 2\n",
    "                cy = (top + bottom) // 2\n",
    "                if x1 <= cx <= x2 and y1 <= cy <= y2:\n",
    "                    face_found = True\n",
    "                    # compute distances to known faces\n",
    "                    if self.known_face_encodings:\n",
    "                        try:\n",
    "                            dists = face_recognition.face_distance(self.known_face_encodings, fenc)\n",
    "                            best_idx = int(np.argmin(dists))\n",
    "                            best_dist = float(dists[best_idx])\n",
    "                            if best_dist <= self.face_tolerance:\n",
    "                                candidate_name = self.known_face_names[best_idx]\n",
    "                            else:\n",
    "                                candidate_name = \"Unknown\"\n",
    "                        except Exception:\n",
    "                            candidate_name = \"Unknown\"\n",
    "                    else:\n",
    "                        candidate_name = \"Unknown\"\n",
    "\n",
    "                    # update per-track recognition state\n",
    "                    tr = self._track_recognition.setdefault(track_id, {\"name\": None, \"votes\": 0, \"frames\": 0, \"last_seen\": 0})\n",
    "                    if tr[\"name\"] == candidate_name:\n",
    "                        tr[\"votes\"] = min(tr[\"votes\"] + 1, self.recognition_window)\n",
    "                    else:\n",
    "                        tr[\"name\"] = candidate_name\n",
    "                        tr[\"votes\"] = 1\n",
    "                    tr[\"frames\"] += 1\n",
    "                    tr[\"last_seen\"] = 0\n",
    "                    # decide confirmed name\n",
    "                    if tr[\"name\"] != \"Unknown\" and tr[\"votes\"] >= self.recognition_min_votes:\n",
    "                        confirmed = tr[\"name\"]\n",
    "                    elif tr[\"name\"] == \"Unknown\" and tr[\"votes\"] >= 1:\n",
    "                        confirmed = \"Unknown\"\n",
    "                    else:\n",
    "                        confirmed = None\n",
    "                    matched_candidate = confirmed\n",
    "                    break  # use first associated face\n",
    "\n",
    "            # if no face found, age track recognition\n",
    "            if not face_found and track_id in self._track_recognition:\n",
    "                tr = self._track_recognition[track_id]\n",
    "                tr[\"last_seen\"] = tr.get(\"last_seen\", 0) + 1\n",
    "                # decay votes over time if not seen\n",
    "                if tr[\"last_seen\"] > (self.recognition_window * 2):\n",
    "                    tr[\"votes\"] = max(0, tr[\"votes\"] - 1)\n",
    "\n",
    "            # determine label/color based on confirmed identity\n",
    "            if matched_candidate is None:\n",
    "                if face_found:\n",
    "                    label = f\"Checking... ({conf:.2f})\"\n",
    "                    color = (0, 0, 255)\n",
    "                else:\n",
    "                    label = f\"Person (No Face) ({conf:.2f})\"\n",
    "                    color = (200, 200, 200)\n",
    "            elif matched_candidate == \"Unknown\":\n",
    "                label = f\"Unknown (ALARM!) ({conf:.2f})\"\n",
    "                color = (0, 0, 255)\n",
    "                unknown_person_count += 1\n",
    "            else:\n",
    "                label = f\"{matched_candidate} (Known) ({conf:.2f})\"\n",
    "                color = (0, 255, 0)\n",
    "\n",
    "            try:\n",
    "                annotator.box_label(box, label=label, color=color)\n",
    "            except Exception:\n",
    "                cv2.rectangle(im0, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(im0, label, (x1, y1 - 6), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 1)\n",
    "\n",
    "        # alarm control using unknown_person_count (do not mutate parent's lists)\n",
    "        if unknown_person_count >= self.records:\n",
    "            if not self.email_sent:\n",
    "                # placeholder: self.send_email(im0, unknown_person_count)  # enable if configured\n",
    "                self.email_sent = True\n",
    "                logger.info(f\"Email alert condition met for {unknown_person_count} unknown person(s).\")\n",
    "            if not self.sound_played:\n",
    "                self._maybe_play_alarm()\n",
    "        else:\n",
    "            # reset alarm state when condition clears\n",
    "            if self.sound_played or self.email_sent:\n",
    "                self._reset_alarm()\n",
    "\n",
    "        # cleanup old tracked entries\n",
    "        to_delete = []\n",
    "        for tid, tr in self._track_recognition.items():\n",
    "            if tr.get(\"last_seen\", 0) > 300:\n",
    "                to_delete.append(tid)\n",
    "            else:\n",
    "                tr[\"last_seen\"] = tr.get(\"last_seen\", 0) + 1\n",
    "        for tid in to_delete:\n",
    "            del self._track_recognition[tid]\n",
    "\n",
    "        plot_im = annotator.result()\n",
    "        # optional display through base class utility\n",
    "        try:\n",
    "            self.display_output(plot_im)\n",
    "        except Exception:\n",
    "            pass\n",
    "\n",
    "        total_tracks = len(getattr(self, \"track_ids\", []))\n",
    "        # Return SolutionResults (include extra info for callers)\n",
    "        return SolutionResults(\n",
    "            plot_im=plot_im,\n",
    "            # im0=im0,\n",
    "            total_tracks=total_tracks,\n",
    "            email_sent=self.email_sent,\n",
    "            sound_played=self.sound_played,\n",
    "            unknown_persons=unknown_person_count,\n",
    "        )\n",
    "# ...existing code...\n",
    "if __name__ == \"__main__\":\n",
    "    # Open video\n",
    "    # cap = cv2.VideoCapture(\"media_files/animal_surveillance/goru-churi.mp4\")\n",
    "    cap = cv2.VideoCapture(\"media_files/WIN_20251103_14_11_20_Pro.mp4\")\n",
    "    # cap = cv2.VideoCapture(\"media_files/theaf_surveillance/1092701461-preview.mp4\")\n",
    "    # cap = cv2.VideoCapture(0)\n",
    "    assert cap.isOpened(), \" Error: Cannot read video file.\"\n",
    "\n",
    "    # Video writer setup\n",
    "    w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    # fps = cap.get(cv2.CAP_PROP_FPS) or 25.0\n",
    "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    fourcc = cv2.VideoWriter_fourcc(*\"mp4v\")\n",
    "    video_writer = cv2.VideoWriter(\"security_output.avi\", fourcc, fps, (w, h))\n",
    "\n",
    "    # Initialize the FaceRecognitionAlarm\n",
    "    # Alarm triggers if 3 or more UNKNOWN persons are detected.\n",
    "    security_alarm = FaceRecognitionAlarm(\n",
    "        show=True,\n",
    "        model=\"yolo11m.pt\",  # Using a standard model\n",
    "        records=1,\n",
    "        # classes=[0],  # Only detect 'person' (ID 0) for face recognition\n",
    "        face_data_path=KNOWN_FACE_DIR,\n",
    "        face_tolerance=0.55,  # L2 distance threshold for face recognition\n",
    "        recognition_min_votes=2,  # Minimum votes to confirm recognition\n",
    "        recognition_window=3,  # Number of frames to consider for voting\n",
    "        recognition_interval=1, # Compute face encodings every N frames\n",
    "        conf=0.5,\n",
    "        \n",
    "        \n",
    "    )\n",
    "\n",
    "    print(\"--- Starting Video Processing. Press 'q' to terminate. ---\")\n",
    "    while cap.isOpened():\n",
    "        success, im0 = cap.read()\n",
    "        if not success:\n",
    "            print(\" Video processing completed.\")\n",
    "            break\n",
    "\n",
    "        # Run detection and alarm logic\n",
    "        try:\n",
    "            results = security_alarm(im0)\n",
    "        except Exception as e:\n",
    "            LOGGER.error(f\"Processing frame failed: {e}\")\n",
    "            break\n",
    "\n",
    "        # Write the processed frame to the output video\n",
    "        # if results and getattr(results, \"plot_im\", None) is not None:\n",
    "        #     video_writer.write(results.plot_im)\n",
    "\n",
    "        # Check for 'q' key press to terminate\n",
    "        if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "            print(\" Termination key 'q' pressed. Stopping...\")\n",
    "            break\n",
    "\n",
    "        # Allow pygame to process events to keep the sound responsive (optional)\n",
    "        # try:\n",
    "        #     pygame.event.pump()\n",
    "        # except Exception:\n",
    "        #     pass\n",
    "\n",
    "    # Cleanup\n",
    "    cap.release()\n",
    "    video_writer.release()\n",
    "    cv2.destroyAllWindows()\n",
    "    try:\n",
    "        pygame.mixer.quit()\n",
    "    except Exception:\n",
    "        pass\n",
    "    print(\"--- Resources released. ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c857ece1",
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install insightface "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab14ed42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import insightface\n",
    "import numpy as np\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Initialize face analysis model\n",
    "app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])  # Use 'CUDAExecutionProvider' for GPU\n",
    "app.prepare(ctx_id=0)  # ctx_id=-1 for CPU, 0 for GPU\n",
    "\n",
    "def get_face_embedding(image_path):\n",
    "    \"\"\"Extract face embedding from an image\"\"\"\n",
    "    img = cv2.imread(image_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"Could not read image: {image_path}\")\n",
    "    \n",
    "    faces = app.get(img)\n",
    "    \n",
    "    if len(faces) < 1:\n",
    "        raise ValueError(\"No faces detected in the image\")\n",
    "    if len(faces) > 1:\n",
    "        print(\"Warning: Multiple faces detected. Using first detected face\")\n",
    "    \n",
    "    return faces[0].embedding\n",
    "\n",
    "def compare_faces(emb1, emb2, threshold=0.65): # Adjust this threshold according to your usecase.\n",
    "    \"\"\"Compare two embeddings using cosine similarity\"\"\"\n",
    "    similarity = np.dot(emb1, emb2) / (np.linalg.norm(emb1) * np.linalg.norm(emb2))\n",
    "    return similarity, similarity > threshold\n",
    "\n",
    "# Paths to your Indian face images\n",
    "image1_path = \"path/to/face1.jpg\"\n",
    "image2_path = \"path/to/face2.jpg\"\n",
    "\n",
    "try:\n",
    "    # Get embeddings\n",
    "    emb1 = get_face_embedding(image1_path)\n",
    "    emb2 = get_face_embedding(image2_path)\n",
    "    \n",
    "    # Compare faces\n",
    "    similarity_score, is_same_person = compare_faces(emb1, emb2)\n",
    "    \n",
    "    print(f\"Similarity Score: {similarity_score:.4f}\")\n",
    "    print(f\"Same person? {'YES' if is_same_person else 'NO'}\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"Error: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca162b64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Replace previous failing FaceAnalysis init with this robust cell\n",
    "\n",
    "import torch\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "import logging\n",
    "\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "# choose context: 0 for first GPU, -1 for CPU\n",
    "ctx_id = 0 if torch.cuda.is_available() else -1\n",
    "\n",
    "def init_insightface(ctx_id=ctx_id, det_size=(640, 640)):\n",
    "    \"\"\"\n",
    "    Initialize InsightFace FaceAnalysis with both detection and recognition modules.\n",
    "    Falls back gracefully and logs helpful errors.\n",
    "    \"\"\"\n",
    "    allowed = ['detection', 'recognition']\n",
    "    try:\n",
    "        app = FaceAnalysis(allowed_modules=allowed)\n",
    "        app.prepare(ctx_id=ctx_id, det_size=det_size)\n",
    "        # show which models were loaded\n",
    "        loaded = list(app.models.keys()) if hasattr(app, \"models\") else []\n",
    "        print(f\"InsightFace initialized (ctx_id={ctx_id}). Loaded modules: {loaded}\")\n",
    "        return app\n",
    "    except AssertionError as ae:\n",
    "        # common cause: detection module missing; try re-init forcing detection+recognition\n",
    "        logger.warning(f\"FaceAnalysis assertion failed: {ae}. Retrying with both detection and recognition modules.\")\n",
    "        try:\n",
    "            app = FaceAnalysis(allowed_modules=['detection', 'recognition'])\n",
    "            app.prepare(ctx_id=ctx_id, det_size=det_size)\n",
    "            loaded = list(app.models.keys()) if hasattr(app, \"models\") else []\n",
    "            print(f\"InsightFace initialized after retry (ctx_id={ctx_id}). Loaded modules: {loaded}\")\n",
    "            return app\n",
    "        except Exception as e:\n",
    "            logger.error(f\"Failed to initialize InsightFace after retry: {e}\")\n",
    "            raise\n",
    "    except Exception as e:\n",
    "        logger.error(f\"Failed to initialize InsightFace: {e}\")\n",
    "        raise\n",
    "\n",
    "# Initialize\n",
    "app = init_insightface()\n",
    "\n",
    "# Example check (optional)\n",
    "if app is not None:\n",
    "    try:\n",
    "        print(\"Available face_analysis models:\", list(app.models.keys()))\n",
    "    except Exception:\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "616c2b7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "\n",
    "# Initialize InsightFace\n",
    "app = FaceAnalysis(allowed_modules=['detection', 'recognition'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "# Initialize face analysis model\n",
    "# app = FaceAnalysis(name='buffalo_l', providers=['CPUExecutionProvider'])  # Use 'CUDAExecutionProvider' for GPU\n",
    "# app.prepare(ctx_id=0)  # ctx_id=-1 for CPU, 0 for GPU\n",
    "\n",
    "# Path to the directory containing known faces\n",
    "DATA_DIR = 'family_members'\n",
    "known_encodings = []\n",
    "known_names = []\n",
    "\n",
    "print(\"Starting face registration process...\")\n",
    "for name in os.listdir(DATA_DIR):\n",
    "    person_dir = os.path.join(DATA_DIR, name)\n",
    "    if not os.path.isdir(person_dir):\n",
    "        continue\n",
    "\n",
    "    print(f\"Processing faces for: {name}\")\n",
    "    for image_name in os.listdir(person_dir):\n",
    "        image_path = os.path.join(person_dir, image_name)\n",
    "        img = cv2.imread(image_path)\n",
    "        if img is None:\n",
    "            print(f\"Warning: Could not read image {image_path}\")\n",
    "            continue\n",
    "\n",
    "        faces = app.get(img)\n",
    "        if faces:\n",
    "            # Assume one face per registration image for simplicity\n",
    "            face_encoding = faces[0].embedding\n",
    "            known_encodings.append(face_encoding)\n",
    "            known_names.append(name)\n",
    "            print(f\"  - Encoded {image_name}\")\n",
    "        else:\n",
    "            print(f\"  - No face detected in {image_name}\")\n",
    "\n",
    "if known_encodings:\n",
    "    np.save('known_face_encodings.npy', np.array(known_encodings))\n",
    "    np.save('known_face_names.npy', np.array(known_names))\n",
    "    print(\"\\nRegistration complete. Embeddings saved.\")\n",
    "else:\n",
    "    print(\"\\nNo faces registered. Check your image directories.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af53647a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import insightface\n",
    "from insightface.app import FaceAnalysis\n",
    "from ultralytics import YOLO\n",
    "\n",
    "# Load pre-registered face data\n",
    "try:\n",
    "    known_face_encodings = np.load('known_face_encodings.npy')\n",
    "    known_face_names = np.load('known_face_names.npy')\n",
    "    if known_face_encodings.size == 0:\n",
    "        raise FileNotFoundError\n",
    "    print(f\"Loaded {len(known_face_names)} known faces.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Warning: No known face data found. Please run register_faces.py first.\")\n",
    "    known_face_encodings = []\n",
    "    known_face_names = []\n",
    "\n",
    "# Initialize YOLO for face detection\n",
    "# Using a model pre-trained for face detection is more efficient\n",
    "yolo_model = YOLO('yolo11m.pt')  # Assumes a YOLOv11 face model is used\n",
    "\n",
    "# Initialize InsightFace for facial recognition\n",
    "app = FaceAnalysis(allowed_modules=['recognition', 'detection'])\n",
    "app.prepare(ctx_id=0, det_size=(640, 640))\n",
    "\n",
    "# Open webcam\n",
    "cap = cv2.VideoCapture(\"media_files/WIN_20251103_14_11_20_Pro.mp4\")\n",
    "if not cap.isOpened():\n",
    "    print(\"Error: Could not open video stream.\")\n",
    "    exit()\n",
    "\n",
    "def get_face_embedding(face_img):\n",
    "    \"\"\"Generate InsightFace embedding from a cropped face image.\"\"\"\n",
    "    faces = app.get(face_img)\n",
    "    if faces:\n",
    "        return faces[0].embedding\n",
    "    return None\n",
    "\n",
    "def find_match(embedding):\n",
    "    \"\"\"Compare a new embedding with known ones.\"\"\"\n",
    "    if not known_face_encodings or embedding is None:\n",
    "        return \"Unknown\"\n",
    "        # return [\"unknown\"]\n",
    "    \n",
    "    # Calculate cosine similarity with known embeddings\n",
    "    # Add a small epsilon to avoid division by zero in case of zero norm\n",
    "    epsilon = 1e-10 \n",
    "    similarities = np.dot(known_face_encodings, embedding) / (\n",
    "        np.linalg.norm(known_face_encodings, axis=1) * np.linalg.norm(embedding) + epsilon\n",
    "    )\n",
    "    \n",
    "    # Find the best match\n",
    "    best_match_index = np.argmax(similarities)\n",
    "    \n",
    "    # Threshold for a positive match (adjust as needed)\n",
    "    similarity_threshold = 0.5  \n",
    "    \n",
    "    if similarities[best_match_index] > similarity_threshold:\n",
    "        return known_face_names[best_match_index]\n",
    "    return \"Unknown\"\n",
    "\n",
    "print(\"Starting real-time facial recognition. Press 'q' to exit.\")\n",
    "while True:\n",
    "    ret, frame = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    # Use YOLO to find faces\n",
    "    yolo_results = yolo_model(frame)\n",
    "    \n",
    "    # Process YOLO results\n",
    "    for r in yolo_results:\n",
    "        for box in r.boxes:\n",
    "            x1, y1, x2, y2 = map(int, box.xyxy[0])\n",
    "            \n",
    "            # Ensure coordinates are within frame bounds\n",
    "            y1, y2, x1, x2 = max(0, y1), min(frame.shape[0], y2), max(0, x1), min(frame.shape[1], x2)\n",
    "\n",
    "            # Crop the face region from the original frame\n",
    "            face_img = frame[y1:y2, x1:x2]\n",
    "            \n",
    "            if face_img.size > 0:\n",
    "                face_embedding = get_face_embedding(face_img)\n",
    "                name = find_match(face_embedding)\n",
    "                \n",
    "                # Draw bounding box and name\n",
    "                color = (0, 255, 0) if name != \"Unknown\" else (0, 0, 255)\n",
    "                cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
    "                cv2.putText(frame, name, (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, color, 2)\n",
    "                        \n",
    "    cv2.imshow('YOLO + InsightFace Facial Recognition', frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
=======
>>>>>>> 302d2d26dbbece0806ee2341c3a90065f0fa928f
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ENV_YOLO_WITH_FACE_RECOGNITION",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
